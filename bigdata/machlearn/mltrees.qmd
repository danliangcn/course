---
title: "有监督学习：基于决策树的模型"
author: ""
date: "`r Sys.Date()`"
format: 
  html:
    toc-depth: 5
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(
  echo = TRUE,
  fig.align = "center",
  message = FALSE,
  warning = FALSE,
  collapse = TRUE,
  cache = FALSE
)

library(tidyverse)

theme_set(theme_light())

library(showtext)
library(sysfonts)

# 加载黑体，最好找到字体的路径再加载 mac可用fc-list :lang=zh命令查看
font_add("Heiti SC", "/System/Library/Fonts/STHeiti Medium.ttc")  

# 启用 showtext 自动渲染
showtext_auto()
```

# 1 决策树

基于树的模型是一类非参数算法，通过使用一组分割规则将特征空间划分为多个较小的（非重叠）区域，这些区域具有相似的响应值。在每个区域内拟合一个简单模型（例如，区域内响应值的平均值）来获得预测值。这种分而治之的方法可以产生简单易解释的规则，并可以通过树形图进行可视化。单个决策树的预测性能通常较弱，但是通过集成算法（如随机森林和梯度提升机）可以组合多个决策树构建预测能力更强的模型。

## 1.1 工具和数据

使用以下R包：

```{r}
# 辅助包
library(dplyr)       # 用于数据处理
library(ggplot2)     # 用于出色的绘图

# 建模包
library(rpart)       # 决策树直接引擎
library(caret)       # 决策树元引擎

# 模型解释包
library(rpart.plot)  # 用于绘制决策树
library(vip)         # 用于特征重要性
library(pdp)         # 用于特征效应
```

使用Ames住房示例来展示主要概念。

```{r}
ames <- AmesHousing::make_ames()

library(rsample)
# 分层抽样划分训练集和测试集数据
set.seed(123)
split  <- initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
```

## 2.2 决策树的结构

构建决策树有许多方法，但最常见的是分类与回归树（CART）算法。基本的决策树将训练数据划分为同质子组（即响应值相似的群体），然后在每个子组内拟合一个简单常数（例如，回归问题中子组内响应值的均值）。子组（也称为节点）是通过使用二元分割递归形成，每次分割基于对每个特征的简单“是否”问题进行（例如，年龄\<18岁？）。这一过程重复多次，直到满足适当的停止标准（例如，达到树的最大深度）。在所有分区完成后，模型根据不同的问题的需要预测输出，对于回归问题，使用落入该子组的所有观测值的平均响应值输出预测；对于分类问题，使用子组内占多数的类别预测，同时可以使用子组内各类别的比例获得预测概率。

结果是一个倒挂的树形结构，本质上，决策树是一组规则，允许通过对每个特征提出简单的“是否”问题来进行预测。例如，如果客户是忠诚客户，家庭收入超过15万美元，并且在商店购物，下面示例的决策树将预测客户会兑换优惠券。

![](decisiontree.png)

将树的顶部第一个子组称为根节点（此节点包含所有训练数据）。树底部的最终子组称为终端节点或叶节点。之间的子组称为内部节点。节点之间的连接称为分支。

![](treeterms.png)

## 2.3 分区

CART使用二元递归分区（递归是因为每个分割或规则依赖于其上方的分割）。每个节点的目标是找到“最佳”特征（$x_i$）将剩余数据划分为两个区域（$R_1$ 和 $R_2$），使实际响应值（$y_i$）与预测常数（$c_i$）之间的总体误差最小化。对于回归问题，要最小化的目标函数是总平方误差和（SSE），如方程定义：

$$
SSE = \sum_{i \in R_1} (y_i - c_1)^2 + \sum_{i \in R_2} (y_i - c_2)^2 
$$

对于分类问题，分区通常旨在最大化交叉熵或基尼指数的减少。在回归和分类树中，分区的目标是最小化终端节点中的不相似性。

找到最佳特征/分割组合后，数据被划分为两个区域，并对这两个区域重复分割过程（因此称为二元递归分区）。此过程持续进行，直到达到适当的停止标准（例如，达到最大深度或树变得“过于复杂”）。

需要注意的是，单个特征可以在树中多次使用。例如，假设从简单正弦函数加随机噪声生成的数据：$Y_i \stackrel{iid}{\sim} N(\sin(X_i), \sigma^2)$，其中 $i=1,2,\dots,500$。构建的回归树只有一个根节点（通常称为决策桩），在 $x=3.1$ 处发生分割。

```{r, echo=F}
# create data
set.seed(1112)  # for reproducibility
df <- tibble::tibble(
  x = seq(from = 0, to = 2 * pi, length = 500),
  y = sin(x) + rnorm(length(x), sd = 0.5),
  truth = sin(x)
)

# run decision stump model
ctrl <- list(cp = 0, minbucket = 5, maxdepth = 1)
fit <- rpart(y ~ x, data = df, control = ctrl)

# plot tree 
par(mar = c(1, 1, 1, 1))
rpart.plot(fit)
```

决策树展示了对特征 $x$ 的单一分割（左）。决策边界显示了当 $x < 3.1$ 时，预测值为0.64；当 $x > 3.1$ 时，预测值为-0.67。

```{r, echo=FALSE}
# plot decision boundary
df %>%
  mutate(pred = predict(fit, df)) %>%
  ggplot(aes(x, y)) +
  geom_point(alpha = .2, size = 1) +
  geom_line(aes(x, y = truth), color = "blue", size = .75) +
  geom_line(aes(y = pred), color = "red", size = .75) +
  geom_segment(x = 3.1, xend = 3.1, y = -Inf, yend = -.95,
               arrow = arrow(length = unit(0.25,"cm")), size = .25) +
  annotate("text", x = 3.1, y = -Inf, label = "split", hjust = 1.2, vjust = -1, size = 3) +
  geom_segment(x = 5.5, xend = 6, y = 2, yend = 2, size = .75, color = "blue") +
  geom_segment(x = 5.5, xend = 6, y = 1.7, yend = 1.7, size = .75, color = "red") +
  annotate("text", x = 5.3, y = 2, label = "真实值", hjust = 1, size = 3, color = "blue") +
  annotate("text", x = 5.3, y = 1.7, label = "决策边界", hjust = 1, size = 3, color = "red")
```

如果继续构建更深的树，可以在同一特征（$x$）上继续分割。因为 $x$ 是唯一可用于分割的特征，因此将继续沿该特征的值寻找最佳分割点，直到达到预定的停止标准。

```{r, echo=FALSE}
# fit depth 3 decision tree
ctrl <- list(cp = 0, minbucket = 5, maxdepth = 3)
fit <- rpart(y ~ x, data = df, control = ctrl)
rpart.plot(fit)

# plot decision boundary
df %>%
  mutate(pred = predict(fit, df)) %>%
  ggplot(aes(x, y)) +
  geom_point(alpha = .2, size = 1) +
  geom_line(aes(x, y = truth), color = "blue", size = .75) +
  geom_line(aes(y = pred), color = "red", size = .75)
```


深度为3的决策树，展示了对特征 $x$ 的7次分割，生成8个预测区域。

即使有多个特征可用，如果某个特征在每次分区后仍能提供最佳分割，它可能仍然占据主导地位。例如，应用于鸢尾花数据集的决策树，其中根据两个特征（萼片宽度和萼片长度）预测花的种类（setosa、versicolor和virginica），结果生成了一个包含每个特征两次分割的最佳决策树。分类问题的决策边界生成包围观测值的矩形区域。预测值是区域内占比最高的响应类别。

```{r, echo=FALSE}
# decision tree
iris_fit <- rpart(Species ~ Sepal.Length + Sepal.Width, data = iris)
rpart.plot(iris_fit)

# decision boundary
ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species, shape = Species)) +
  geom_point(show.legend = FALSE) +
  annotate("rect", xmin = -Inf, xmax = 5.44, ymin = 2.8, ymax = Inf, alpha = .75, fill = "orange") +
  annotate("text", x = 4.0, y = 4.4, label = "setosa", hjust = 0, size = 3) +
  annotate("rect", xmin = -Inf, xmax = 5.44, ymin = 2.79, ymax = -Inf, alpha = .75, fill = "grey") +
  annotate("text", x = 4.0, y = 2, label = "versicolor", hjust = 0, size = 3) +
  annotate("rect", xmin = 5.45, xmax = 6.15, ymin = 3.1, ymax = Inf, alpha = .75, fill = "orange") +
  annotate("text", x = 6, y = 4.4, label = "setosa", hjust = 1, vjust = 0, size = 3) +
  annotate("rect", xmin = 5.45, xmax = 6.15, ymin = 3.09, ymax = -Inf, alpha = .75, fill = "grey") +
  annotate("text", x = 6.15, y = 2, label = "versicolor", hjust = 1, vjust = 0, fill = "grey", size = 3) +
  annotate("rect", xmin = 6.16, xmax = Inf, ymin = -Inf, ymax = Inf, alpha = .75, fill = "green") +
  annotate("text", x = 8, y = 2, label = "virginica", hjust = 1, vjust = 0, fill = "green", size = 3)
```


## 2.4 决策树深度的选择

这引出了一个重要问题：树应该有多深（即多复杂）？如果构建过于复杂的树，往往会过拟合训练数据，导致泛化性能较差。

有56次分割的过拟合决策树。

```{r, echo=F}
ctrl <- list(cp = 0, minbucket = 1, maxdepth = 50)
fit <- rpart(y ~ x, data = df, control = ctrl)
rpart.plot(fit)

df %>%
  mutate(pred = predict(fit, df)) %>%
  ggplot(aes(x, y)) +
  geom_point(alpha = .2, size = 1) +
  geom_line(aes(x, y = truth), color = "blue", size = 0.75) +
  geom_line(aes(y = pred), color = "red", size = 0.75)
```


因此，需要在树的深度和复杂性之间取得平衡，以优化对未来未见数据的预测性能。为了找到这种平衡，有两种主要方法：早期停止和修剪。

### 早期停止

早期停止明确限制树的生长。可以通过以下几种方式限制树生长，但最常见的方法是限制树深度到某个水平或限制终端节点的最小观测数。限制树深度意味着在达到某个深度后停止分割（例如，仅生长深度为5层的树）。树越浅，预测的方差越小（对训练集数据的变化不敏感）；然而，过浅的树（如决策桩）可能无法捕捉数据中的交互和复杂模式，从而引入过多偏差。

限制终端节点的最小大小（例如，叶节点必须包含至少10个观测值用于预测），即决定不分割包含过少数据点的中间节点。在极端情况下，终端节点大小为1允许单个观测值被捕获在叶节点中并用作预测（完全拟合训练集数据），会导致高方差和较差的泛化能力。另一方面，较大的值限制进一步分割，从而减少方差。

这两种方法可以独立实施，然而，它们会相互影响。列展示了树深度如何影响决策边界，行展示了终端节点最小观测数如何影响决策边界。

```{r, echo=FALSE}
hyper_grid <- expand.grid(
  maxdepth = c(1, 5, 15),
  minbucket = c(1, 5, 15)
)
results <- data.frame(NULL)

for(i in seq_len(nrow(hyper_grid))) {
 ctrl <- list(cp = 0, maxdepth = hyper_grid$maxdepth[i], minbucket = hyper_grid$minbucket[i])
 fit <- rpart(y ~ x, data = df, control = ctrl) 
 
 predictions <- mutate(
   df, 
   minbucket = factor(paste("叶节点最小规模为", hyper_grid$minbucket[i]), ordered = TRUE),
   maxdepth = factor(paste("决策树最大深度为", hyper_grid$maxdepth[i]), ordered = TRUE)
   )
 predictions$pred <- predict(fit, df)
 results <- rbind(results, predictions)
   
}

ggplot(results, aes(x, y)) +
  geom_point(alpha = .2, size = 1) +
  geom_line(aes(x, y = truth), color = "blue", size = .75) +
  geom_line(aes(y = pred), color = "red", size = 1) +
  facet_grid(minbucket ~ maxdepth)
```


### 修剪

另一种在树的深度和复杂性之间取得平衡的方法是不明确指定决策树的深度，而是先生长一个非常大且复杂的树，然后通过修剪找到最佳子树。例如可以使用成本复杂度参数（$\alpha$）惩罚目标函数中的终端节点数（$T$），如方程所示：

$$
\text{minimize} \left\{ SSE + \alpha |T| \right\} 
$$

对于给定的 $\alpha$ 值，我们找到具有最低惩罚误差的最小修剪树。这和之前讨论的Lasso惩罚很像。与正则化方法类似，较小的惩罚倾向于生成更复杂的模型，导致更大的树。而较大的惩罚导致更小的树。因此，随着树变大，SSE的减少必须大于成本复杂度惩罚。通常，可以评估多个 $\alpha$ 值的模型，并使用交叉验证（CV）确定最佳值，从而找到泛化能力最好的最佳子树。

为了修剪树，我们首先生长一个过于复杂的树（左），然后使用成本复杂度参数识别最佳子树（右）。

```{r, echo=F}
ctrl <- list(cp = 0, minbucket = 1, maxdepth = 50)
fit <- rpart(y ~ x, data = df, control = ctrl)

p1 <- df %>%
  mutate(pred = predict(fit, df)) %>%
  ggplot(aes(x, y)) +
  geom_point(alpha = .3, size = 2) +
  geom_line(aes(x, y = truth), color = "blue", size = 1) +
  geom_line(aes(y = pred), color = "red", size = 1)

fit2 <- rpart(y ~ x, data = df)

p2 <- df %>%
  mutate(pred2 = predict(fit2, df)) %>%
  ggplot(aes(x, y)) +
  geom_point(alpha = .3, size = 2) +
  geom_line(aes(x, y = truth), color = "blue", size = 1) +
  geom_line(aes(y = pred2), color = "red", size = 1)

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

## 2.5 示例

可以使用`rpart`拟合回归树，然后使用`rpart.plot`可视化。回归树和分类树的拟合过程和可视化输出非常相似。两者都使用公式方法表达模型（类似于`lm()`）。然而，拟合回归树时，需要设置`method = "anova"`。默认情况下，`rpart()`会根据响应列的数据类型猜测使用的方法，但明确设置更清晰。

```{r}
ames_dt1 <- rpart(
  formula = Sale_Price ~ .,
  data    = ames_train,
  method  = "anova"
)
```

拟合模型后，可以查看决策树输出。打印关于不同分割的各种信息。例如，从根节点的2049个观测值开始，第一次分割的变量（即提供最大SSE减少的变量）是`Overall_Qual`。在第一个节点，所有`Overall_Qual`属于 {Very_Poor, Poor, Fair, Below_Average, Average, Above_Average, Good} 的观测值进入第2分支（2)）。沿此分支的总观测数（1701）、平均销售价格（155897）和SSE（4.109012e+12）。如果查看第3分支（3)），会看到348个`Overall_Qual`属于 {Very_Good, Excellent, Very_Excellent} 的观测值沿此分支，平均销售价格为303245.90，该区域的SSE为2.838371e+12。所以，`Overall_Qual`是销售价格的重要预测变量，品质高端的房屋平均销售价格几乎是其他房屋的两倍。

```{r}
ames_dt1

```

可以使用`rpart.plot()`可视化树模型。`rpart.plot()`函数有许多绘图选项，可以自行探索。然而，在默认输出中，会显示每个节点的数据百分比和该节点的预测结果。此树包含10个内部节点，12个终端节点。训练数据中有80个特征，但是决策树只在10个特征上进行分区。

```{r}
rpart.plot(ames_dt1)
```

`rpart()`会自动应用一系列成本复杂度（$\alpha$）值来修剪树，找到最佳的树深度。为了比较每个 $\alpha$ 值的误差，`rpart()`默认执行10折交叉验证。超过12个终端节点后，回报递减，如图所示（y轴是交叉验证误差，下方x轴是成本复杂度（$\alpha$）值，上方x轴是终端节点数（即树大小 $|T|$））。在实际操作中，建议使用最小交叉验证误差的1标准误差（SE）内的最小树（称为1-SE规则），穿过 $|T|=11$ 点附近的虚线，代表可以使用具有11个终端节点的树，并合理期望在小误差范围内获得相似结果。

```{r}
plotcp(ames_dt1)
```

修剪复杂度参数（cp）图，展示不同cp值（下方x轴）的相对交叉验证误差（y轴）。较小的cp值导致更大的树（上方x轴），即cp值越小，对模型复杂度的惩罚越轻，树变越大。使用1-SE规则，树大小为10-12时提供最佳交叉验证结果。

可以通过设置`cp = 0`（无惩罚导致完全生长的树）强制`rpart()`生成完整树。在11个终端节点后，误差减少的回报递减。因此，可以修剪完整树，同时实现最小的预期误差。

```{r}
ames_dt2 <- rpart(
    formula = Sale_Price ~ .,
    data    = ames_train,
    method  = "anova", 
    control = list(cp = 0, xval = 10)
)

plotcp(ames_dt2)
abline(v = 11, lty = "dashed")
```

默认情况下，`rpart()`执行了一些自动调参，得到最佳子树，包含10次分割、11个终端节点和交叉验证SSE为0.22。`rpart()`不提供RMSE或其他指标，但可以使用`caret`获取。在这两种情况下，较小的惩罚（更深的树）提供了更好的交叉验证结果。

```{r}
# rpart交叉验证结果
ames_dt1$cptable

# caret交叉验证结果
ames_dt3 <- train(
  Sale_Price ~ .,
  data = ames_train,
  method = "rpart",
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 20
)

ggplot(ames_dt3)
```


## 2.6 特征解释

为了衡量特征重要性，统计每个变量在每次分割中对损失函数（例如，SSE）的减少量。在某些情况下，单个变量可能在树中多次使用；因此，某个变量在所有分割中的损失函数总减少量被加起来，用作总特征重要性。使用`caret`时，这些值被标准化，使最重要的特征值为100，其余特征根据其相对损失函数减少量评分。此外，由于可能存在重要但未用于分割的候选变量，每次分割还会统计顶级竞争变量。

图展示了Ames住房决策树的前40个特征。与MARS类似，决策树执行自动特征选择，不使用的特征被视为无信息特征。图中底部四个特征的重要性为零。

```{r}
vip(ames_dt3, num_features = 40, bar = FALSE)
```

通过部分依赖图，可以看到决策树如何建模特征与目标之间的关系。`Gr_Liv_Area`具有非线性关系，在1000-2500之间对预测销售价格的影响越来越强，但超过2500后几乎没有影响。`Gr_Liv_Area`和`Year_Built`交互效应的三维图显示了决策树与之前MARS的一个关键区别：决策树具有刚性的非平滑预测表面（台阶状的）。MARS是作为CART在回归问题上的改进。

```{r}
# 构建部分依赖图
p1 <- partial(ames_dt3, pred.var = "Gr_Liv_Area") %>% autoplot()
p2 <- partial(ames_dt3, pred.var = "Year_Built") %>% autoplot()
p3 <- partial(ames_dt3, pred.var = c("Gr_Liv_Area", "Year_Built")) %>% 
  plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, 
              colorkey = TRUE, screen = list(z = -20, x = -60))

# 并排显示图
gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```

决策树具有许多优点：  

- 决策树需要很少的预处理。特征工程也会改善决策树，但是没有预处理要求。  
- 决策树可以轻松处理分类特征而无需预处理。对于具有两个以上级别的无序分类特征，根据结果对其类别进行排序（对于回归问题，使用响应均值；对于分类问题，使用结果类别的比例）。  
- 大多数决策树实现可以处理特征中的缺失值，不需要插补。最常见的是为分类变量创建新的“缺失”类别等。  

然而，单个决策树通常无法达到最先进的预测准确性，因此需要集成模型。

# 2 装袋法

自举法（bootstrapping）作为一种重采样方法，通过对原始训练数据进行有放回抽样，创建 $b$ 个自举样本。本部分介绍如何利用自举法创建预测的集成模型。**自举聚合**（bootstrap aggregating，也称为装袋法，bagging）是机器学习的集成算法之一，旨在提高回归和分类算法的稳定性和准确性。通过模型平均，装袋法有助于降低方差并减少过拟合。尽管通常应用于决策树方法，但它可以与任何类型的方法一起使用。

## 2.1 工具与数据

```{r}
# 辅助包
library(dplyr)       # 用于数据处理
library(ggplot2)     # 用于出色的绘图
library(doParallel)  # 用于foreach的并行后端
library(foreach)     # 用于并行处理的for循环

# 建模包
library(caret)       # 用于通用模型拟合
library(rpart)       # 用于拟合决策树
library(ipred)       # 用于拟合装袋决策树
```

使用 `ames_train` 数据集来展示主要概念。

## 2.2 装袋法使用条件

自举聚合（装袋法）是一种通用的预测模型方法，通过拟合多个版本的预测模型，然后将它们组合（或集成）成一个聚合预测。装袋法是一种相当直接的算法，其中创建 $b$ 个原始训练数据的自举副本，对每个自举样本应用回归或分类算法（通常称为基础学习器或分类器），在回归情境中，通过平均各个基础学习器的预测值生成新预测。对于分类问题，通过多数投票或平均估计的类别概率来组合基础学习器的预测。可以通过方程表示，其中 $X$ 是希望生成预测的记录输入，$\hat{f}_{bag}$ 是装袋预测，$\hat{f}_1(X), \hat{f}_2(X), \dots, \hat{f}_b(X)$ 是各个基础学习器的预测。

$$
\hat{f}_{bag} = \frac{\hat{f}_1(X) + \hat{f}_2(X) + \dots + \hat{f}_b(X)}{b} \tag{10.1}
$$

由于聚合过程，装袋法有效降低了单个基础学习器的方差（即平均化降低了方差）；然而，装袋法并不总是优于单个基础学习器。一些模型的方差比其他模型更大。装袋法对不稳定、高方差的基础学习器尤其有效，因为这些算法的预测输出对训练数据的微小变化会产生重大变化，例如决策树和K近邻（当 $k$ 足够小时）等算法。然而，对于更稳定或高偏差的算法，装袋法对预测输出的改进较小，因为变异性较小（例如，装袋线性回归模型对于足够大的 $b$ 实际上只返回原始预测）。

装袋法背后的总体理念被称为“群体智慧”（wisdom of the crowd）效应。它本质上意味着大型多样化群体中的信息聚合通常比群体中任何单一成员的决策更好。群体成员越多样化，他们的观点和预测就越多样化，这通常导致更好的聚合信息。

```{r, echo=FALSE}
# Simulate some nonlinear monotonic data
set.seed(123)  # for reproducibility
x <- seq(from = 0, to = 2 * pi, length = 500)
y <- sin(x) + rnorm(length(x), sd = 0.3)
df <- data.frame(x, y) %>%
  filter(x < 4.5)

# bootstrapped polynomial model fit
bootstrap_n <- 100
bootstrap_results <- NULL
for(i in seq_len(bootstrap_n)) {
  # reproducible sampled data frames
  set.seed(i)
  index <- sample(seq_len(nrow(df)), nrow(df), replace = TRUE)
  df_sim <- df[index, ]
  
  # fit model and add predictions to results data frame
  fit <- lm(y ~ I(x^3), data = df_sim)
  df_sim$predictions <- predict(fit, df_sim)
  df_sim$model <- paste0("model", i)
  df_sim$ob <- index
  bootstrap_results <- rbind(bootstrap_results, df_sim)
}

p1 <- ggplot(bootstrap_results, aes(x, predictions)) +
  geom_point(data = df, aes(x, y), alpha = .25) +
  geom_line(aes(group = model), show.legend = FALSE, size = .5, alpha = .2) +
  stat_summary(fun.y = "mean", colour = "red", size = 1, geom = "line") +
  scale_y_continuous("Response", limits = c(-2, 2), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 5), expand = c(0, 0)) +
  ggtitle("A) 多项式回归")

# bootstrapped MARS model fit
bootstrap_n <- 100
bootstrap_results <- NULL
for(i in seq_len(bootstrap_n)) {
  # reproducible sampled data frames
  set.seed(i)
  index <- sample(seq_len(nrow(df)), nrow(df), replace = TRUE)
  df_sim <- df[index, ]
  
  # fit model and add predictions to results data frame
  fit <- earth::earth(y ~ x, data = df_sim)
  df_sim$predictions <- predict(fit, df_sim)
  df_sim$model <- paste0("model", i)
  df_sim$ob <- index
  bootstrap_results <- rbind(bootstrap_results, df_sim)
}

p2 <- ggplot(bootstrap_results, aes(x, predictions)) +
  geom_point(data = df, aes(x, y), alpha = .25) +
  geom_line(aes(group = model), show.legend = FALSE, size = .5, alpha = .2) +
  stat_summary(fun.y = "mean", colour = "red", size = 1, geom = "line") +
  scale_y_continuous(NULL, limits = c(-2, 2), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 5), expand = c(0, 0)) +
  ggtitle("B) MARS")

# bootstrapped decision trees fit
bootstrap_n <- 100
bootstrap_results <- NULL
for(i in seq_len(bootstrap_n)) {
  # reproducible sampled data frames
  set.seed(i)
  index <- sample(seq_len(nrow(df)), nrow(df), replace = TRUE)
  df_sim <- df[index, ]
  
  # fit model and add predictions to results data frame
  fit <- rpart::rpart(y ~ x, data = df_sim)
  df_sim$predictions <- predict(fit, df_sim)
  df_sim$model <- paste0("model", i)
  df_sim$ob <- index
  bootstrap_results <- rbind(bootstrap_results, df_sim)
}

p3 <- ggplot(bootstrap_results, aes(x, predictions)) +
  geom_point(data = df, aes(x, y), alpha = .25) +
  geom_line(aes(group = model), show.legend = FALSE, size = .5, alpha = .2) +
  stat_summary(fun.y = "mean", colour = "red", size = 1, geom = "line") +
  scale_y_continuous(NULL, limits = c(-2, 2), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 5), expand = c(0, 0)) +
  ggtitle("C) 决策树")

gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```


在图中展示比较了 $b=100$ 个多项式回归模型、MARS模型和CART决策树的装袋效果。可以看到低方差基础学习器（多项式回归）从装袋中获益甚微，而高方差学习器（决策树）获益更多。装袋不仅有助于减少单棵树的变异性（不稳定性），还有助于平滑预测表面。

最佳性能通常通过装袋50-500棵树获得。具有少量强预测变量的数据集通常需要较少的树；而具有大量噪声或多个强预测变量的数据集可能需要更多树。使用过多树不会导致过拟合。然而，由于运行多个模型，迭代次数越多，计算和时间需求越高。随着这些需求的增加，执行k折交叉验证可能变得计算负担沉重。

通过装袋创建集成的一个好处是，它基于有放回的重采样，可以提供其自身的内部预测性能估计，即**袋外（out-of-bag, OOB）样本**。OOB样本可用于测试预测性能，其结果通常与k折交叉验证相当，假设数据集足够大（例如 $n \geq 1,000$）。因此，随着数据集变大和装袋迭代增加，通常使用OOB误差估计作为预测性能的代表。

可以将OOB泛化性能估计视为一种非结构化但免费的交叉验证统计量。

## 2.3 模型实现

单个决策树在预测Ames住房数据的销售价格时表现不佳，可以使用100棵未修剪的装袋树，而不是单一修剪决策树（不修剪树，保持低偏差和高方差，这时装袋效果最大）。如下代码块所示，获得了比单个（修剪）决策树显著的改进（装袋树的RMSE为26216.47，单棵决策树的RMSE为41019）。

`bagging()` 函数来自 `ipred` 包，使用 `nbagg` 控制装袋模型中的迭代次数，`coob = TRUE` 表示使用OOB误差率。默认情况下，`bagging()` 使用 `rpart::rpart()` 作为决策树基础学习器，但其他基础学习器也可用。由于装袋只是聚合基础学习器，我们可以照常调整基础学习器的参数。这里，通过 `control` 参数传递给 `rpart()`，构建深树（不修剪），每个节点只需两个观测值即可分割。

```{r}
# 使自举法可重现
set.seed(123)

# 训练装袋模型
ames_bag1 <- bagging(
  formula = Sale_Price ~ .,
  data = ames_train,
  nbagg = 100,  
  coob = TRUE,
  control = rpart.control(minsplit = 2, cp = 0)
)

ames_bag1

```

需要注意的一点是，通常树越多越好。随着添加更多树，可以在更多高方差决策树上进行平均。起初误差会显著减少，但最终误差通常会趋于平稳，表明已达到合适的树数量。一般只需50-100棵树即可稳定误差（在其他情况下可能需要500棵或更多）。对于Ames数据，误差在略超过100棵树时便会稳定，此后通过装袋更多树可能不会显著改进。

```{r, echo=FALSE}
# # assess 10-200 bagged trees
# ntree <- seq(10, 200, by = 2)
# 
# # create empty vector to store OOB RMSE values
# rmse <- vector(mode = "numeric", length = length(ntree))
# 
# for (i in seq_along(ntree)) {
#   # reproducibility
#   set.seed(123)
#   # perform bagged model
#   model <- bagging(
#   formula = Sale_Price ~ .,
#   data    = ames_train,
#   coob    = TRUE,
#   control = rpart.control(minsplit = 2, cp = 0),
#   nbagg   = ntree[i]
# )
#   # get OOB error
#   rmse[i] <- model$err
# }
# 
# bagging_errors <- data.frame(ntree, rmse)

# using ranger to do the same as above.  Will allow for bagging under 10 trees
# and is much faster!
ntree <- seq(1, 200, by = 2)
# create empty vector to store OOB RMSE values
rmse <- vector(mode = "numeric", length = length(ntree))

for (i in seq_along(ntree)) {
  # reproducibility
  set.seed(123)
  # perform bagged model
  model <- ranger::ranger(
  formula = Sale_Price ~ .,
  data    = ames_train,
  num.trees = ntree[i],
  mtry = ncol(ames_train) - 1,
  min.node.size = 1
)
  # get OOB error
  rmse[i] <- sqrt(model$prediction.error)
}

bagging_errors <- data.frame(ntree, rmse)

ggplot(bagging_errors, aes(ntree, rmse)) +
  geom_line() +
  geom_hline(yintercept = 41019, lty = "dashed", color = "grey50") +
  annotate("text", x = 100, y = 41385, label = "最佳单个修剪树", vjust = 0, hjust = 0, color = "grey50") +
  annotate("text", x = 100, y = 26750, label = "装袋树", vjust = 0, hjust = 0) +
  ylab("RMSE") +
  xlab("决策树的数量")

```

图中显示装袋1-200棵深层未修剪决策树的误差曲线。装袋的好处在170-180左右棵树时达到最优，但是在前100棵树前，误差就已经大量减少。

还可以在 `caret` 中应用装袋，并使用10折交叉验证来查看集成模型的泛化能力。可以看到200棵树的交叉验证RMSE与OOB估计值相似（差值为495）。然而，使用OOB误差计算耗时几十秒，而执行以下10折交叉验证大约需要二十多分钟。

```{r, eval=FALSE}
ames_bag2 <- train(
  Sale_Price ~ .,
  data = ames_train,
  method = "treebag",
  trControl = trainControl(method = "cv", number = 10),
  nbagg = 200,  
  control = rpart.control(minsplit = 2, cp = 0)
)

ames_bag2

# Bagged CART 
# 
# 2049 samples
#   80 predictor
# 
# No pre-processing
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 1844, 1843, 1846, 1844, 1844, 1844, ... 
# Resampling results:
# 
#   RMSE      Rsquared   MAE    
#   28315.53  0.8766892  16992.4
```

## 2.4 易于并行化

随着迭代次数的增加，装袋可能需要更多的计算资源。不过，装袋过程是针对每个自举样本独立拟合模型，因此可以单独地并行训练模型后，将结果聚集合并为最终模型。通过访问大型集群或多个计算核心，可以更快地在大型数据集上创建装袋后集成。

以下代码展示了在Ames住房数据上并行化装袋算法（使用 $b=160$ 棵决策树），使用8个核心并返回测试数据的预测。

```{r}
# 创建并行套接字集群
cl <- makeCluster(8) # 使用8个工作进程
registerDoParallel(cl) # 注册并行后端

# 并行拟合树并计算测试集的预测
predictions <- foreach(
  icount(160), 
  .packages = "rpart", 
  .combine = cbind
  ) %dopar% {
    # 训练数据的自举副本
    index <- sample(nrow(ames_train), replace = TRUE)
    ames_train_boot <- ames_train[index, ]  
  
    # 对自举副本拟合树
    bagged_tree <- rpart(
      Sale_Price ~ ., 
      control = rpart.control(minsplit = 2, cp = 0),
      data = ames_train_boot
    ) 
    
    predict(bagged_tree, newdata = ames_test)
}

predictions[1:5, 1:7]

```

然后，可以绘制在不同决策树数量条件下预测的RMSE，测试误差与上面OOB误差的变化趋势很像。

```{r}
predictions %>%
  as.data.frame() %>%
  mutate(
    observation = 1:n(),
    actual = ames_test$Sale_Price) %>%
  tidyr::gather(tree, predicted, -c(observation, actual)) %>%
  group_by(observation) %>%
  mutate(tree = stringr::str_extract(tree, '\\d+') %>% as.numeric()) %>%
  ungroup() %>%
  arrange(observation, tree) %>%
  group_by(observation) %>%
  mutate(avg_prediction = cummean(predicted)) %>%
  group_by(tree) %>%
  summarize(RMSE = RMSE(avg_prediction, actual)) %>%
  ggplot(aes(tree, RMSE)) +
  geom_line() +
  xlab('决策树数量')
```

最后需要关闭并行集群。

```{r}
# 关闭并行集群
stopCluster(cl)
```

## 2.5 特征解释

装袋过程会使得可解释的模型不再具有可解释性。然而，基于给定树中每次分割对损失函数（例如，SSE）的减少量总和来衡量特征重要性，仍然可以推断特征如何影响模型。

对于每棵树，计算所有分割中损失函数减少的总和。然后，对每个特征在所有树上聚合同一指标。SSE（对于回归）平均减少量最大的特征被认为是最重要的。可以使用 `vip` 构建 `ames_bag2` 模型的前40个特征的变量重要性图（VIP）。

对于单棵决策树，许多无信息特征未在树中使用。然而，装袋使用基于自举样本的多个决策树，所以会有更多特征用于分割。因此，一般而言，装袋模型会涉及更多特征，但单个特征重要性水平会较低。

```{r}
vip::vip(ames_bag2, num_features = 40, geom = "point")
```

装袋模型中特征与预测响应之间的关系可以采用部分依赖图（PDP），它能直观地展现每个特征如何平均影响预测输出。尽管装袋的平均效应降低了最终集成的可解释性，PDP和其他可解释性方法有助于解释任何“黑盒”模型。部分依赖图突出了特征与响应之间可能存在的独特、有时非线性、非单调的关系。下图呈现销售价格与地块面积和临街宽度特征之间的关系。


```{r}
# 构建部分依赖图
p1 <- pdp::partial(
  ames_bag2, 
  pred.var = "Lot_Area",
  grid.resolution = 20
) %>% 
  autoplot()

p2 <- pdp::partial(
  ames_bag2, 
  pred.var = "Lot_Frontage", 
  grid.resolution = 20
) %>% 
  autoplot()

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

装袋法以牺牲可解释性和计算速度为代价，改善了高方差（低偏差）模型的预测准确性。然而，通过使用各种可解释性算法，如VIP和PDP，仍然可以推断装袋模型如何利用特征信息。此外，由于装袋由独立过程组成，该算法易于并行化。

然而，在装袋树时，仍然存在一个问题。虽然模型构建步骤是独立的，但由于每次分割都考虑所有原始特征，装袋中的树并非完全独立。因为数据结构中存在潜在的强关系，不同自举样本的树通常具有相似的结构，尤其是在树顶部。

例如，使用波士顿住房数据的不同自举样本创建六棵决策树，会看到树顶部具有相似的结构。尽管有15个预测变量可供分割，所有六棵树的头几次分割都由 `lstat` 和 `rm` 变量驱动。

```{r, echo=FALSE}
library(caret)
library(randomForest)
iter = 6
par(mfrow = c(3, 3))
for(i in 1:iter){
  set.seed(i+30)
  # create train/test sets
  train_index <- caret::createDataPartition(pdp::boston$cmedv, p = .6333,
                                     list = FALSE,
                                     times = 1)
  
  train_DF <- pdp::boston[train_index,]
  validate_DF <- pdp::boston[-train_index,]
  
  train_y <- train_DF$cmedv
  train_x <- train_DF[, setdiff(names(train_DF), "cmedv")]
  
  validate_y <- validate_DF$cmedv
  validate_x <- validate_DF[, setdiff(names(validate_DF), "cmedv")]
  
  d_tree <- rpart::rpart(cmedv ~ ., train_DF)
  
  # graphs
  
  rpart.plot::rpart.plot(d_tree, main = paste0("Decision Tree ", i), type = 0, extra = 0) 
  
}
```


这种特性被称为**树相关性**（tree correlation），它阻止装袋进一步降低基础学习器的方差。而下面的随机森林算法可以通过减少这种相关性来扩展和改进装袋决策树，从而提高整体集成的准确性。


# 3 随机森林

随机森林是对装袋决策树的改进，通过构建大量去相关（de-correlated）的树来进一步提高预测性能。随机森林已成为一种非常流行的“开箱即用”或“现成”学习算法，凭借相对较少的超参数调优即可获得良好的预测性能。

## 3.1 工具与数据

重点在于使用 `ranger`和 `h2o` 包实现随机森林。

```{r}
# 辅助包
library(dplyr)    # 用于数据处理
library(ggplot2)  # 用于出色的图形

# 建模包
library(ranger)   # 随机森林的C++实现
library(h2o)      # 随机森林的Java实现
```

继续使用 `ames_train` 数据集来展示主要概念。

## 3.2 扩展装袋法

随机森林基于决策树和装袋法的基本原理构建。装袋法通过在训练数据的自举副本上构建多棵树，为树的构建过程引入随机成分。然后，装袋法通过聚合所有树的预测来降低整体过程的方差，并提高预测性能。但是，简单地装袋树会导致树相关性（tree correlation），限制了方差减少的效果。

随机森林通过在树生长过程中注入更多随机性来帮助减少树相关性。具体来说，在装袋过程中生长决策树时，随机森林执行**分割变量随机化**（split-variable randomization），即每次进行分割时，仅从原始 $p$ 个特征中随机选择 $m_{try}$ 个特征子集进行搜索。通常的默认值是回归问题中 $m_{try} = \frac{p}{3}$，分类问题中 $m_{try} = \sqrt{p}$，可以看做是一个调优参数。

回归或分类随机森林的基本算法可概括如下：

1. 给定训练数据集
2. 选择要构建的树数量（`n_trees`）
3. 对于 $i = 1$ 到 `n_trees`，执行：
4. | 生成原始数据的自举样本
5. | 对自举数据生长回归/分类树
6. | 对于每次分割：
7. | | 从所有 $p$ 个变量中随机选择 $m_{try}$ 个变量
8. | | 在 $m_{try}$ 个变量中选择最佳变量/分割点
9. | | 将节点分割为两个子节点
10. | 结束
11. | 使用典型的树模型停止标准确定树何时完成（但不修剪）
12. 结束
13. 输出树的集成

当 $m_{try} = p$ 时，该算法等同于装袋决策树。

由于算法随机选择自举样本进行训练并在每次分割时随机选择特征子集，生成了更多样化的树集，这往往比装袋树进一步减少树相关性，并显著提高预测能力。

## 3.3 开箱即用（Out-of-the-box）的性能

随机森林之所以流行，是因为它们通常提供非常好的开箱即用性能。尽管有多个可调超参数，但是默认值往往就能产生良好的结果。此外，在流行的机器学习算法中，随机森林在调优时的预测准确性变异性最小。

例如，使用所有超参数设置为默认值的随机森林模型进行训练，获得的袋外（OOB）RMSE优于之前运行的任何模型（无需任何调优）。

默认情况下，`ranger` 将 $m_{try}$ 参数设置为 $\text{floor}(\sqrt{\text{特征数量}})$；然而，对于回归问题，建议以 $\text{floor}(\frac{\text{特征数量}}{3})$ 开始。设置 `respect.unordered.factors = "order"`，指定如何处理无序因子变量，建议设置为“order”。

```{r}
# 特征数量
n_features <- length(setdiff(names(ames_train), "Sale_Price"))

# 训练默认随机森林模型
ames_rf1 <- ranger(
  Sale_Price ~ ., 
  data = ames_train,
  mtry = floor(n_features / 3),
  respect.unordered.factors = "order",
  seed = 123
)

# 获取OOB RMSE
(default_rmse <- sqrt(ames_rf1$prediction.error))
```

## 3.4 超参数

尽管随机森林开箱即用表现良好，但在训练模型时还应考虑几个可调参数。主要考虑的超参数包括：

1. 森林中树的棵数
2. 每次分割考虑的特征数量：$m_{try}$
3. 每棵树的复杂性
4. 采样方案
5. 树构建期间使用的分割规则

其中 (1) 和 (2) 对预测准确性影响最大，应始终进行调优。(3) 和 (4) 对预测准确性的影响较小，但仍值得探索。它们还能影响计算效率。(5) 对预测准确性的影响最小，主要用于提高计算效率。

### 树的棵数

随机森林中的树的数量，虽然严格来说不是超参数，但树的数量需要足够大以稳定误差率。经验法则是从特征数量的10倍开始；随着其他超参数（如 $m_{try}$ 和节点大小）的调整，可能需要更多或更少的树。更多树提供更稳健和稳定的误差估计和变量重要性度量；然而，计算时间随树数量线性增加。

**建议**：从 $p \times 10$ 棵树开始，并根据需要调整。

Ames数据有80个特征，从特征数量的10倍开始通常可确保误差估计收敛。

```{r, echo=FALSE}
# number of features
n_features <- ncol(ames_train) - 1

# tuning grid
tuning_grid <- expand.grid(
  trees = seq(10, 1000, by = 20),
  rmse  = NA
)

for(i in seq_len(nrow(tuning_grid))) {

  # Fit a random forest
  fit <- ranger(
    formula = Sale_Price ~ ., 
    data = ames_train, 
    num.trees = tuning_grid$trees[i],
    mtry = floor(n_features / 3),
    respect.unordered.factors = 'order',
    verbose = FALSE,
    seed = 123
  )
  
  # Extract OOB RMSE
  tuning_grid$rmse[i] <- sqrt(fit$prediction.error)
  
}

ggplot(tuning_grid, aes(trees, rmse)) +
  geom_line(size = 1) +
  ylab("OOB Error (RMSE)") +
  xlab("树的数量")
```


### $m_{try}$

控制随机森林分割变量随机化特征的超参数通常称为 $m_{try}$，它有助于平衡树根部的相关性与合理的预测强度。对于回归问题，默认值通常为 $m_{try} = \frac{p}{3}$，对于分类问题为 $m_{try} = \sqrt{p}$。然而，当具有相关关系的预测变量比较少（例如，噪声预测变量）时，较高的 $m_{try}$ 值往往表现更好，因为它更有可能选择信号最强的特征。当有许多预测变量具有相关性时，较低的 $m_{try}$ 可能表现更好。

**建议**：将恰好五等分区间[2, p]的4个分割点取值和为中心的默认值作为 $m_{try}$ 值开始。

对于Ames数据，略低于默认值（26）的 $m_{try}$ 值（21）可提高性能。

```{r, echo=FALSE}
tuning_grid <- expand.grid(
  trees = seq(10, 1000, by = 20),
  mtry  = floor(c(seq(2, 80, length.out = 5), 26)),
  rmse  = NA
)

for(i in seq_len(nrow(tuning_grid))) {
  fit <- ranger(
  formula    = Sale_Price ~ ., 
  data       = ames_train, 
  num.trees  = tuning_grid$trees[i],
  mtry       = tuning_grid$mtry[i],
  respect.unordered.factors = 'order',
  verbose    = FALSE,
  seed       = 123
)
  
  tuning_grid$rmse[i] <- sqrt(fit$prediction.error)
  
}

labels <- tuning_grid %>%
  filter(trees == 990) %>%
  mutate(mtry = as.factor(mtry))

tuning_grid %>%
  mutate(mtry = as.factor(mtry)) %>%
  ggplot(aes(trees, rmse, color = mtry)) +
  geom_line(size = 1, show.legend = FALSE) +
  ggrepel::geom_text_repel(data = labels, aes(trees, rmse, label = mtry), nudge_x = 50, show.legend = FALSE) +
  ylab("OOB Error (RMSE)") +
  xlab("树的数量")
```


### 树复杂性

随机森林基于单个决策树构建；因此，大多数随机森林模型有一个或多个超参数，允许控制单个树的深度和复杂性。通常包括节点大小、最大深度、最大终端节点数或允许额外分割的所需节点大小等超参数。节点大小是控制树复杂性的最常见超参数，大多数实现使用分类问题为1、回归问题为5的默认值，这些值往往产生良好的结果。然而，如果数据有许多噪声预测变量且较高的 $m_{try}$ 值表现最佳，则增加节点大小（即减少树深度和复杂性）可能提高性能。此外，如果计算时间是一个问题，增加节点大小通常可以显著减少运行时间，且对误差估计的影响较小。

**建议**：调整节点大小时，从1-10之间的三个值开始，并根据对准确性和运行时间的影响进行调整。

增加节点大小以降低树复杂性通常对计算速度（右）的影响大于对误差估计的影响。

```{r, echo=FALSE}
tuning_grid <- expand.grid(
  min.node.size = 1:20,
  run_time  = NA,
  rmse = NA
)

for(i in seq_len(nrow(tuning_grid))) {
  fit_time <- system.time({
    fit <- ranger(
    formula    = Sale_Price ~ ., 
    data       = ames_train, 
    num.trees  = 1000,
    mtry       = 26,
    min.node.size = tuning_grid$min.node.size[i],
    respect.unordered.factors = 'order',
    verbose    = FALSE,
    seed       = 123
  )
})
  
  tuning_grid$run_time[i] <- fit_time[[3]]
  tuning_grid$rmse[i] <- sqrt(fit$prediction.error)
  
}

min_node_size <- tuning_grid %>% 
  mutate(
    error_first = first(rmse),
    runtime_first = first(run_time),
    `Error Growth` = (rmse / error_first) - 1,
    `Run Time Reduction` = (run_time / runtime_first) - 1
    )

p1 <-  ggplot(min_node_size, aes(min.node.size, `Error Growth`)) +
  geom_smooth(size = 1, se = FALSE, color = "black") +
  scale_y_continuous("误差增长百分比", labels = scales::percent) +
  xlab("节点大小") +
  ggtitle("A) 对误差估计的影响")

p2 <-  ggplot(min_node_size, aes(min.node.size, `Run Time Reduction`)) +
  geom_smooth(size = 1, se = FALSE, color = "black") +
  scale_y_continuous("运行时间减少", labels = scales::percent) +
  xlab("节点大小") +
  ggtitle("B) 对运行时间的影响")

gridExtra::grid.arrange(p1, p2, nrow = 1)
```


### 采样方案

随机森林的默认采样方案是自举法，其中对100%的观测值进行有放回采样（换句话说，每个自举副本与原始训练数据大小相同）；可以通过调整样本大小以及是否有放回采样来对模型效果产生影响。样本大小参数决定为每棵树的训练抽取多少观测值，减少样本大小会产生更多样化的树，从而降低树间相关性，这可能对预测准确性产生积极影响。如果数据集中有几个主导特征，减少样本大小也有助于最小化树间相关性。

此外，当有许多具有不同级别数的分类特征（类别变量的各类别的频次差别比较大）时，有放回采样可能导致变量分割选择的偏差。因此，如果有不平衡的类别，无放回采样可提供更少偏差的跨树级别使用。

**建议**：评估25%-100%的3-4个样本大小值，如果有不平衡的分类特征，尝试无放回采样。

Ames数据有几个不平衡的分类特征，如社区、分区、整体质量等。因此，无放回采样提高了性能，因为它导致更少偏差的分割变量选择和更多不相关的树。

```{r, echo=FALSE}
tuning_grid <- expand.grid(
  sample.fraction = seq(.05, .95, by = .05),
  replace  = c(TRUE, FALSE),
  rmse = NA
)

for(i in seq_len(nrow(tuning_grid))) {
  fit <- ranger(
    formula    = Sale_Price ~ ., 
    data       = ames_train, 
    num.trees  = 1000,
    mtry       = 26,
    sample.fraction = tuning_grid$sample.fraction[i],
    replace = tuning_grid$replace[i],
    respect.unordered.factors = 'order',
    verbose    = FALSE,
    seed       = 123
  )

  tuning_grid$rmse[i] <- sqrt(fit$prediction.error)
  
}

tuning_grid %>%
  ggplot(aes(sample.fraction, rmse, color = replace)) +
  geom_line(size = 1) +
  scale_x_continuous("抽样比例", breaks = seq(.1, .9, by = .1), labels = scales::percent) +
  ylab("OOB Error (RMSE)") +
  scale_color_discrete("有放回抽样") +
  theme(legend.position = c(0.8, 0.85),
        legend.key = element_blank(),
        legend.background = element_blank())
```


### 分割规则

随机森林树构建期间的默认分割规则包括从（随机选择的 $m_{try}$）个候选变量的所有分割中选择最小化基尼不纯度（分类问题）或SSE（回归问题）的分割。然而，这些默认分割规则偏向于选择具有更多可能分割的特征（例如，连续变量或具有多个类别的分类变量），而非分割较少的变量（极端情况是只有一种可能分割的二值变量）。条件推理树（conditional inference trees）实现了一种替代分割机制，有助于减少这种变量选择偏差。然而，集成条件推理树在预测准确性方面尚未被证明优于传统方法，且训练时间更长。

为了提高计算效率，可以随机化分割规则，仅考虑变量的随机子集的可能分割值。如果仅随机选择单一分割值，则称为**极随机树**（extremely randomized trees）。由于分割点的额外随机性，这种方法通常对预测准确性没有改进，甚至可能有负面影响。

关于运行时间，极随机树最快，因为分割点完全随机抽取，其次是经典随机森林，而条件推理森林的运行时间最长。

**建议**：如果需要显著提高计算时间，尝试完全随机化树；然而，要确保与传统分割规则的预测准确性进行比较，因为这种方法通常对损失函数有负面影响。

## 3.5 调优策略

随着更复杂的算法和更多的超参数，需要考虑调优策略。之前采用的完全笛卡尔网格搜索，即评估所有感兴趣的超参数组合，往往需要较长的计算时间。下面的代码块搜索120种超参数组合，此网格搜索大约需要3分钟。

```{r}
# 创建超参数网格
hyper_grid <- expand.grid(
  mtry = floor(n_features * c(.05, .15, .25, .333, .4)),
  min.node.size = c(1, 3, 5, 10), 
  replace = c(TRUE, FALSE),                               
  sample.fraction = c(.5, .63, .8),                       
  rmse = NA                                               
)

# 执行完全笛卡尔网格搜索
for(i in seq_len(nrow(hyper_grid))) {
  # 为第i个超参数组合拟合模型
  fit <- ranger(
    formula         = Sale_Price ~ ., 
    data            = ames_train, 
    num.trees       = n_features * 10,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$min.node.size[i],
    replace         = hyper_grid$replace[i],
    sample.fraction = hyper_grid$sample.fraction[i],
    verbose         = FALSE,
    seed            = 123,
    respect.unordered.factors = 'order',
  )
  # 导出OOB误差
  hyper_grid$rmse[i] <- sqrt(fit$prediction.error)
}

# 评估前10个模型
hyper_grid %>%
  arrange(rmse) %>%
  mutate(perc_gain = (default_rmse - rmse) / default_rmse * 100) %>%
  head(10)

```

从结果来看，前10个模型的RMSE接近或低于25000（比基线模型提高了2.5%-3.5%）。在这些结果中，默认 $m_{try}$ 值 $\lfloor \frac{\text{特征数量}}{3} \rfloor = 26$ 几乎足够，较小的节点大小（更深的树）表现最佳。最突出的是，采样率低于100%且无放回采样始终表现最佳。低于100%的采样率增加了程序的随机性，有助于进一步降低树的相关性。无放回采样可能提高性能，因为该数据有许多多类别的不平衡的分类特征。

然而，随着超参数和搜索值的增加以及数据集的扩大，完全笛卡尔搜索可能变得耗时且计算成本高。除了完全笛卡尔搜索，`h2o` 包提供随机网格搜索，允许从一个随机组合跳转到另一个，并提供早期停止规则，允许在满足特定条件（例如，训练了特定数量的模型、经过特定运行时间或准确性的提升非常小）时停止网格搜索。尽管随机离散搜索路径可能无法找到最优模型，但通常能找到一个已经足够好的模型。

要使用 `h2o` 拟合随机森林模型，首先需要启动 `h2o` 会话。

```{r}
h2o.no_progress()
h2o.init(max_mem_size = "5g")
```

接下来，我们需要将训练和测试数据集转换为 `h2o` 可处理的对象。

```{r}
# 将训练数据转换为h2o对象
train_h2o <- as.h2o(ames_train)

# 设置响应列为Sale_Price
response <- "Sale_Price"

# 设置预测变量名称
predictors <- setdiff(colnames(ames_train), response)
```

以下代码使用 `h2o` 拟合默认随机森林模型，展示基线结果（OOB RMSE = 25045.8）与之前拟合的 `ranger` 基线模型非常相似。

```{r}
h2o_rf1 <- h2o.randomForest(
    x = predictors, 
    y = response,
    training_frame = train_h2o, 
    ntrees = n_features * 10,
    seed = 123
)

h2o_rf1

```

要在 `h2o` 中执行网格搜索，需要将超参数网格设置为列表。例如，以下代码搜索比之前更大的网格空间，共240个超参数组合。然后，创建随机网格搜索策略，如果最后10个模型与之前的最佳模型相比，没有哪个模型的MSE改进能够达到0.1%，则停止。如果还能继续搜索改进，在300秒（5分钟）后也中断网格搜索。

```{r}
# 超参数网格
hyper_grid <- list(
  mtries = floor(n_features * c(.05, .15, .25, .333, .4)),
  min_rows = c(1, 3, 5, 10),
  max_depth = c(10, 20, 30),
  sample_rate = c(.55, .632, .70, .80)
)

# 随机网格搜索策略
search_criteria <- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.001,   # 如果改进<0.1%则停止
  stopping_rounds = 10,         # 在最后10个模型上
  max_runtime_secs = 60*5      # 或在5分钟后停止搜索
)
```

然后，可以使用 `h2o.grid()` 执行网格搜索。以下代码启用早期停止执行网格搜索。在 `h2o.grid()` 中指定的早期停止方式是在最后10棵树中，如果单个随机森林模型的整体OOB误差改进小于0.05%时则停止生长单个随机森林模型。这可能可以大大减少单个随机森林模型的构建复杂度。此网格搜索需要5分钟。

```{r}
# 执行网格搜索
random_grid <- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_random_grid",
  x = predictors, 
  y = response, 
  training_frame = train_h2o,
  hyper_params = hyper_grid,
  ntrees = n_features * 10,
  seed = 123,
  stopping_metric = "RMSE",   
  stopping_rounds = 10,           # 如果最后10棵树没有改进
  stopping_tolerance = 0.005,     # RMSE没有0.5%的改进
  search_criteria = search_criteria
)
```

该网格搜索在时间停止前评估了129个模型。最佳模型（`max_depth = 30`, `min_rows = 1`, `mtries = 32`, `sample_rate = 0.7`）实现了OOB RMSE为25030。因此，尽管随机搜索评估的模型数量仅为完全网格搜索的约53%，更有效的随机搜索在指定时间约束内找到了接近最优的模型。

```{r}
# 收集结果并按我们的模型性能指标排序
random_grid_perf <- h2o.getGrid(
  grid_id = "rf_random_grid", 
  sort_by = "mse", 
  decreasing = FALSE
)

random_grid_perf

```

## 3.6 特征解释

随机森林的特征重要性和特征效应计算遵循之前讨论的过程。然而，除了基于不纯度的特征重要性度量（其中特征重要性基于所有树中给定特征的损失函数平均总减少量）外，随机森林通常还包括基于排列的特征重要性度量。在基于排列的方法中，对于每棵树，将OOB样本传递到树中并记录预测准确性。然后，逐一随机打乱每个变量的值并再次计算准确性。由于随机打乱特征值导致的准确性下降在所有树上对每个预测变量取平均值。准确性平均下降最大的变量被认为是最重要的。

例如，可以通过设置 `ranger` 的 `importance` 参数来计算两种特征重要性度量。

对于 `ranger`，一旦从网格搜索中确定了最优参数值，需要使用这些超参数值重新运行模型。还可以增加树的数量，这有助于创建更稳定的变量重要性值。

```{r}
# 使用基于不纯度的变量重要性重新运行模型
rf_impurity <- ranger(
  formula = Sale_Price ~ ., 
  data = ames_train, 
  num.trees = 2000,
  mtry = 32,
  min.node.size = 1,
  sample.fraction = .80,
  replace = FALSE,
  importance = "impurity",
  respect.unordered.factors = "order",
  verbose = FALSE,
  seed  = 123
)

# 使用基于排列的变量重要性重新运行模型
rf_permutation <- ranger(
  formula = Sale_Price ~ ., 
  data = ames_train, 
  num.trees = 2000,
  mtry = 32,
  min.node.size = 1,
  sample.fraction = .80,
  replace = FALSE,
  importance = "permutation",
  respect.unordered.factors = "order",
  verbose = FALSE,
  seed  = 123
)
```

结果的变量重要性图（VIP）如图11.5所示。通常，两种方法下的变量重要性顺序不同；通常会在图的顶部（和底部）看到相似的变量。因此，在本例中，可以有信心地说，有足够证据表明以下三个变量最具影响力：

- `Overall_Qual`
- `Gr_Liv_Area`
- `Neighborhood`

查看两个图中的接下来的约10个变量，还会看到一些共同的影响变量（例如，`Garage_Cars`、`Exter_Qual`、`Bsmt_Qual` 和 `Year_Built`）。

```{r}
p1 <- vip::vip(rf_impurity, num_features = 25, bar = FALSE)
p2 <- vip::vip(rf_permutation, num_features = 25, bar = FALSE)

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

基于不纯度（左）和排列（右）的前25个最重要的变量。

随机森林提供了一种非常强大的开箱即用算法，通常具有出色的预测准确性。它们具有决策树（除了代理分割外）和装袋法的所有优点，但大大降低了不稳定性和树间相关性。由于增加了分割变量选择属性，随机森林比装袋法更快，因为每次树分割的特征搜索空间更小。然而，随着数据集的扩大，随机森林仍会面临计算速度慢的问题，但与装袋法类似，该算法基于独立步骤，大多数现代实现（例如，`ranger`、`h2o`）允许并行化以改善训练时间。


# 4 梯度提升




```{r, echo=FALSE}
# clean up
rm(list = ls())
```

## 参考书籍

-   Bradley Boehmke & Brandon Greenwell，Hands-On Machine Learning with R，CRC Press, 2020.\
-   Pang-Ning Tan 数据挖掘导论（第2版），机械工业出版社，2019.\
-   Ian Foster等 Big Data and Social Science: Data Science Methods and Tools for Research and Practice, CRC Press, 2021.\
