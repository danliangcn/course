[
  {
    "objectID": "bigdata/dataclean/dataclean.html",
    "href": "bigdata/dataclean/dataclean.html",
    "title": "数据获取与预处理",
    "section": "",
    "text": "本部分介绍数据导入与获取、数据转化和清理。"
  },
  {
    "objectID": "bigdata/dataclean/dataclean.html#文本文件",
    "href": "bigdata/dataclean/dataclean.html#文本文件",
    "title": "数据获取与预处理",
    "section": "1.1 文本文件",
    "text": "1.1 文本文件\nreadr包提供了将分隔文本文件导入 R 数据框的函数选择。\n\nlibrary(readr)\n\n# 导入逗号分割的数据\ncgss2017 &lt;- read_csv(\"cgss2017.csv\")\n\n# 导入制表符分割的数据\ncgss2017 &lt;- read_tsv(\"cgss2017.txt\")"
  },
  {
    "objectID": "bigdata/dataclean/dataclean.html#excel文件",
    "href": "bigdata/dataclean/dataclean.html#excel文件",
    "title": "数据获取与预处理",
    "section": "1.2 Excel文件",
    "text": "1.2 Excel文件\nreadxl包可以从 Excel 文件导入数据，支持 xls 和 xlsx 格式。\n\nlibrary(readxl)\n\n# 从excel工作表导入数据\ncgss2017 &lt;- read_excel(\"cgss2017.xlsx\", sheet=1)\n\n由于excel文件可以包含多个工作表，因此您可以使用sheet选项指定所需的工作表。默认值为sheet=1。"
  },
  {
    "objectID": "bigdata/dataclean/dataclean.html#专业统计软件数据文件",
    "href": "bigdata/dataclean/dataclean.html#专业统计软件数据文件",
    "title": "数据获取与预处理",
    "section": "1.3 专业统计软件数据文件",
    "text": "1.3 专业统计软件数据文件\nhaven包提供了从各种统计包导入数据的功能。示例采用的CGSS2015子数据集\n\nlibrary(haven)\n\n# 导入stata数据\ncgss2017 &lt;- read_dta(\"cgss2017.dta\")\n\n# 导入SPSS数据\ncgss &lt;- read_sav(\"cgss2015subset.sav\")\n\n# 导入SAS数据\ncgss2017 &lt;- read_sas(\"cgss2017.sas7bdat\")"
  },
  {
    "objectID": "bigdata/dataclean/dataclean.html#数据库",
    "href": "bigdata/dataclean/dataclean.html#数据库",
    "title": "数据获取与预处理",
    "section": "1.4 数据库",
    "text": "1.4 数据库\n本部分介绍如何从数据库获取数据。数据库操作用到的包中，DBI 是连接数据库并执行 SQL（结构化查询语言） 的低级接口；dbplyr 是高级接口，将 dplyr 代码转换为 SQL 查询，然后使用 DBI 执行。\n\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(tidyverse)\n\n数据库可以看做数据框（data frame）的集合，在数据库术语中称为表（table）。数据库表存储在磁盘上，大小可以任意。数据库表有索引，可以快速找到感兴趣的行。数据库针对快速收集数据而非分析现有数据进行了优化。\n数据库由数据库管理系统（简称DBMS ）运行，数据库管理系统有三种基本形式，包括客户端-服务器型（ 运行在中央服务器上，通过客户端连接，例如PostgreSQL、MariaDB、SQL Server 和 Oracle）、云端数据库（例如亚马逊的 RedShift 和谷歌的 BigQuery）、进程内数据库（例如 SQLite 或 duckdb，完全在个人计算机上）。\n\nDBI基础\n\n# 连接到duckdb，命令会创建一个临时数据库\n# 如需建立永久数据库，可设置保存文件夹\ncon &lt;- DBI::dbConnect(duckdb::duckdb())\n\n# 从其他包加载数据创建数据库表\ndbWriteTable(con, \"mpg\", ggplot2::mpg)\ndbWriteTable(con, \"diamonds\", ggplot2::diamonds)\n\n# 检查数据库中的表\ndbListTables(con)\n\n[1] \"diamonds\" \"mpg\"     \n\n# 检查表diamonds中的内容\ncon |&gt; \n  dbReadTable(\"diamonds\") |&gt; \n  as_tibble()\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\n\n\n# 采用SQL对数据库进行查询，并获取数据\nsql &lt;- \"\n  SELECT carat, cut, clarity, color, price \n  FROM diamonds \n  WHERE price &gt; 15000\n\"\nas_tibble(dbGetQuery(con, sql))\n\n# A tibble: 1,655 × 5\n   carat cut       clarity color price\n   &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;   &lt;fct&gt; &lt;int&gt;\n 1  1.54 Premium   VS2     E     15002\n 2  1.19 Ideal     VVS1    F     15005\n 3  2.1  Premium   SI1     I     15007\n 4  1.69 Ideal     SI1     D     15011\n 5  1.5  Very Good VVS2    G     15013\n 6  1.73 Very Good VS1     G     15014\n 7  2.02 Premium   SI2     G     15014\n 8  2.05 Very Good SI2     F     15017\n 9  1.5  Very Good VS1     F     15022\n10  1.82 Very Good SI1     G     15025\n# ℹ 1,645 more rows\n\n\n\n\ndbplyr基础\ndbplyr 是 dplyr 的后端，意味着可以通过 dplyr 函数代码来实现SQL操作。\n\n# 生成一个数据库表对象\ndiamonds_db &lt;- tbl(con, \"diamonds\")\ndiamonds_db\n\n# Source:   table&lt;diamonds&gt; [?? x 10]\n# Database: DuckDB v1.3.2 [liangdan@Darwin 24.6.0:R 4.4.2/:memory:]\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ more rows\n\nbig_diamonds_db &lt;- diamonds_db |&gt; \n  filter(price &gt; 15000) |&gt; \n  select(carat:clarity, price)\n\nbig_diamonds_db\n\n# Source:   SQL [?? x 5]\n# Database: DuckDB v1.3.2 [liangdan@Darwin 24.6.0:R 4.4.2/:memory:]\n   carat cut       color clarity price\n   &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n 1  1.54 Premium   E     VS2     15002\n 2  1.19 Ideal     F     VVS1    15005\n 3  2.1  Premium   I     SI1     15007\n 4  1.69 Ideal     D     SI1     15011\n 5  1.5  Very Good G     VVS2    15013\n 6  1.73 Very Good G     VS1     15014\n 7  2.02 Premium   G     SI2     15014\n 8  2.05 Very Good F     SI2     15017\n 9  1.5  Very Good F     VS1     15022\n10  1.82 Very Good G     SI1     15025\n# ℹ more rows\n\n\n该对象只代表一个数据库查询，并不是完整数据，顶部显示了 DBMS 名称，并且只有列数，没有行数。因为查找总行数需要执行完整的查询，而这正是在处理大数据时要避免的，执行完整查询会消耗计算资源。\n\n# 展示dplyr代码所实现的SQL\nbig_diamonds_db |&gt;\n  show_query()\n\n&lt;SQL&gt;\nSELECT carat, cut, color, clarity, price\nFROM diamonds\nWHERE (price &gt; 15000.0)\n\n\n获取完整数据\n\nbig_diamonds &lt;- big_diamonds_db |&gt; \n  collect()\nbig_diamonds\n\n# A tibble: 1,655 × 5\n   carat cut       color clarity price\n   &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n 1  1.54 Premium   E     VS2     15002\n 2  1.19 Ideal     F     VVS1    15005\n 3  2.1  Premium   I     SI1     15007\n 4  1.69 Ideal     D     SI1     15011\n 5  1.5  Very Good G     VVS2    15013\n 6  1.73 Very Good G     VS1     15014\n 7  2.02 Premium   G     SI2     15014\n 8  2.05 Very Good F     SI2     15017\n 9  1.5  Very Good F     VS1     15022\n10  1.82 Very Good G     SI1     15025\n# ℹ 1,645 more rows\n\n\n\n\nSQL基础\nSQL由语句组成，例如CREATE、INSERT、SELECT等。查询语句SELECT由5个子句组成，SELECT决定选择哪些列或变量；FROM指明数据库表；WHERE决定选择哪些行；ORDER BY决定如何排序；GROUP BY将查询进行汇总聚合（类似分类汇总）。\n\n# 获取数据，建立数据表对象\ndbplyr::copy_nycflights13(con)\nflights &lt;- tbl(con, \"flights\")\nplanes &lt;- tbl(con, \"planes\")\n\nflights |&gt; show_query()\n\n&lt;SQL&gt;\nSELECT *\nFROM flights\n\n\n\nflights |&gt; \n  filter(dest == \"IAH\") |&gt; \n  arrange(dep_delay) |&gt;\n  show_query()\n\n&lt;SQL&gt;\nSELECT flights.*\nFROM flights\nWHERE (dest = 'IAH')\nORDER BY dep_delay\n\n\n\nflights |&gt; \n  group_by(dest) |&gt; \n  summarize(dep_delay = mean(dep_delay, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT dest, AVG(dep_delay) AS dep_delay\nFROM flights\nGROUP BY dest\n\n\nSQL的表连接与dplyr的数据框连接类似。\n\nflights |&gt; \n  left_join(planes |&gt; rename(year_built = year), join_by(tailnum)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  flights.*,\n  planes.\"year\" AS year_built,\n  \"type\",\n  manufacturer,\n  model,\n  engines,\n  seats,\n  speed,\n  engine\nFROM flights\nLEFT JOIN planes\n  ON (flights.tailnum = planes.tailnum)"
  },
  {
    "objectID": "bigdata/dataclean/dataclean.html#arrow-工具",
    "href": "bigdata/dataclean/dataclean.html#arrow-工具",
    "title": "数据获取与预处理",
    "section": "1.4 Arrow 工具",
    "text": "1.4 Arrow 工具\nParquet 格式是一种基于开放标准的格式，被大数据系统广泛使用替代csv格式。Apache Arrow是为高效分析和传输大数据而设计的工具箱，处理速度极快。R 语言提供了 arrow 包来使用 Apache Arrow ，该包提供了一个 dplyr 后端，允许使用熟悉的 dplyr 语法分析大于内存的数据集。\n\nlibrary(tidyverse)\nlibrary(arrow)\n\nlibrary(dbplyr, warn.conflicts = FALSE)\nlibrary(duckdb)\n\n数据为西雅图公共图书馆书籍借阅记录的CSV文件，数据共有41,389,465行，显示从2005-2022年每本书每月的借阅次数，数据大小为9GB。代码仅为演示。\n\ndir.create(\"data\", showWarnings = FALSE)\n\n# 下载西雅图公共图书馆书籍借阅记录\n# 数据共有41,389,465行，显示从2005-2022年每本书每月的借阅次数\ncurl::multi_download(\n  \"https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv\",\n  \"data/seattle-library-checkouts.csv\",\n  resume = TRUE\n)\n#&gt; # A tibble: 1 × 10\n#&gt;   success status_code resumefrom url                    destfile        error\n#&gt;   &lt;lgl&gt;         &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;                  &lt;chr&gt;           &lt;chr&gt;\n#&gt; 1 TRUE            200          0 https://r4ds.s3.us-we… data/seattle-l… &lt;NA&gt; \n#&gt; # ℹ 4 more variables: type &lt;chr&gt;, modified &lt;dttm&gt;, time &lt;dbl&gt;,\n#&gt; #   headers &lt;list&gt;\n\n\n打开数据集\n\nseattle_csv &lt;- open_dataset(\n  sources = \"data/seattle-library-checkouts.csv\", \n  # 函数会自动扫描几千行数据猜测数据结构\n  # 但由于部分书籍ISBN为空，直接指定该列的数据类型\n  col_types = schema(ISBN = string()),\n  format = \"csv\"\n)\n\n命令并未读取全部数据，数据仍在磁盘上，等待需要时才加载到内存。可以展现文件的元数据信息。\n\nseattle_csv\n#&gt; FileSystemDataset with 1 csv file\n#&gt; UsageClass: string\n#&gt; CheckoutType: string\n#&gt; MaterialType: string\n#&gt; CheckoutYear: int64\n#&gt; CheckoutMonth: int64\n#&gt; Checkouts: int64\n#&gt; Title: string\n#&gt; ISBN: string\n#&gt; Creator: string\n#&gt; Subjects: string\n#&gt; Publisher: string\n#&gt; PublicationYear: string\n\n查看文件内容\n\nseattle_csv |&gt; glimpse()\n#&gt; FileSystemDataset with 1 csv file\n#&gt; 41,389,465 rows x 12 columns\n#&gt; $ UsageClass      &lt;string&gt; \"Physical\", \"Physical\", \"Digital\", \"Physical\", \"Ph…\n#&gt; $ CheckoutType    &lt;string&gt; \"Horizon\", \"Horizon\", \"OverDrive\", \"Horizon\", \"Hor…\n#&gt; $ MaterialType    &lt;string&gt; \"BOOK\", \"BOOK\", \"EBOOK\", \"BOOK\", \"SOUNDDISC\", \"BOO…\n#&gt; $ CheckoutYear     &lt;int64&gt; 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 20…\n#&gt; $ CheckoutMonth    &lt;int64&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,…\n#&gt; $ Checkouts        &lt;int64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 3, 2, 1, 3, 2,…\n#&gt; $ Title           &lt;string&gt; \"Super rich : a guide to having it all / Russell S…\n#&gt; $ ISBN            &lt;string&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#&gt; $ Creator         &lt;string&gt; \"Simmons, Russell\", \"Barclay, James, 1965-\", \"Tim …\n#&gt; $ Subjects        &lt;string&gt; \"Self realization, Conduct of life, Attitude Psych…\n#&gt; $ Publisher       &lt;string&gt; \"Gotham Books,\", \"Pyr,\", \"Random House, Inc.\", \"Di…\n#&gt; $ PublicationYear &lt;string&gt; \"c2011.\", \"2010.\", \"2015\", \"2005.\", \"c2004.\", \"c20…\n\n获取每年的借阅总数\n\nseattle_csv |&gt; \n  group_by(CheckoutYear) |&gt; \n  summarise(Checkouts = sum(Checkouts)) |&gt; \n  arrange(CheckoutYear) |&gt; \n  collect()\n#&gt; # A tibble: 18 × 2\n#&gt;   CheckoutYear Checkouts\n#&gt;          &lt;int&gt;     &lt;int&gt;\n#&gt; 1         2005   3798685\n#&gt; 2         2006   6599318\n#&gt; 3         2007   7126627\n#&gt; 4         2008   8438486\n#&gt; 5         2009   9135167\n#&gt; 6         2010   8608966\n#&gt; # ℹ 12 more rows\n\n\n\n采用Parquet格式进行分区\nParquet 是专为大数据需求而设计的自定义二进制格式。这意味着通常等效的 CSV 文件更小，访问速度更快。Parquet 文件拥有丰富的类型系统，能够将数据类型与数据一起记录（类似SPSS的SAV文件）。Parquet 文件是按列组织的，类似于 R 的数据框，便于数据分析。Parquet 文件是分区块的，这使得可以同时处理文件的不同部分，有时可以完全跳过一些块。\n\n# 指定分区存储路径\npq_path &lt;- \"data/seattle-library-checkouts\"\n\n# 按照借阅年份进行分区，分成18个区块\nseattle_csv |&gt;\n  group_by(CheckoutYear) |&gt;\n  write_dataset(path = pq_path, format = \"parquet\")\n\n# 展示分好的区块\ntibble(\n  files = list.files(pq_path, recursive = TRUE),\n  size_MB = file.size(file.path(pq_path, files)) / 1024^2\n)\n#&gt; # A tibble: 18 × 2\n#&gt;   files                            size_MB\n#&gt;   &lt;chr&gt;                              &lt;dbl&gt;\n#&gt; 1 CheckoutYear=2005/part-0.parquet    109.\n#&gt; 2 CheckoutYear=2006/part-0.parquet    164.\n#&gt; 3 CheckoutYear=2007/part-0.parquet    178.\n#&gt; 4 CheckoutYear=2008/part-0.parquet    195.\n#&gt; 5 CheckoutYear=2009/part-0.parquet    214.\n#&gt; 6 CheckoutYear=2010/part-0.parquet    222.\n#&gt; # ℹ 12 more rows\n\n每个区块大小在100到300MB之间，总共4GB，远小于CSV文件的大小。\n\n\n使用Arrow支持的dplyr\n\nseattle_pq &lt;- open_dataset(pq_path)\n\n# 统计过去五年内每月借出的图书总数\nquery &lt;- seattle_pq |&gt; \n  filter(CheckoutYear &gt;= 2018, MaterialType == \"BOOK\") |&gt;\n  group_by(CheckoutYear, CheckoutMonth) |&gt;\n  summarize(TotalCheckouts = sum(Checkouts)) |&gt;\n  arrange(CheckoutYear, CheckoutMonth)\n\n# 获取数据结果\nquery |&gt; collect()\n#&gt; # A tibble: 58 × 3\n#&gt; # Groups:   CheckoutYear [5]\n#&gt;   CheckoutYear CheckoutMonth TotalCheckouts\n#&gt;          &lt;int&gt;         &lt;int&gt;          &lt;int&gt;\n#&gt; 1         2018             1         355101\n#&gt; 2         2018             2         309813\n#&gt; 3         2018             3         344487\n#&gt; 4         2018             4         330988\n#&gt; 5         2018             5         318049\n#&gt; 6         2018             6         341825\n#&gt; # ℹ 52 more rows\n\n\n\n效率比较\n\n# 不分区的运行时间\nseattle_csv |&gt; \n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |&gt;\n  group_by(CheckoutMonth) |&gt;\n  summarize(TotalCheckouts = sum(Checkouts)) |&gt;\n  arrange(desc(CheckoutMonth)) |&gt;\n  collect() |&gt; \n  system.time()\n#&gt;    user  system elapsed \n#&gt;  11.951   1.297  11.387\n\n# 分区后的运行时间\nseattle_pq |&gt; \n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |&gt;\n  group_by(CheckoutMonth) |&gt;\n  summarize(TotalCheckouts = sum(Checkouts)) |&gt;\n  arrange(desc(CheckoutMonth)) |&gt;\n  collect() |&gt; \n  system.time()\n#&gt;    user  system elapsed \n#&gt;   0.263   0.058   0.063  \n\n分区后，效率提升了100倍。"
  },
  {
    "objectID": "bigdata/dataclean/dataclean.html#分层数据",
    "href": "bigdata/dataclean/dataclean.html#分层数据",
    "title": "数据获取与预处理",
    "section": "1.5 分层数据",
    "text": "1.5 分层数据\n本部分先介绍R语言中的另一个重要对象list，以及通过list构建分层数据解决实际问题，此外，介绍网络数据的常见格式JSON文件。\n\nlibrary(tidyverse)\nlibrary(repurrrsive)\nlibrary(jsonlite)\n\n\n列表\n列表可以在同一个向量中存储不同类型的元素。\n\nx1 &lt;- list(1:4, \"a\", TRUE)\nx1\n\n[[1]]\n[1] 1 2 3 4\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1] TRUE\n\n# 展示列表的结构\nstr(x1)\n\nList of 3\n $ : int [1:4] 1 2 3 4\n $ : chr \"a\"\n $ : logi TRUE\n\n\n\nx2 &lt;- list(1, list(2, list(3, list(4, list(5)))))\n\n# 如果列表结构过于复杂，可以采用View()进行交互探索\nView(x2)\n\n列表列是由列表（list）构成的列数据（column），而不是常见的变量。列表列便于存储分析模型的输出结果（例如，回归分析获得的系数和标准误等）、地理数据（地图对象多边形的坐标点对）等非常规数据类型。\n\ndf &lt;- tibble(\n  x = 1:2, \n  y = c(\"a\", \"b\"),\n  z = list(list(1, 2), list(3, 4, 5))\n)\ndf\n\n# A tibble: 2 × 3\n      x y     z         \n  &lt;int&gt; &lt;chr&gt; &lt;list&gt;    \n1     1 a     &lt;list [2]&gt;\n2     2 b     &lt;list [3]&gt;\n\n\n\n\n解除列表列的嵌套\n列表列的数据不便于计算与分析，常需要先进行解除嵌套操作。\n\n元素具有命名的列表列（或列表等长度）\n\n\ndf1 &lt;- tribble(\n  ~x, ~y,\n  1, list(a = 11, b = 12),\n  2, list(a = 21, b = 22),\n  3, list(a = 31, b = 32),\n)\n\ndf1 |&gt; \n  unnest_wider(y)\n\n# A tibble: 3 × 3\n      x     a     b\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1    11    12\n2     2    21    22\n3     3    31    32\n\n\n\n元素未命名的列表列（通常列表长度不等）\n\n\ndf2 &lt;- tribble(\n  ~x, ~y,\n  1, list(11, 12, 13),\n  2, list(21),\n  3, list(31, 32),\n)\n\ndf2 |&gt; \n  unnest_longer(y)\n\n# A tibble: 6 × 2\n      x     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1    11\n2     1    12\n3     1    13\n4     2    21\n5     3    31\n6     3    32\n\n\n\n宽数据\n\n有些层次数据的结构中含有多个嵌套，并且多次解除嵌套后的变量列非常长。\n\n# 获取层次数据，并查看结构\nrepos &lt;- tibble(json = gh_repos)\nrepos\n\n# A tibble: 6 × 1\n  json       \n  &lt;list&gt;     \n1 &lt;list [30]&gt;\n2 &lt;list [30]&gt;\n3 &lt;list [30]&gt;\n4 &lt;list [26]&gt;\n5 &lt;list [30]&gt;\n6 &lt;list [30]&gt;\n\n\n数据为6行1列，列表列为不等长的未命名列表\n\n# 解除嵌套，将每个列表元素列为单独一行\nrepos |&gt; \n  unnest_longer(json)\n\n# A tibble: 176 × 1\n   json             \n   &lt;list&gt;           \n 1 &lt;named list [68]&gt;\n 2 &lt;named list [68]&gt;\n 3 &lt;named list [68]&gt;\n 4 &lt;named list [68]&gt;\n 5 &lt;named list [68]&gt;\n 6 &lt;named list [68]&gt;\n 7 &lt;named list [68]&gt;\n 8 &lt;named list [68]&gt;\n 9 &lt;named list [68]&gt;\n10 &lt;named list [68]&gt;\n# ℹ 166 more rows\n\n\n解除完依然是列表，但是获得等长的命名列表。\n\n# 解除嵌套，将每个列表元素置入相应列\nrepos |&gt; \n  unnest_longer(json) |&gt; \n  unnest_wider(json) \n\n# A tibble: 176 × 68\n        id name  full_name owner        private html_url description fork  url  \n     &lt;int&gt; &lt;chr&gt; &lt;chr&gt;     &lt;list&gt;       &lt;lgl&gt;   &lt;chr&gt;    &lt;chr&gt;       &lt;lgl&gt; &lt;chr&gt;\n 1  6.12e7 after gaborcsa… &lt;named list&gt; FALSE   https:/… Run Code i… FALSE http…\n 2  4.05e7 argu… gaborcsa… &lt;named list&gt; FALSE   https:/… Declarativ… FALSE http…\n 3  3.64e7 ask   gaborcsa… &lt;named list&gt; FALSE   https:/… Friendly C… FALSE http…\n 4  3.49e7 base… gaborcsa… &lt;named list&gt; FALSE   https:/… Do we get … FALSE http…\n 5  6.16e7 cite… gaborcsa… &lt;named list&gt; FALSE   https:/… Test R pac… TRUE  http…\n 6  3.39e7 clis… gaborcsa… &lt;named list&gt; FALSE   https:/… Unicode sy… FALSE http…\n 7  3.72e7 cmak… gaborcsa… &lt;named list&gt; FALSE   https:/… port of cm… TRUE  http…\n 8  6.80e7 cmark gaborcsa… &lt;named list&gt; FALSE   https:/… CommonMark… TRUE  http…\n 9  6.32e7 cond… gaborcsa… &lt;named list&gt; FALSE   https:/… &lt;NA&gt;        TRUE  http…\n10  2.43e7 cray… gaborcsa… &lt;named list&gt; FALSE   https:/… R package … FALSE http…\n# ℹ 166 more rows\n# ℹ 59 more variables: forks_url &lt;chr&gt;, keys_url &lt;chr&gt;,\n#   collaborators_url &lt;chr&gt;, teams_url &lt;chr&gt;, hooks_url &lt;chr&gt;,\n#   issue_events_url &lt;chr&gt;, events_url &lt;chr&gt;, assignees_url &lt;chr&gt;,\n#   branches_url &lt;chr&gt;, tags_url &lt;chr&gt;, blobs_url &lt;chr&gt;, git_tags_url &lt;chr&gt;,\n#   git_refs_url &lt;chr&gt;, trees_url &lt;chr&gt;, statuses_url &lt;chr&gt;,\n#   languages_url &lt;chr&gt;, stargazers_url &lt;chr&gt;, contributors_url &lt;chr&gt;, …\n\n\n得到68列，挑选一些列查看。\n\nrepos |&gt; \n  unnest_longer(json) |&gt; \n  unnest_wider(json) |&gt; \n  select(id, full_name, owner, description)\n\n# A tibble: 176 × 4\n         id full_name               owner             description               \n      &lt;int&gt; &lt;chr&gt;                   &lt;list&gt;            &lt;chr&gt;                     \n 1 61160198 gaborcsardi/after       &lt;named list [17]&gt; Run Code in the Background\n 2 40500181 gaborcsardi/argufy      &lt;named list [17]&gt; Declarative function argu…\n 3 36442442 gaborcsardi/ask         &lt;named list [17]&gt; Friendly CLI interaction …\n 4 34924886 gaborcsardi/baseimports &lt;named list [17]&gt; Do we get warnings for un…\n 5 61620661 gaborcsardi/citest      &lt;named list [17]&gt; Test R package and repo f…\n 6 33907457 gaborcsardi/clisymbols  &lt;named list [17]&gt; Unicode symbols for CLI a…\n 7 37236467 gaborcsardi/cmaker      &lt;named list [17]&gt; port of cmake to r        \n 8 67959624 gaborcsardi/cmark       &lt;named list [17]&gt; CommonMark parsing and re…\n 9 63152619 gaborcsardi/conditions  &lt;named list [17]&gt; &lt;NA&gt;                      \n10 24343686 gaborcsardi/crayon      &lt;named list [17]&gt; R package for colored ter…\n# ℹ 166 more rows\n\n\n对owner进行解除嵌套。\n\nrepos |&gt; \n  unnest_longer(json) |&gt; \n  unnest_wider(json) |&gt; \n  select(id, full_name, owner, description) |&gt; \n  unnest_wider(owner, names_sep = \"_\")\n\n# A tibble: 176 × 20\n         id full_name    owner_login owner_id owner_avatar_url owner_gravatar_id\n      &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;            &lt;chr&gt;            \n 1 61160198 gaborcsardi… gaborcsardi   660288 https://avatars… \"\"               \n 2 40500181 gaborcsardi… gaborcsardi   660288 https://avatars… \"\"               \n 3 36442442 gaborcsardi… gaborcsardi   660288 https://avatars… \"\"               \n 4 34924886 gaborcsardi… gaborcsardi   660288 https://avatars… \"\"               \n 5 61620661 gaborcsardi… gaborcsardi   660288 https://avatars… \"\"               \n 6 33907457 gaborcsardi… gaborcsardi   660288 https://avatars… \"\"               \n 7 37236467 gaborcsardi… gaborcsardi   660288 https://avatars… \"\"               \n 8 67959624 gaborcsardi… gaborcsardi   660288 https://avatars… \"\"               \n 9 63152619 gaborcsardi… gaborcsardi   660288 https://avatars… \"\"               \n10 24343686 gaborcsardi… gaborcsardi   660288 https://avatars… \"\"               \n# ℹ 166 more rows\n# ℹ 14 more variables: owner_url &lt;chr&gt;, owner_html_url &lt;chr&gt;,\n#   owner_followers_url &lt;chr&gt;, owner_following_url &lt;chr&gt;,\n#   owner_gists_url &lt;chr&gt;, owner_starred_url &lt;chr&gt;,\n#   owner_subscriptions_url &lt;chr&gt;, owner_organizations_url &lt;chr&gt;,\n#   owner_repos_url &lt;chr&gt;, owner_events_url &lt;chr&gt;,\n#   owner_received_events_url &lt;chr&gt;, owner_type &lt;chr&gt;, …\n\n\n\n关系型数据\n\n权力的游戏中的角色信息\n\nchars &lt;- tibble(json = got_chars)\nchars\n\n# A tibble: 30 × 1\n   json             \n   &lt;list&gt;           \n 1 &lt;named list [18]&gt;\n 2 &lt;named list [18]&gt;\n 3 &lt;named list [18]&gt;\n 4 &lt;named list [18]&gt;\n 5 &lt;named list [18]&gt;\n 6 &lt;named list [18]&gt;\n 7 &lt;named list [18]&gt;\n 8 &lt;named list [18]&gt;\n 9 &lt;named list [18]&gt;\n10 &lt;named list [18]&gt;\n# ℹ 20 more rows\n\n\n按命名列表列解除嵌套\n\nchars |&gt; \n  unnest_wider(json)\n\n# A tibble: 30 × 18\n   url           id name  gender culture born  died  alive titles aliases father\n   &lt;chr&gt;      &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;list&gt; &lt;list&gt;  &lt;chr&gt; \n 1 https://w…  1022 Theo… Male   \"Ironb… \"In … \"\"    TRUE  &lt;chr&gt;  &lt;chr&gt;   \"\"    \n 2 https://w…  1052 Tyri… Male   \"\"      \"In … \"\"    TRUE  &lt;chr&gt;  &lt;chr&gt;   \"\"    \n 3 https://w…  1074 Vict… Male   \"Ironb… \"In … \"\"    TRUE  &lt;chr&gt;  &lt;chr&gt;   \"\"    \n 4 https://w…  1109 Will  Male   \"\"      \"\"    \"In … FALSE &lt;chr&gt;  &lt;chr&gt;   \"\"    \n 5 https://w…  1166 Areo… Male   \"Norvo… \"In … \"\"    TRUE  &lt;chr&gt;  &lt;chr&gt;   \"\"    \n 6 https://w…  1267 Chett Male   \"\"      \"At … \"In … FALSE &lt;chr&gt;  &lt;chr&gt;   \"\"    \n 7 https://w…  1295 Cres… Male   \"\"      \"In … \"In … FALSE &lt;chr&gt;  &lt;chr&gt;   \"\"    \n 8 https://w…   130 Aria… Female \"Dorni… \"In … \"\"    TRUE  &lt;chr&gt;  &lt;chr&gt;   \"\"    \n 9 https://w…  1303 Daen… Female \"Valyr… \"In … \"\"    TRUE  &lt;chr&gt;  &lt;chr&gt;   \"\"    \n10 https://w…  1319 Davo… Male   \"Weste… \"In … \"\"    TRUE  &lt;chr&gt;  &lt;chr&gt;   \"\"    \n# ℹ 20 more rows\n# ℹ 7 more variables: mother &lt;chr&gt;, spouse &lt;chr&gt;, allegiances &lt;list&gt;,\n#   books &lt;list&gt;, povBooks &lt;list&gt;, tvSeries &lt;list&gt;, playedBy &lt;list&gt;\n\n\n检查列表列\n\nchars |&gt; \n  unnest_wider(json) |&gt; \n  select(id, where(is.list))\n\n# A tibble: 30 × 8\n      id titles    aliases    allegiances books     povBooks  tvSeries  playedBy\n   &lt;int&gt; &lt;list&gt;    &lt;list&gt;     &lt;list&gt;      &lt;list&gt;    &lt;list&gt;    &lt;list&gt;    &lt;list&gt;  \n 1  1022 &lt;chr [2]&gt; &lt;chr [4]&gt;  &lt;chr [1]&gt;   &lt;chr [3]&gt; &lt;chr [2]&gt; &lt;chr [6]&gt; &lt;chr&gt;   \n 2  1052 &lt;chr [2]&gt; &lt;chr [11]&gt; &lt;chr [1]&gt;   &lt;chr [2]&gt; &lt;chr [4]&gt; &lt;chr [6]&gt; &lt;chr&gt;   \n 3  1074 &lt;chr [2]&gt; &lt;chr [1]&gt;  &lt;chr [1]&gt;   &lt;chr [3]&gt; &lt;chr [2]&gt; &lt;chr [1]&gt; &lt;chr&gt;   \n 4  1109 &lt;chr [1]&gt; &lt;chr [1]&gt;  &lt;NULL&gt;      &lt;chr [1]&gt; &lt;chr [1]&gt; &lt;chr [1]&gt; &lt;chr&gt;   \n 5  1166 &lt;chr [1]&gt; &lt;chr [1]&gt;  &lt;chr [1]&gt;   &lt;chr [3]&gt; &lt;chr [2]&gt; &lt;chr [2]&gt; &lt;chr&gt;   \n 6  1267 &lt;chr [1]&gt; &lt;chr [1]&gt;  &lt;NULL&gt;      &lt;chr [2]&gt; &lt;chr [1]&gt; &lt;chr [1]&gt; &lt;chr&gt;   \n 7  1295 &lt;chr [1]&gt; &lt;chr [1]&gt;  &lt;NULL&gt;      &lt;chr [2]&gt; &lt;chr [1]&gt; &lt;chr [1]&gt; &lt;chr&gt;   \n 8   130 &lt;chr [1]&gt; &lt;chr [1]&gt;  &lt;chr [1]&gt;   &lt;chr [4]&gt; &lt;chr [1]&gt; &lt;chr [1]&gt; &lt;chr&gt;   \n 9  1303 &lt;chr [5]&gt; &lt;chr [11]&gt; &lt;chr [1]&gt;   &lt;chr [1]&gt; &lt;chr [4]&gt; &lt;chr [6]&gt; &lt;chr&gt;   \n10  1319 &lt;chr [4]&gt; &lt;chr [5]&gt;  &lt;chr [2]&gt;   &lt;chr [1]&gt; &lt;chr [3]&gt; &lt;chr [5]&gt; &lt;chr&gt;   \n# ℹ 20 more rows\n\n\n对称号李表列进行解除嵌套，解除后的数据可以采用连接（join）与主数据合并成结构化数据框。\n\ntitles &lt;- chars |&gt; \n  unnest_wider(json) |&gt; \n  select(id, titles) |&gt; \n  unnest_longer(titles) |&gt; \n  filter(titles != \"\") |&gt; \n  rename(title = titles)\ntitles\n\n# A tibble: 52 × 2\n      id title                                                                  \n   &lt;int&gt; &lt;chr&gt;                                                                  \n 1  1022 Prince of Winterfell                                                   \n 2  1022 Lord of the Iron Islands (by law of the green lands)                   \n 3  1052 Acting Hand of the King (former)                                       \n 4  1052 Master of Coin (former)                                                \n 5  1074 Lord Captain of the Iron Fleet                                         \n 6  1074 Master of the Iron Victory                                             \n 7  1166 Captain of the Guard at Sunspear                                       \n 8  1295 Maester                                                                \n 9   130 Princess of Dorne                                                      \n10  1303 Queen of the Andals and the Rhoynar and the First Men, Lord of the Sev…\n# ℹ 42 more rows\n\n\n\n深度嵌套数据\n\n\ngmaps_cities\n\n# A tibble: 5 × 2\n  city       json            \n  &lt;chr&gt;      &lt;list&gt;          \n1 Houston    &lt;named list [2]&gt;\n2 Washington &lt;named list [2]&gt;\n3 New York   &lt;named list [2]&gt;\n4 Chicago    &lt;named list [2]&gt;\n5 Arlington  &lt;named list [2]&gt;\n\n\n数据为5行，5个城市名称和google geocoding API返回的地理信息列表列\n\ngmaps_cities |&gt; \n  unnest_wider(json)\n\n# A tibble: 5 × 3\n  city       results    status\n  &lt;chr&gt;      &lt;list&gt;     &lt;chr&gt; \n1 Houston    &lt;list [1]&gt; OK    \n2 Washington &lt;list [2]&gt; OK    \n3 New York   &lt;list [1]&gt; OK    \n4 Chicago    &lt;list [1]&gt; OK    \n5 Arlington  &lt;list [2]&gt; OK    \n\n\n\n# 剔除status列后继续对未命名列表进行解除嵌套\ngmaps_cities |&gt; \n  unnest_wider(json) |&gt; \n  select(-status) |&gt; \n  unnest_longer(results)\n\n# A tibble: 7 × 2\n  city       results         \n  &lt;chr&gt;      &lt;list&gt;          \n1 Houston    &lt;named list [5]&gt;\n2 Washington &lt;named list [5]&gt;\n3 Washington &lt;named list [5]&gt;\n4 New York   &lt;named list [5]&gt;\n5 Chicago    &lt;named list [5]&gt;\n6 Arlington  &lt;named list [5]&gt;\n7 Arlington  &lt;named list [5]&gt;\n\n\n继续解除results列表列的嵌套，得到地理信息列表列geometry\n\nlocations &lt;- gmaps_cities |&gt; \n  unnest_wider(json) |&gt; \n  select(-status) |&gt; \n  unnest_longer(results) |&gt; \n  unnest_wider(results)\nlocations\n\n# A tibble: 7 × 6\n  city       address_components formatted_address   geometry     place_id types \n  &lt;chr&gt;      &lt;list&gt;             &lt;chr&gt;               &lt;list&gt;       &lt;chr&gt;    &lt;list&gt;\n1 Houston    &lt;list [4]&gt;         Houston, TX, USA    &lt;named list&gt; ChIJAYW… &lt;list&gt;\n2 Washington &lt;list [2]&gt;         Washington, USA     &lt;named list&gt; ChIJ-bD… &lt;list&gt;\n3 Washington &lt;list [4]&gt;         Washington, DC, USA &lt;named list&gt; ChIJW-T… &lt;list&gt;\n4 New York   &lt;list [3]&gt;         New York, NY, USA   &lt;named list&gt; ChIJOwg… &lt;list&gt;\n5 Chicago    &lt;list [4]&gt;         Chicago, IL, USA    &lt;named list&gt; ChIJ7cv… &lt;list&gt;\n6 Arlington  &lt;list [4]&gt;         Arlington, TX, USA  &lt;named list&gt; ChIJ05g… &lt;list&gt;\n7 Arlington  &lt;list [4]&gt;         Arlington, VA, USA  &lt;named list&gt; ChIJD6e… &lt;list&gt;\n\n\n对地理几何体解除嵌套后，得到城市边界（bounds）和位置（location）的列表列\n\nlocations |&gt; \n  select(city, formatted_address, geometry) |&gt; \n  unnest_wider(geometry)\n\n# A tibble: 7 × 6\n  city    formatted_address bounds       location     location_type viewport    \n  &lt;chr&gt;   &lt;chr&gt;             &lt;list&gt;       &lt;list&gt;       &lt;chr&gt;         &lt;list&gt;      \n1 Houston Houston, TX, USA  &lt;named list&gt; &lt;named list&gt; APPROXIMATE   &lt;named list&gt;\n2 Washin… Washington, USA   &lt;named list&gt; &lt;named list&gt; APPROXIMATE   &lt;named list&gt;\n3 Washin… Washington, DC, … &lt;named list&gt; &lt;named list&gt; APPROXIMATE   &lt;named list&gt;\n4 New Yo… New York, NY, USA &lt;named list&gt; &lt;named list&gt; APPROXIMATE   &lt;named list&gt;\n5 Chicago Chicago, IL, USA  &lt;named list&gt; &lt;named list&gt; APPROXIMATE   &lt;named list&gt;\n6 Arling… Arlington, TX, U… &lt;named list&gt; &lt;named list&gt; APPROXIMATE   &lt;named list&gt;\n7 Arling… Arlington, VA, U… &lt;named list&gt; &lt;named list&gt; APPROXIMATE   &lt;named list&gt;\n\n\n继续对location进行解除嵌套，可以得到城市的经纬度坐标。\n\nlocations |&gt; \n  select(city, formatted_address, geometry) |&gt; \n  unnest_wider(geometry) |&gt; \n  unnest_wider(location)\n\n# A tibble: 7 × 7\n  city    formatted_address bounds         lat    lng location_type viewport    \n  &lt;chr&gt;   &lt;chr&gt;             &lt;list&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;         &lt;list&gt;      \n1 Houston Houston, TX, USA  &lt;named list&gt;  29.8  -95.4 APPROXIMATE   &lt;named list&gt;\n2 Washin… Washington, USA   &lt;named list&gt;  47.8 -121.  APPROXIMATE   &lt;named list&gt;\n3 Washin… Washington, DC, … &lt;named list&gt;  38.9  -77.0 APPROXIMATE   &lt;named list&gt;\n4 New Yo… New York, NY, USA &lt;named list&gt;  40.7  -74.0 APPROXIMATE   &lt;named list&gt;\n5 Chicago Chicago, IL, USA  &lt;named list&gt;  41.9  -87.6 APPROXIMATE   &lt;named list&gt;\n6 Arling… Arlington, TX, U… &lt;named list&gt;  32.7  -97.1 APPROXIMATE   &lt;named list&gt;\n7 Arling… Arlington, VA, U… &lt;named list&gt;  38.9  -77.1 APPROXIMATE   &lt;named list&gt;\n\n\n感兴趣可以继续对城市边界（bounds）进行操作。在空间数据分析部分会继续介绍地理数据。"
  },
  {
    "objectID": "bigdata/dataclean/dataclean.html#json",
    "href": "bigdata/dataclean/dataclean.html#json",
    "title": "数据获取与预处理",
    "section": "1.6 JSON",
    "text": "1.6 JSON\nJSON（JavaScript Object Notation）是大多数网页API返回的数据格式。JSON的数据类型包括null（与R的NA类似）、字符串（需用双引号）、数字、布尔值、数组和对象。数组和对象都类似于 R 中的列表；区别在于它们是否命名。 数组（类似未命名列表）：[1, 2, 3] 、[null, 1, “string”, false] 对象（类似命名列表）：{“x”: 1, “y”: 2}\njsonlite包可以将JSON转换为R数据结构。read_json()从磁盘读取JSON文件，parse_json()进行转化。\n\nstr(parse_json('1'))\n\n int 1\n\nstr(parse_json('[1, 2, 3]'))\n\nList of 3\n $ : int 1\n $ : int 2\n $ : int 3\n\nstr(parse_json('{\"x\": [1, 2, 3]}'))\n\nList of 1\n $ x:List of 3\n  ..$ : int 1\n  ..$ : int 2\n  ..$ : int 3\n\n\n常见JSON文件只包含一个顶层数组（意味着代表多个事物，例如返回多个网页、多个记录、多个结果），可以直接转化\n\n# 网页API返回的人员年龄数据\njson &lt;- '[\n  {\"name\": \"张三\", \"age\": 34},\n  {\"name\": \"李四\", \"age\": 27}\n]'\ndf &lt;- tibble(json = parse_json(json))\ndf |&gt; \n  unnest_wider(json)\n\n# A tibble: 2 × 2\n  name    age\n  &lt;chr&gt; &lt;int&gt;\n1 张三     34\n2 李四     27\n\n\n偶尔JSON文件包含一个顶级对象，需要对其先包装到列表，再进行转化。\n\n# 返回结果里包含了本次请求是否成功的信息\njson &lt;- '{\n  \"status\": \"OK\", \n  \"results\": [\n    {\"name\": \"John\", \"age\": 34},\n    {\"name\": \"Susan\", \"age\": 27}\n ]\n}\n'\ndf &lt;- tibble(json = list(parse_json(json)))\ndf |&gt;\n  unnest_wider(json) |&gt; \n  unnest_longer(results) |&gt; \n  unnest_wider(results)\n\n# A tibble: 2 × 3\n  status name    age\n  &lt;chr&gt;  &lt;chr&gt; &lt;int&gt;\n1 OK     John     34\n2 OK     Susan    27"
  },
  {
    "objectID": "bigdata/dataclean/dataclean.html#网页爬取",
    "href": "bigdata/dataclean/dataclean.html#网页爬取",
    "title": "数据获取与预处理",
    "section": "1.7 网页爬取",
    "text": "1.7 网页爬取\nR语言可以承担一些简单的网页爬取工作，并且可以很方便的进行分析，但是具有反爬虫机制的动态网页还需更专业的爬虫工具，例如Python等。\n\nHTML基础\nHTML是一种描述网页的语言，全称是超文本标记语言( Hyper Text Markup Language )。\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;Page title&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1 id='first'&gt;A heading&lt;/h1&gt;\n  &lt;p&gt;Some text &amp; &lt;b&gt;some bold text.&lt;/b&gt;&lt;/p&gt;\n  &lt;img src='myimg.png' width='100' height='100'&gt;\n&lt;/body&gt;\nHTML 是层次结构的文件，包括开始标记（例如&lt;tag&gt;），可选属性（id='first'），结束标记（如&lt;/tag&gt;）和内容（开始标记和结束标记之间的所有内容）。\nHTML 元素有 100 多个。最重要的元素是：\n\n每个 HTML 页面都必须包含在一个&lt;html&gt;元素中，并且该元素必须具有两个子元素：&lt;head&gt;，其中包含页面标题等文档元数据；以及&lt;body&gt;，其中包含在浏览器上看到的内容。\n&lt;h1&gt;（标题 1）、&lt;section&gt;（部分）、&lt;p&gt;（段落）和&lt;ol&gt;（有序列表）等块标签构成了页面的整体结构。\n内联标签（例如&lt;b&gt;（粗体）、&lt;i&gt;（斜体）和&lt;a&gt;（链接））格式化块标签内的文本。\n\n标签可以具有类似于 name1='value1' name2='value2' 之类的属性。其中两个最重要的属性是id和class，它们与 CSS（层叠样式表）结合使用，以控制页面的视觉外观。这些属性在从页面抓取数据时通常很有用。属性还用于记录链接的目标（元素href的属性&lt;a&gt;）和图像的来源（元素src的属性&lt;img&gt;）。\n\n\n静态网页数据爬取\n以厦门市生态环境局行政执法栏目为例说明静态网页爬取过程。\n爬取网页数据，首先需要确定网页数据的位置。通常可以采用CSS selector he和Xpath。CSS selector通过数据所在对象的样式和模式指定位置，参见CSS Selector的语法；而XPath使用路径表达式来选取网页文档中的节点，可以参考XPath的语法。可以通过Chrome浏览器的开发者检查元素功能，找到数据所在的页面的对象，帮助获取样式或模式信息，确定对象的位置。示例网站处罚书来自div的标签对象，其属性class类别名为gl_list1，并且依次向下有ul、li和a三个层级，a链接对象中是处罚书的名称等内容。通过rvest包中的read_html()、html_nodes()、html_text()、html_attr()函数可以实现获取html文本、网页对象节点、网页对象内容、网页对象属性内容等功能。\n\nlibrary(rvest)\n\n\nmyurl = \"https://sthjj.xm.gov.cn/zwgk/gsgk/xzcf/\"\n\n#获取html网页文本\nweb&lt;-read_html(myurl, encoding=\"UTF-8\") \n\n# 获取网页数据处罚书\npunishcompany &lt;-  web |&gt; html_nodes(\"div.gl_list1 ul li a\") |&gt; html_text()  \n# 获取处罚时间\npunishdate &lt;-  web |&gt; html_nodes(\"div.gl_list1 ul li span\") |&gt; html_text()  \n# 获取处罚书链接\npunishlink &lt;- web |&gt; html_nodes(\"div.gl_list1 ul li a\") |&gt; html_attr(name = \"href\")  \n# 将数据保存到数据框\npunish &lt;- data.frame(company=punishcompany, date=punishdate, link=punishlink) \n\n可以通过循环，快速爬取多个类似结构的静态页面，循环从1到10，对应第2页到第11页的信息。\n\nfor(i in 1:10){\n  url &lt;- paste(myurl, \"index_\", i, \".htm\", sep = \"\")\n  web&lt;-read_html(url,encoding=\"UTF-8\")\n  punishcompany &lt;-  web |&gt; html_nodes(\"div.gl_list1 ul li a\") |&gt; html_text() \n  punishdate &lt;-  web |&gt; html_nodes(\"div.gl_list1 ul li span\") |&gt; html_text() \n  punishlink &lt;- web |&gt; html_nodes(\"div.gl_list1 ul li a\") |&gt; html_attr(name = \"href\") \n\n  punish1 &lt;- data.frame(company = punishcompany, date = punishdate, link = punishlink)\n  punish &lt;- punish |&gt; rbind(punish1)\n}\n\nhead(punish)  # 展示获取的处罚书数据\n\n                                     company       date\n1 厦门市腾盛兴电子技术有限公司行政处罚决定书 2024-11-26\n2 厦门市玖玖机动车检测有限公司行政处罚决定书 2024-11-13\n3 解除扣押决定书厦门市腾盛兴电子技术有限公司 2024-10-16\n4                       张帅涛行政处罚决定书 2024-10-11\n5         厦门日上钢圈有限公司行政处罚决定书 2024-10-11\n6   厦门钟利机动车检测有限公司行政处罚决定书 2024-09-29\n                            link\n1 ./202411/t20241126_2903149.htm\n2 ./202411/t20241113_2900716.htm\n3 ./202411/t20241129_2903969.htm\n4 ./202410/t20241011_2894766.htm\n5 ./202410/t20241011_2894762.htm\n6 ./202409/t20240929_2892737.htm\n\ntail(punish)  # 确认是否已完整获取数据\n\n                                                                       company\n193 厦门文仪电脑材料有限公司责令改正违法行为决定书 闽厦（执法）环改〔2022〕1号\n194                                责令改正违法行为决定书 厦门日上金属有限公司\n195               厦门恒兴兴业机械有限公司行政处罚决定书 闽厦环罚〔2021〕363号\n196                  厦门海湾化工有限公司行政处罚决定书  闽厦环罚〔2021〕345号\n197                   厦门海湾化工有限公司行政处罚决定书 闽厦环罚〔2021〕344号\n198  厦门三荣陶瓷开发有限公司不予行政处罚决定书  厦环（执法）不罚决字[2021]1号\n          date                           link\n193 2022-01-10 ./202201/t20220110_2616014.htm\n194 2022-01-05 ./202305/t20230526_2761318.htm\n195 2021-12-31 ./202201/t20220110_2616044.htm\n196 2021-12-15 ./202112/t20211215_2608718.htm\n197 2021-12-15 ./202112/t20211215_2608715.htm\n198 2021-12-10 ./202112/t20211215_2608668.htm\n\n\n\n\n动态页面的爬取\n动态页面与静态页面不同，一般通过与用户建立对话（session）获取用户的数据请求，采用Javascript等程序语言建立定制页面返回数据。因此采用静态页面的方法是无法定位数据的。下面以厦门市生态环境局的咨询投诉栏目为例，介绍动态页面的爬取。\n\nmyurl = \"https://sthjj.xm.gov.cn/gzcy/wyzx/\"\n\nweb&lt;-read_html(myurl, encoding=\"UTF-8\")\n\nweb |&gt; html_nodes(\"div.gl_list2 ul li a\") \n\n{xml_nodeset (1)}\n[1] &lt;a ms-attr-href=\"'./index_18763.htm?id='+el.letterId+'&amp;chnlId='+el.ch ...\n\nweb |&gt; html_nodes(\"div.gl_list2 ul li a\") |&gt; html_text()\n\n[1] \"\"\n\n\n动态页面可以先通过会话请求页面，在爬取数据后，可以模拟点击下一页，此时会话会指向下一页，便可以爬取下一页的数据，依此直到爬取结束。\n\nlibrary(chromote)\n\n# 请求动态页面\nmyurl = \"https://sthjj.xm.gov.cn/gzcy/wyzx/\"\nsess &lt;- read_html_live(myurl) \n\n#sess$view()\n\nconsultdata &lt;- NULL\ni=1\n# 爬取前10页咨询投诉信息\nwhile(i&lt;=10){\n  consult &lt;-  sess |&gt; html_elements(\"div.gl_list2 ul li a\") |&gt; html_text()   # 获取相关数据\n  date &lt;-  sess |&gt; html_elements(\"div.gl_list2 ul li font\") |&gt; html_text() \n  link &lt;- sess |&gt; html_elements(\"div.gl_list2 ul li a\") |&gt; html_attr(name = \"href\") \n  if(length(consult)==0| !(length(consult)==length(date)&length(consult)==length(link))) next \n  consulttemp &lt;- data.frame(consult=consult, date=date, link=link)\n  consultdata &lt;- consultdata %&gt;% rbind(consulttemp)     # 保存到数据框\n  \n  if(nrow(consultdata) == 200) break       # 如果完成爬取，退出\n  sess$click(\"a.next.b-free-read-leaf\")     # 模拟页面点击下一页\n  i=i+1\n}\n\n# 检查数据\nhead(consultdata)  \ntail(consultdata)"
  },
  {
    "objectID": "bigdata/dataclean/dataclean.html#选择变量",
    "href": "bigdata/dataclean/dataclean.html#选择变量",
    "title": "数据获取与预处理",
    "section": "2.1 选择变量",
    "text": "2.1 选择变量\n该select函数从数据集选取指定的变量或列。\n\nlibrary(haven)\n\n# 导入SPSS数据\ncgss &lt;- read_sav(\"cgss2015subset.sav\")\n\nlibrary(dplyr)\n\n# 从数据框cgss中选择变量id，sex和age构成新数据集\nnewdata &lt;- select(cgss, id, sex, age)\n\n# 选择变量id以及hp1与hp4之间的所有变量 \nnewdata &lt;- select(cgss, id, hp1:hp4)\n\n# 选择除了edu和lnincome以外的所有变量\nnewdata &lt;- select(cgss, -edu, -lnincome)"
  },
  {
    "objectID": "bigdata/dataclean/dataclean.html#选择观测",
    "href": "bigdata/dataclean/dataclean.html#选择观测",
    "title": "数据获取与预处理",
    "section": "2.2 选择观测",
    "text": "2.2 选择观测\n该filter函数选择符合特定条件的观测（行）。可以使用&(AND) 和|(OR) 逻辑符号组合多个条件。\n\nlibrary(dplyr)\n\n# 为了方便展示，将带有值标签的变量转换为因子(使用值标签作为取值)   \ncgss &lt;- cgss %&gt;% mutate(across(where(is.labelled), as_factor))\n\n# 选择女性观测\nnewdata &lt;- filter(cgss, sex == \"女\")\n\n# 选择来自山东的女性\nnewdata &lt;- filter(cgss, sex == \"女\" & province == \"山东省\")\n\n# 选择来自东北三省的观测\nnewdata &lt;- filter(cgss, \n                  province == \"辽宁省\" | \n                  province == \"吉林省\" | \n                  province == \"黑龙江省\")\n\n# 更简洁的代码\nnewdata &lt;- filter(cgss, \n                  province %in% \n                    c(\"辽宁省\", \"吉林省\", \"黑龙江省\"))"
  },
  {
    "objectID": "bigdata/dataclean/dataclean.html#变量创建与重新编码",
    "href": "bigdata/dataclean/dataclean.html#变量创建与重新编码",
    "title": "数据获取与预处理",
    "section": "2.3 变量创建与重新编码",
    "text": "2.3 变量创建与重新编码\nmutate函数可以创建新变量或转换现有变量。\n\nlibrary(dplyr)\n\n# 将身高从厘米转化成英寸，将体重从斤转化成磅 \nnewdata &lt;- mutate(cgss, \n                  height = height * 0.394,\n                  weight   = (weight/2) * 2.205)\n\n# 如果收入高于8万元，则变量incomecat取值为\"高\"，否则取值为\"低\"\n\nnewdata &lt;- mutate(cgss, \n                  incomecat = ifelse(income &gt; 80000, \n                                     \"高\", \n                                     \"低\"))\n                  \n# 将教育程度不是初中、普通高中、中专的转化为其他类别\nnewdata &lt;- mutate(cgss, \n                  edu = ifelse(edu %in% \n                                     c(\"初中\", \"普通高中\", \"中专\"),\n                                     edu,\n                                     \"其他\"))\n                  \n# 将身高大于200或小于75设定为缺失值\nnewdata &lt;- mutate(cgss, \n                  height = ifelse(height &lt; 75 | height &gt; 200,\n                                     NA,\n                                     height))"
  },
  {
    "objectID": "bigdata/dataclean/dataclean.html#汇总数据",
    "href": "bigdata/dataclean/dataclean.html#汇总数据",
    "title": "数据获取与预处理",
    "section": "2.4 汇总数据",
    "text": "2.4 汇总数据\nsummarize函数可用于描述性统计的数据汇总。可与by_group函数结合使用，用于按组计算统计值。na.rm=TRUE选项用于在计算平均值之前删除缺失值。\n\nlibrary(dplyr)\n\n# 计算身高和体重的平均值\nnewdata &lt;- summarize(cgss, \n                     mean_ht = mean(height, na.rm=TRUE), \n                     mean_weight = mean(weight, na.rm=TRUE))\nnewdata\n\n# A tibble: 1 × 2\n  mean_ht mean_weight\n    &lt;dbl&gt;       &lt;dbl&gt;\n1    164.        122.\n\n# 分性别组计算身高和体重的平均值\nnewdata &lt;- group_by(cgss, sex)\nnewdata &lt;- summarize(newdata, \n                     mean_ht = mean(height, na.rm=TRUE), \n                     mean_wt = mean(weight, na.rm=TRUE))\nnewdata\n\n# A tibble: 2 × 3\n  sex   mean_ht mean_wt\n  &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 男       170.    133.\n2 女       159.    113."
  },
  {
    "objectID": "bigdata/dataclean/dataclean.html#使用管道",
    "href": "bigdata/dataclean/dataclean.html#使用管道",
    "title": "数据获取与预处理",
    "section": "2.5 使用管道",
    "text": "2.5 使用管道\ndplyr和tidyr软件包允许使用管道运算符以紧凑的格式编写代码%&gt;%，运算符%&gt;%将左边的结果传递给右边的函数的第一个参数。例如：\n\nlibrary(dplyr)\n\nnewdata &lt;- filter(cgss, \n                  sex == \"女\")\nnewdata &lt;- group_by(newdata, province)\nnewdata &lt;- summarize(newdata, \n                     mean_ht = mean(height, na.rm = TRUE))\n\n# 采用管道操作符更简洁，也更符合人类思维\nnewdata &lt;- cgss %&gt;%\n  filter(sex == \"女\") %&gt;%\n  group_by(province) %&gt;%\n  summarize(mean_ht = mean(height, na.rm = TRUE))"
  },
  {
    "objectID": "bigdata/dataclean/dataclean.html#日期数据的处理",
    "href": "bigdata/dataclean/dataclean.html#日期数据的处理",
    "title": "数据获取与预处理",
    "section": "2.6 日期数据的处理",
    "text": "2.6 日期数据的处理\n在 R 中，日期值以字符的形式输入。例如，记录 3 个人出生日期的简单数据集。\n\ndf &lt;- data.frame(\n  dob = c(\"11/10/1963\", \"Jan-23-91\", \"12:1:2001\")\n)\n\nstr(df) \n\n'data.frame':   3 obs. of  1 variable:\n $ dob: chr  \"11/10/1963\" \"Jan-23-91\" \"12:1:2001\"\n\n\n将字符变量转换为日期变量的方法有很多。最简单的方法之一是使用lubridate包中提供的函数。这些函数包括ymd、dmy和 ，mdy分别用于导入年-月-日、日-月-年和月-日-年的格式。\n\nlibrary(lubridate)\n# 将dob变量值从字符转化为日期型数据\ndf$dob &lt;- mdy(df$dob)\nstr(df)\n\n'data.frame':   3 obs. of  1 variable:\n $ dob: Date, format: \"1963-11-10\" \"1991-01-23\" ...\n\n\n这些值在R的内部记录为自 1970 年 1 月 1日以来的天数，可以方便地执行日期运算，提取日期元素（月、日、年），重新格式化（例如，1963年10月11日）。 日期变量对于制作时间相关图表非常重要。"
  },
  {
    "objectID": "bigdata/dataclean/dataclean.html#重塑数据",
    "href": "bigdata/dataclean/dataclean.html#重塑数据",
    "title": "数据获取与预处理",
    "section": "2.7 重塑数据",
    "text": "2.7 重塑数据\n有些图表要求数据为宽格式，而有些图表则要求数据为长格式，示例如下。\n将宽数据集转换为长数据集\n\nlibrary(tidyr)\nwide_data &lt;- data.frame(id = c(\"01\", \"02\", \"03\"),\n                        name = c(\"张三\", \"李四\", \"王五\"),\n                        sex = c(\"男\", \"男\", \"女\"),\n                        height = c(70, 72, 62),\n                        weight = c(180, 195, 130))\nknitr::kable(wide_data, caption = \"宽数据\")\n\n\n宽数据\n\n\nid\nname\nsex\nheight\nweight\n\n\n\n\n01\n张三\n男\n70\n180\n\n\n02\n李四\n男\n72\n195\n\n\n03\n王五\n女\n62\n130\n\n\n\n\nlong_data &lt;- pivot_longer(wide_data,\n                          cols = c(\"height\", \"weight\"),\n                          names_to = \"variable\", \n                          values_to =\"value\")\n\n将长数据转化为宽数据\n\nlibrary(tidyr)\nknitr::kable(long_data, caption = \"长数据\")\n\n\n长数据\n\n\nid\nname\nsex\nvariable\nvalue\n\n\n\n\n01\n张三\n男\nheight\n70\n\n\n01\n张三\n男\nweight\n180\n\n\n02\n李四\n男\nheight\n72\n\n\n02\n李四\n男\nweight\n195\n\n\n03\n王五\n女\nheight\n62\n\n\n03\n王五\n女\nweight\n130\n\n\n\n\nwide_data &lt;- pivot_wider(long_data,\n                         names_from = \"variable\",\n                         values_from = \"value\")"
  },
  {
    "objectID": "bigdata/dataclean/dataclean.html#缺失数据",
    "href": "bigdata/dataclean/dataclean.html#缺失数据",
    "title": "数据获取与预处理",
    "section": "2.8 缺失数据",
    "text": "2.8 缺失数据\n真实数据很可能包含缺失值。处理缺失数据有三种基本方法：特征选择、列删除和插补。ggplot2包中的msleep数据集描述了哺乳动物的睡眠习惯，并且在多个变量上存在缺失值。\n\n特征选择 在特征选择中，可以删除包含太多缺失值的变量（列）。\n\n\ndata(msleep, package=\"ggplot2\")\n\n# 每个变量中缺失值的比例\npctmiss &lt;- colSums(is.na(msleep))/nrow(msleep)\nround(pctmiss, 2)\n\n        name        genus         vore        order conservation  sleep_total \n        0.00         0.00         0.08         0.00         0.35         0.00 \n   sleep_rem  sleep_cycle        awake      brainwt       bodywt \n        0.27         0.61         0.00         0.33         0.00 \n\n\n61% 的 sleep_cycle 值缺失。可以决定将其删除。\n\n按列删除\n整行删除包含缺失值的观测。\n\n\nnewdata &lt;- select(msleep, genus, vore, conservation)\nnewdata &lt;- na.omit(newdata)\n\n\n补值\n插补涉及用“合理”的猜测值（假设缺失值不存在时的值）来替换缺失值。有几种方法，详见VIM、mice、Amelia和missForest等包。这里将使用VIMkNN()包中的函数，用插补值替换缺失值。\n\n\n# 用5个最近邻的值插补缺失值\nlibrary(VIM)\nnewdata &lt;- kNN(msleep, k=5)\n\n基本上，对于每个有缺失值的案例，都会选择k个最相似的、没有缺失值的案例。如果缺失值为数值型，则使用这k 个案例的中位数作为插补值。如果缺失值为类别值，则使用这k 个案例中出现频率最高的值。该过程会迭代所有观测和变量，直到结果收敛（趋于稳定）。\n重要提示：缺失值可能会对研究结果造成偏差（有时甚至非常严重）。如果有大量缺失数据，在删除观测或填补缺失值之前，要慎重考虑其合理性。"
  },
  {
    "objectID": "bigdata/dataclean/dataclean.html#参考书籍",
    "href": "bigdata/dataclean/dataclean.html#参考书籍",
    "title": "数据获取与预处理",
    "section": "参考书籍",
    "text": "参考书籍\n\nHadley Wickham, Mine Cetinkaya-Rundel, Garrett Grolemund. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data, (2nd edition)，O’Reilly Media, 2023.\n\nKabacoff （王小宁等译） R语言实战（第3版），人民邮电出版社，2023.\n\nUlrich Matter Big Data Analytics: A Guide to Data Science Practitioners Making the Transition to Big Data, CRC Press, 2024."
  },
  {
    "objectID": "bigdata/spatial/shp/worldmap/world.html",
    "href": "bigdata/spatial/shp/worldmap/world.html",
    "title": "course",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     \n\n\n         0 0     false"
  },
  {
    "objectID": "bigdata/index.html#课堂练习",
    "href": "bigdata/index.html#课堂练习",
    "title": "公共管理大数据分析",
    "section": "课堂练习",
    "text": "课堂练习"
  },
  {
    "objectID": "bigdata/machlearn/regression.html",
    "href": "bigdata/machlearn/regression.html",
    "title": "有监督学习：回归模型",
    "section": "",
    "text": "线性回归是经典统计建模的基础，是最简单的监督学习算法之一。\n\n\n\n# 辅助包\nlibrary(dplyr)    # 用于数据操作\nlibrary(ggplot2)  # 用于出色的图形展示\n\n# 建模包\nlibrary(caret)    # 用于交叉验证等\n\n# 模型可解释性包\nlibrary(vip)      # 变量重要性\n\n\names &lt;- AmesHousing::make_ames()\n\nlibrary(rsample)\n# 分层抽样划分训练集和测试集数据\nset.seed(123)\nsplit  &lt;- initial_split(ames, prop = 0.7, strata = \"Sale_Price\")\names_train  &lt;- training(split)\names_test   &lt;- testing(split)\n\n\n\n\n简单线性回归（SLR）假设两个连续变量（例如X和Y之间的统计关系（至少近似）是线性的：\n\\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\quad  i = 1, 2, \\dots, n \\]\n其中 \\(Y_i\\) 表示第i个响应值，\\(X_i\\) 表示第i个特征值，\\(\\beta_0\\) 和 \\(\\beta_1\\) 是未知的常数（通常称为系数或参数），分别表示回归线的截距和斜率，\\(\\epsilon_i\\) 表示噪声或随机误差。假设误差服从均值为零、方差为 \\(\\sigma^2\\) 的正态分布。由于随机误差的均值为零（即 \\(E(\\epsilon) = 0)\\)），线性回归实际上是估计条件均值的问题：\n\\[ E(Y_i | X_i) = \\beta_0 + \\beta_1 X_i\\]\n系数的解释是基于平均或均值响应的。例如，截距 \\(\\beta_0\\) 表示当 X = 0 时的平均响应变量值（通常没有实际意义或不被关注，有时被称为偏置项）。斜率 \\(\\beta_1\\) 表示 X 每增加一个单位，平均响应的增加量（即变化率）。\n\n\n最常用的方法是使用普通最小二乘（OLS）法估计\\(\\beta_0\\) 和 \\(\\beta_1\\)，最小二乘法的准则是通过最小化残差平方和（RSS）来找到最佳拟合线：\n\\[ RSS(\\beta_0, \\beta_1) = \\sum_{i=1}^n [Y_i - (\\beta_0 + \\beta_1 X_i)]^2 = \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2 \\]\n\\(\\beta_0\\) 和 \\(\\beta_1\\) 的最小二乘估计分别表示为 \\(\\hat{\\beta}_0\\) 和 \\(\\hat{\\beta}_1\\)。一旦获得这些估计值，可以使用估计的回归方程生成预测值，例如在 \\(X = X_{新}\\) 处：\n$ _{新} = _0 + 1 X{新} $\n以Ames住房数据为例，建模房屋地上总居住面积（Gr_Liv_Area）与售价（Sale_Price）之间的线性关系。\n\nmodel1 &lt;- lm(Sale_Price ~ Gr_Liv_Area, data = ames_train)\n\n\n\n\n\n\n\n\n\n\n使用 coef() 函数可以提取模型的估计系数，使用 summary() 可以获取模型结果的详细报告：\n\nsummary(model1)\n## \n## Call:\n## lm(formula = Sale_Price ~ Gr_Liv_Area, data = ames_train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -474682  -30794   -1678   23353  328183 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 15938.173   3851.853   4.138 3.65e-05 ***\n## Gr_Liv_Area   109.667      2.421  45.303  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 56790 on 2047 degrees of freedom\n## Multiple R-squared:  0.5007, Adjusted R-squared:  0.5004 \n## F-statistic:  2052 on 1 and 2047 DF,  p-value: &lt; 2.2e-16\n\n模型估计系数为 \\(\\hat{\\beta}_0 = 15938.17\\) 和 \\(\\hat{\\beta}_1 = 109.67\\)。解释为：地上居住面积每增加一平方英尺，平均售价增加109.66美元。\n另一重要的估计量是误差方差 \\(\\sigma^2\\)，通常假设其服从正态分布，由最大似然估计获得。\n\\[ \\hat{\\sigma}^2 = \\frac{1}{n-p} \\sum_{i=1}^n r_i^2, \\]\n其中 \\(r_i = (Y_i - \\hat{Y}_i)\\) 称为第 i 个残差（即第 i 个观测响应值与预测响应值之间的差）。 \\(\\hat{\\sigma}^2\\) 也称为均方误差（MSE），其平方根称为RMSE。可以使用 sigma() 函数提取线性模型的RMSE。通常，这些误差指标在单独的验证集上或通过交叉验证来计算；但也可以在训练模型的训练数据上计算。\n\nsigma(model1)    # RMSE\n## [1] 56787.94\n\nsigma(model1)^2  # MSE\n## [1] 3224869786\n\n\n\n\n\\(\\beta_0\\) 和 \\(\\beta_1\\) 参数估计的变异性通过其标准误差（SE）来衡量。在 summary() 输出的Std. Error列中显示。同时还有报告 (t) 统计量和p值。还可以使用 confint() 函数获取每个回归系数置信区间。\n\nconfint(model1, level = 0.95)\n##                2.5 %     97.5 %\n## (Intercept) 8384.213 23492.1336\n## Gr_Liv_Area  104.920   114.4149\n\n解释是有95%信心，地上居住面积每增加一平方英尺，平均售价在104.92美元至114.41美元之间增加。\n\n\n\n\n现实中通常有多个预测变量，可以将简单线性回归模型扩展为包括多个预测变量的多元线性回归（MLR）模型。对于两个预测变量，MLR模型为：\n\\[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon, \\]\n其中 \\(X_1\\) 和 \\(X_2\\) 是感兴趣的特征。\n\n(model2 &lt;- lm(Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train))\n## \n## Call:\n## lm(formula = Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train)\n## \n## Coefficients:\n## (Intercept)  Gr_Liv_Area   Year_Built  \n##  -2.103e+06    9.383e+01    1.087e+03\n\n或者，可以使用 update() 函数更新 model1 中使用的模型公式。新公式可以使用 . 作为简写，表示保留公式左侧或右侧的所有内容，并使用 + 或 - 分别添加或移除原始模型中的项。\n\n(model2 &lt;- update(model1, . ~ . + Year_Built))\n## \n## Call:\n## lm(formula = Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train)\n## \n## Coefficients:\n## (Intercept)  Gr_Liv_Area   Year_Built  \n##  -2.103e+06    9.383e+01    1.087e+03\n\n回归系数的LS估计为 \\(\\hat{\\beta}_1 = 93.8\\) 和 \\(\\hat{\\beta}_2 = 1087\\)。解释为在保持房屋建造年份不变的情况下，地上居住面积每增加一平方英尺，平均售价增加93.8美元。同样，在保持地上居住面积不变的情况下，房屋建造年份每晚一年，平均售价增加约1087美元。\n在线性回归中，交互效应可以通过特征的乘积（即 \\(X_1 \\times X_2\\)）来获取。\n\nlm(Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area:Year_Built, data = ames_train)\n## \n## Call:\n## lm(formula = Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area:Year_Built, \n##     data = ames_train)\n## \n## Coefficients:\n##            (Intercept)             Gr_Liv_Area              Year_Built  \n##             -8.105e+05              -7.285e+02               4.309e+02  \n## Gr_Liv_Area:Year_Built  \n##              4.168e-01\n\n多元线性模型可以包含任意多的预测变量，只要数据的行数多于参数数量即可。具有 (p) 个不同预测变量的通用多元线性回归模型为：\n\\[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\epsilon, \\]\n其中 \\(X_i\\)（对于 \\(i = 1, 2, \\dots, p\\)）是预测变量。\n\n# 包含所有可能的主效应\nmodel3 &lt;- lm(Sale_Price ~ ., data = ames_train)\n\n# 以整洁的数据框形式打印估计系数\nbroom::tidy(model3)\n## # A tibble: 300 × 5\n##    term                                     estimate std.error statistic p.value\n##    &lt;chr&gt;                                       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n##  1 (Intercept)                               -2.73e7 11016450.   -2.47    0.0134\n##  2 MS_SubClassOne_Story_1945_and_Older        3.90e3     3575.    1.09    0.275 \n##  3 MS_SubClassOne_Story_with_Finished_Atti…  -5.39e3    12164.   -0.443   0.658 \n##  4 MS_SubClassOne_and_Half_Story_Unfinishe…  -4.41e2    13942.   -0.0316  0.975 \n##  5 MS_SubClassOne_and_Half_Story_Finished_…   1.04e3     7250.    0.143   0.886 \n##  6 MS_SubClassTwo_Story_1946_and_Newer       -6.67e3     5510.   -1.21    0.226 \n##  7 MS_SubClassTwo_Story_1945_and_Older        1.57e3     6074.    0.259   0.795 \n##  8 MS_SubClassTwo_and_Half_Story_All_Ages     3.41e3    10149.    0.336   0.737 \n##  9 MS_SubClassSplit_or_Multilevel            -6.67e3    11673.   -0.571   0.568 \n## 10 MS_SubClassSplit_Foyer                     1.49e3     7512.    0.199   0.843 \n## # ℹ 290 more rows\n\n\n\n\n上面已经为Ames住房数据拟合了三个主效应模型：单一预测变量、两个预测变量和所有可能预测变量。哪个模型是最佳的？可以使用前面介绍的RMSE指标和交叉验证来确定最佳模型。\n\n# 使用10折交叉验证训练模型\nset.seed(123)  # 为了可重现性\n(cv_model1 &lt;- train(\n  form = Sale_Price ~ Gr_Liv_Area, \n  data = ames_train, \n  method = \"lm\",\n  trControl = trainControl(method = \"cv\", number = 10)\n))\n## Linear Regression \n## \n## 2049 samples\n##    1 predictor\n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 1843, 1844, 1844, 1844, 1844, 1844, ... \n## Resampling results:\n## \n##   RMSE      Rsquared  MAE     \n##   56644.76  0.510273  38851.99\n## \n## Tuning parameter 'intercept' was held constant at a value of TRUE\n\n交叉验证的RMSE为56644.76美元（这是10个交叉验证折的平均RMSE）。当应用于未见过的数据时，该模型的预测平均与实际售价相差约56644.76美元。\n\n# 模型2交叉验证\nset.seed(123)\ncv_model2 &lt;- train(\n  Sale_Price ~ Gr_Liv_Area + Year_Built, \n  data = ames_train, \n  method = \"lm\",\n  trControl = trainControl(method = \"cv\", number = 10)\n)\n\n# 模型3交叉验证\nset.seed(123)\ncv_model3 &lt;- train(\n  Sale_Price ~ ., \n  data = ames_train, \n  method = \"lm\",\n  trControl = trainControl(method = \"cv\", number = 10)\n)\n\n# 提取样本外性能指标\nsummary(resamples(list(\n  model1 = cv_model1, \n  model2 = cv_model2, \n  model3 = cv_model3\n)))\n## \n## Call:\n## summary.resamples(object = resamples(list(model1 = cv_model1, model2\n##  = cv_model2, model3 = cv_model3)))\n## \n## Models: model1, model2, model3 \n## Number of resamples: 10 \n## \n## MAE \n##            Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\n## model1 34076.73 37656.23 39785.18 38851.99 40200.92 42058.68    0\n## model2 29227.14 30885.17 32003.59 31695.48 32710.41 33942.26    0\n## model3 14740.86 15377.88 17564.13 17559.41 19344.23 21180.02    0\n## \n## RMSE \n##            Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\n## model1 45604.65 55896.58 57000.74 56644.76 59544.08 66198.59    0\n## model2 37174.26 42650.00 46869.84 46865.68 51155.14 55780.47    0\n## model3 20737.09 24858.60 40515.19 41691.74 55969.21 69879.47    0\n## \n## Rsquared \n##             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\n## model1 0.4230788 0.4621034 0.5090642 0.5102730 0.5681246 0.5996400    0\n## model2 0.5829425 0.6075293 0.6865871 0.6631008 0.6976664 0.7254572    0\n## model3 0.5241423 0.6343368 0.7483874 0.7578796 0.9137443 0.9328876    0\n\n完整模型的交叉验证RMSE小于两个预测变量的模型和单预测变量模型。因此，包含所有可能主效应的模型表现最佳。\n\n\n\n如前所述，线性回归因其系数易于解释而成为一种流行的建模工具。然而，线性回归依赖于几个强假设，当我们在模型中包含更多预测变量时，这些假设常常被违反。违反这些假设可能导致对系数的错误解释和预测结果的偏差。\n\n线性关系：线性回归假设预测变量与响应变量之间存在线性关系。然而，非线性关系可以通过对响应变量和/或预测变量进行变换使其变为线性（或近似线性）。例如，左图显示了售价与房屋建造年份之间存在的非线性关系。然而，通过对售价进行对数变换，可以实现近似线性的关系，尽管对于较老的房屋仍存在一些非线性。\n\n\n\n\n\n\n\n\n\n\n\n残差方差恒定：线性回归假设误差项（\\(\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_p\\)）的方差是恒定的（此假设称为同方差性）。如果误差方差不恒定，系数的p值和置信区间将失效。与线性关系假设类似，非恒定方差通常可以通过变量变换或添加额外的预测变量来解决。例如，下图显示了model1和model3的残差与预测值的关系。model1显示出典型的异方差问题，表现为锥形模式。然而，model3的残差方差似乎接近相等。\n\n\n\n\n\n\n\n\n\n\n\n无自相关：线性回归假设误差是独立且不相关的。如果误差之间实际上存在相关性，那么系数的估计标准误差将出现偏差，导致预测区间的宽度比实际应有的更窄。左图显示了model1的残差（y轴）与观测ID（x轴）的关系。存在一个明显的模式，表明\\(\\epsilon_1\\)的信息提供了关于\\(\\epsilon_2\\)的信息。\n\n这种模式是由于数据按社区排序，而我们在该模型中未考虑社区因素。因此，同一社区内房屋的残差是相关的（同一社区的房屋通常大小相似，且特征往往类似）。由于model3（右图）包含了社区预测变量，误差中的相关性得到了减少。\n\n\n\n\n\n\n\n\n\n\n观测数多于预测变量：但当特征数量超过观测数量（\\(p &gt; n\\)）时，无法获得OLS估计。解决此问题的一种方法是逐一移除变量，直到\\(p &lt; n\\)。但是比较繁琐，后面介绍正则化回归，作为OLS的替代方法，可用于\\(p &gt; n\\)的情况。\n没有或较低的多重共线性：共线性是指两个或多个预测变量彼此密切相关的情况。共线性的存在会对OLS造成问题，因为很难区分共线性变量对响应的个别影响。事实上，共线性可能导致预测变量在实际上显著时被认为在统计上不显著。这显然会导致对系数的错误解释，并使识别重要预测变量变得困难。\n\n在Ames数据中，例如，Garage_Area和Garage_Cars这两个变量的相关性较强，且两者都与响应变量（Sale_Price）强相关。在包含这两个变量的完整模型中，我们看到Garage_Cars在统计上不显著，而Garage_Area显著：\n\n# 包含两个高度相关变量的拟合\nsummary(cv_model3) %&gt;%\n  broom::tidy() %&gt;%\n  filter(term %in% c(\"Garage_Area\", \"Garage_Cars\"))\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 Garage_Cars   3151.    1733.        1.82  0.0693\n## 2 Garage_Area     11.9      5.69      2.10  0.0359\n\n然而，移除Garage_Area后重新拟合完整模型，Garage_Cars的系数估计值将近增加了一倍，并变得在统计上显著：\n\n# 不包含Garage_Area的模型\nset.seed(123)\nmod_wo_Garage_Area &lt;- train(\n  Sale_Price ~ ., \n  data = select(ames_train, -Garage_Area), \n  method = \"lm\",\n  trControl = trainControl(method = \"cv\", number = 10)\n)\n\nsummary(mod_wo_Garage_Area) %&gt;%\n  broom::tidy() %&gt;%\n  filter(term == \"Garage_Cars\")\n## # A tibble: 1 × 5\n##   term        estimate std.error statistic    p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n## 1 Garage_Cars    5765.     1207.      4.78 0.00000194\n\n这反映了预测变量之间的关系导致的线性回归模型的不稳定性；这种不稳定性也会直接传递到模型预测中。一种选择是手动移除有问题的预测变量（逐一移除），直到所有变量的成对相关性低于某个预定阈值。然而，当预测变量数量较多时，会很繁琐。此外，当一个特征与两个或更多特征线性相关时，多重共线性可能会出现。在这些情况下，手动移除特定预测变量可能不可行，需要采用主成分回归和偏相关的降维方法。\n\n\n\n主成分分析（PCA）可用于将相关变量表示为较少数量的不相关特征（称为主成分），并将这些主成分用作线性回归模型中的预测变量。这种两步过程称为主成分回归（PCR）。\n在train()中指定method = \"pcr\"，即可在拟合回归模型之前对所有数值预测变量执行PCA。通常仅使用主成分中的一小部分作为预测变量，就可以显著提高性能。因此，可以将提取主成分的数量视为调参参数。以下代码执行了使用1到200个主成分的交叉验证PCR。可以看到，只提取五个主成分进入回归方程就可以显著降低预测误差，之后逐渐减少。\n\n# 对PCR模型执行10折交叉验证，调参主成分数量，从1到100\nset.seed(123)\ncv_model_pcr &lt;- train(\n  Sale_Price ~ ., \n  data = ames_train, \n  method = \"pcr\",\n  trControl = trainControl(method = \"cv\", number = 10),\n  preProcess = c(\"zv\", \"center\", \"scale\"),\n  tuneLength = 200\n)\n\n# RMSE最低的模型\ncv_model_pcr$bestTune\n##     ncomp\n## 195   195\n\n\n# RMSE最低的模型结果\ncv_model_pcr$results %&gt;%\n  dplyr::filter(ncomp == pull(cv_model_pcr$bestTune))\n##   ncomp     RMSE  Rsquared     MAE   RMSESD RsquaredSD    MAESD\n## 1   195 31807.17 0.8435625 19228.7 7980.335 0.07041646 1592.957\n\n\n# 绘制交叉验证的RMSE\nggplot(cv_model_pcr)\n\n\n\n\n\n\n\n\n通过使用PCR控制多重共线性，模型预测准确性相比之前获得的线性模型有了显著提升（交叉验证RMSE从约41691.74美元减少到31807.17美元），甚至低于之前的k近邻模型（33582.40）。\n值得注意的是，由于PCA在选择主成分时不考虑响应变量。因此，生成的新预测变量（主成分）是以减少预测变量空间的变异性为原则，并非最大化与响应变量间的关系。如果PCA处理的这种变异性恰好与响应变量的变异性相关，那么PCR会更容易识别出预测关系。但是，如果预测变量空间的变异性与响应变量的变异性无关，那么PCR可能难以识别实际存在的预测关系，甚至会导致预测准确性下降。\n\n\n\n偏最小二乘（PLS）可以视为一种有监督的降维过程。与PCR类似，PLS也为回归模型构建输入的线性组合，但与PCR不同的是，它利用了响应变量来辅助构建主成分，所以不仅能捕捉原始特征中大部分信息，而且能生成与响应变量相关的新特征。\n\n\n\nPCR与PLS之间的差别\n\n\n\n\n\n\n\n\n\n\n\n上图显示使用PCR时前两个主成分与响应变量关系很小，而使用PLS时前两个主成分与响应的关联性强得多。\nPLS是通过\\(y\\)对相应\\(x_j\\)的简单线性回归模型的系数（回归系数与相关系数成正比）来计算第一个主成分（\\(z_1\\)），所以PLS对与响应变量相关性最强的变量赋予最高权重。接下来，计算第二个主成分（\\(z_2\\)）时，先将每个变量回归到\\(z_1\\)上，用每个回归的残差（每个预测变量未被第一个主成分解释的剩余信息）来代替每个预测变量值，同样依据将残差与响应变量的相关性赋予权重计算第二个主成分。以此类推直到计算出所有\\(m\\)个主成分。更详细的内容在主成分分析降维部分介绍。\n与PCR类似，在train()中更改method参数可以拟合PLS模型。与PCR一样，使用的主成分数量是一个调参参数，由最大化预测准确性的模型确定。\n下面使用1到30个主成分的交叉验证PLS。可以看出，与PCR相比，预测误差略有下降，但是使用更少的主成分就达到了最小RMSE，因为这些主成分受响应变量的引导。\n\n# 对PLS模型执行10折交叉验证，调参主成分数量，从1到30\nset.seed(123)\ncv_model_pls &lt;- train(\n  Sale_Price ~ ., \n  data = ames_train, \n  method = \"pls\",\n  trControl = trainControl(method = \"cv\", number = 10),\n  preProcess = c(\"zv\", \"center\", \"scale\"),\n  tuneLength = 30\n)\n\n# RMSE最低的模型\ncv_model_pls$bestTune\n##   ncomp\n## 3     3\n\n\n# RMSE最低的模型结果\ncv_model_pls$results %&gt;%\n  dplyr::filter(ncomp == pull(cv_model_pls$bestTune))\n##   ncomp     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD\n## 1     3 31077.42 0.8493617 19137.51 7878.004 0.07079317 1784.467\n\n# 绘制交叉验证的RMSE\nggplot(cv_model_pls)\n\n\n\n\n\n\n\n\n\n\n\n一旦找到最大化预测准确性的模型，下一个目标是解释模型结构。解释模型结构需要先确定最具影响力的变量，可以使用vip::vip()提取并绘制最重要的变量。重要性度量从100（最重要）归一化到0（最不重要）。下图展示了最重要的四个变量分别是Gr_Liv_Area、First_Flr_SF、Garage_Cars和Garage_Area。\n\nvip(cv_model_pls, num_features = 20, method = \"model\")\n\n\n\n\n\n\n\n\n同时，由于线性回归模型假设单调线性关系。可以针对最重要的变量构建部分依赖图（PDPs）。PDPs绘制了指定特征在其边际分布上变化时平均预测值（\\(\\hat{y}\\)）的变化。当存在非线性关系时，PDPs变得更加有用。pdp包提供了方便的函数来计算和绘制PDPs。\n\np1 &lt;- pdp::partial(cv_model_pls, pred.var = \"Gr_Liv_Area\", grid.resolution = 20) %&gt;% \n  autoplot() +\n  scale_y_continuous(limits = c(0, 300000), labels = scales::dollar)\n\np2 &lt;- pdp::partial(cv_model_pls, pred.var = \"First_Flr_SF\", grid.resolution = 20) %&gt;% \n  autoplot() +\n  scale_y_continuous(limits = c(0, 300000), labels = scales::dollar)\n\np3 &lt;- pdp::partial(cv_model_pls, pred.var = \"Garage_Cars\", grid.resolution = 20) %&gt;% \n  autoplot() +\n  scale_y_continuous(limits = c(0, 300000), labels = scales::dollar)\n\np4 &lt;- pdp::partial(cv_model_pls, pred.var = \"Garage_Area\", grid.resolution = 4) %&gt;% \n  autoplot() +\n  scale_y_continuous(limits = c(0, 300000), labels = scales::dollar)\n\ngridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2)\n\n\n\n\n\n\n\n\n最重要的四个预测变量均与售价呈正相关；然而，我们看到最重要预测变量的斜率（\\(\\hat{\\beta}_i\\)）最陡，对于较不重要的变量逐渐减小。"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#工具和数据",
    "href": "bigdata/machlearn/regression.html#工具和数据",
    "title": "有监督学习：回归模型",
    "section": "",
    "text": "# 辅助包\nlibrary(dplyr)    # 用于数据操作\nlibrary(ggplot2)  # 用于出色的图形展示\n\n# 建模包\nlibrary(caret)    # 用于交叉验证等\n\n# 模型可解释性包\nlibrary(vip)      # 变量重要性\n\n\names &lt;- AmesHousing::make_ames()\n\nlibrary(rsample)\n# 分层抽样划分训练集和测试集数据\nset.seed(123)\nsplit  &lt;- initial_split(ames, prop = 0.7, strata = \"Sale_Price\")\names_train  &lt;- training(split)\names_test   &lt;- testing(split)"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#简单线性回归",
    "href": "bigdata/machlearn/regression.html#简单线性回归",
    "title": "有监督学习：回归模型",
    "section": "",
    "text": "简单线性回归（SLR）假设两个连续变量（例如X和Y之间的统计关系（至少近似）是线性的：\n\\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\quad  i = 1, 2, \\dots, n \\]\n其中 \\(Y_i\\) 表示第i个响应值，\\(X_i\\) 表示第i个特征值，\\(\\beta_0\\) 和 \\(\\beta_1\\) 是未知的常数（通常称为系数或参数），分别表示回归线的截距和斜率，\\(\\epsilon_i\\) 表示噪声或随机误差。假设误差服从均值为零、方差为 \\(\\sigma^2\\) 的正态分布。由于随机误差的均值为零（即 \\(E(\\epsilon) = 0)\\)），线性回归实际上是估计条件均值的问题：\n\\[ E(Y_i | X_i) = \\beta_0 + \\beta_1 X_i\\]\n系数的解释是基于平均或均值响应的。例如，截距 \\(\\beta_0\\) 表示当 X = 0 时的平均响应变量值（通常没有实际意义或不被关注，有时被称为偏置项）。斜率 \\(\\beta_1\\) 表示 X 每增加一个单位，平均响应的增加量（即变化率）。\n\n\n最常用的方法是使用普通最小二乘（OLS）法估计\\(\\beta_0\\) 和 \\(\\beta_1\\)，最小二乘法的准则是通过最小化残差平方和（RSS）来找到最佳拟合线：\n\\[ RSS(\\beta_0, \\beta_1) = \\sum_{i=1}^n [Y_i - (\\beta_0 + \\beta_1 X_i)]^2 = \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2 \\]\n\\(\\beta_0\\) 和 \\(\\beta_1\\) 的最小二乘估计分别表示为 \\(\\hat{\\beta}_0\\) 和 \\(\\hat{\\beta}_1\\)。一旦获得这些估计值，可以使用估计的回归方程生成预测值，例如在 \\(X = X_{新}\\) 处：\n$ _{新} = _0 + 1 X{新} $\n以Ames住房数据为例，建模房屋地上总居住面积（Gr_Liv_Area）与售价（Sale_Price）之间的线性关系。\n\nmodel1 &lt;- lm(Sale_Price ~ Gr_Liv_Area, data = ames_train)\n\n\n\n\n\n\n\n\n\n\n使用 coef() 函数可以提取模型的估计系数，使用 summary() 可以获取模型结果的详细报告：\n\nsummary(model1)\n## \n## Call:\n## lm(formula = Sale_Price ~ Gr_Liv_Area, data = ames_train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -474682  -30794   -1678   23353  328183 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 15938.173   3851.853   4.138 3.65e-05 ***\n## Gr_Liv_Area   109.667      2.421  45.303  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 56790 on 2047 degrees of freedom\n## Multiple R-squared:  0.5007, Adjusted R-squared:  0.5004 \n## F-statistic:  2052 on 1 and 2047 DF,  p-value: &lt; 2.2e-16\n\n模型估计系数为 \\(\\hat{\\beta}_0 = 15938.17\\) 和 \\(\\hat{\\beta}_1 = 109.67\\)。解释为：地上居住面积每增加一平方英尺，平均售价增加109.66美元。\n另一重要的估计量是误差方差 \\(\\sigma^2\\)，通常假设其服从正态分布，由最大似然估计获得。\n\\[ \\hat{\\sigma}^2 = \\frac{1}{n-p} \\sum_{i=1}^n r_i^2, \\]\n其中 \\(r_i = (Y_i - \\hat{Y}_i)\\) 称为第 i 个残差（即第 i 个观测响应值与预测响应值之间的差）。 \\(\\hat{\\sigma}^2\\) 也称为均方误差（MSE），其平方根称为RMSE。可以使用 sigma() 函数提取线性模型的RMSE。通常，这些误差指标在单独的验证集上或通过交叉验证来计算；但也可以在训练模型的训练数据上计算。\n\nsigma(model1)    # RMSE\n## [1] 56787.94\n\nsigma(model1)^2  # MSE\n## [1] 3224869786\n\n\n\n\n\\(\\beta_0\\) 和 \\(\\beta_1\\) 参数估计的变异性通过其标准误差（SE）来衡量。在 summary() 输出的Std. Error列中显示。同时还有报告 (t) 统计量和p值。还可以使用 confint() 函数获取每个回归系数置信区间。\n\nconfint(model1, level = 0.95)\n##                2.5 %     97.5 %\n## (Intercept) 8384.213 23492.1336\n## Gr_Liv_Area  104.920   114.4149\n\n解释是有95%信心，地上居住面积每增加一平方英尺，平均售价在104.92美元至114.41美元之间增加。"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#多元线性回归",
    "href": "bigdata/machlearn/regression.html#多元线性回归",
    "title": "有监督学习：回归模型",
    "section": "",
    "text": "现实中通常有多个预测变量，可以将简单线性回归模型扩展为包括多个预测变量的多元线性回归（MLR）模型。对于两个预测变量，MLR模型为：\n\\[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon, \\]\n其中 \\(X_1\\) 和 \\(X_2\\) 是感兴趣的特征。\n\n(model2 &lt;- lm(Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train))\n## \n## Call:\n## lm(formula = Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train)\n## \n## Coefficients:\n## (Intercept)  Gr_Liv_Area   Year_Built  \n##  -2.103e+06    9.383e+01    1.087e+03\n\n或者，可以使用 update() 函数更新 model1 中使用的模型公式。新公式可以使用 . 作为简写，表示保留公式左侧或右侧的所有内容，并使用 + 或 - 分别添加或移除原始模型中的项。\n\n(model2 &lt;- update(model1, . ~ . + Year_Built))\n## \n## Call:\n## lm(formula = Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train)\n## \n## Coefficients:\n## (Intercept)  Gr_Liv_Area   Year_Built  \n##  -2.103e+06    9.383e+01    1.087e+03\n\n回归系数的LS估计为 \\(\\hat{\\beta}_1 = 93.8\\) 和 \\(\\hat{\\beta}_2 = 1087\\)。解释为在保持房屋建造年份不变的情况下，地上居住面积每增加一平方英尺，平均售价增加93.8美元。同样，在保持地上居住面积不变的情况下，房屋建造年份每晚一年，平均售价增加约1087美元。\n在线性回归中，交互效应可以通过特征的乘积（即 \\(X_1 \\times X_2\\)）来获取。\n\nlm(Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area:Year_Built, data = ames_train)\n## \n## Call:\n## lm(formula = Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area:Year_Built, \n##     data = ames_train)\n## \n## Coefficients:\n##            (Intercept)             Gr_Liv_Area              Year_Built  \n##             -8.105e+05              -7.285e+02               4.309e+02  \n## Gr_Liv_Area:Year_Built  \n##              4.168e-01\n\n多元线性模型可以包含任意多的预测变量，只要数据的行数多于参数数量即可。具有 (p) 个不同预测变量的通用多元线性回归模型为：\n\\[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\epsilon, \\]\n其中 \\(X_i\\)（对于 \\(i = 1, 2, \\dots, p\\)）是预测变量。\n\n# 包含所有可能的主效应\nmodel3 &lt;- lm(Sale_Price ~ ., data = ames_train)\n\n# 以整洁的数据框形式打印估计系数\nbroom::tidy(model3)\n## # A tibble: 300 × 5\n##    term                                     estimate std.error statistic p.value\n##    &lt;chr&gt;                                       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n##  1 (Intercept)                               -2.73e7 11016450.   -2.47    0.0134\n##  2 MS_SubClassOne_Story_1945_and_Older        3.90e3     3575.    1.09    0.275 \n##  3 MS_SubClassOne_Story_with_Finished_Atti…  -5.39e3    12164.   -0.443   0.658 \n##  4 MS_SubClassOne_and_Half_Story_Unfinishe…  -4.41e2    13942.   -0.0316  0.975 \n##  5 MS_SubClassOne_and_Half_Story_Finished_…   1.04e3     7250.    0.143   0.886 \n##  6 MS_SubClassTwo_Story_1946_and_Newer       -6.67e3     5510.   -1.21    0.226 \n##  7 MS_SubClassTwo_Story_1945_and_Older        1.57e3     6074.    0.259   0.795 \n##  8 MS_SubClassTwo_and_Half_Story_All_Ages     3.41e3    10149.    0.336   0.737 \n##  9 MS_SubClassSplit_or_Multilevel            -6.67e3    11673.   -0.571   0.568 \n## 10 MS_SubClassSplit_Foyer                     1.49e3     7512.    0.199   0.843 \n## # ℹ 290 more rows"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#评估模型准确性",
    "href": "bigdata/machlearn/regression.html#评估模型准确性",
    "title": "有监督学习：回归模型",
    "section": "",
    "text": "上面已经为Ames住房数据拟合了三个主效应模型：单一预测变量、两个预测变量和所有可能预测变量。哪个模型是最佳的？可以使用前面介绍的RMSE指标和交叉验证来确定最佳模型。\n\n# 使用10折交叉验证训练模型\nset.seed(123)  # 为了可重现性\n(cv_model1 &lt;- train(\n  form = Sale_Price ~ Gr_Liv_Area, \n  data = ames_train, \n  method = \"lm\",\n  trControl = trainControl(method = \"cv\", number = 10)\n))\n## Linear Regression \n## \n## 2049 samples\n##    1 predictor\n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 1843, 1844, 1844, 1844, 1844, 1844, ... \n## Resampling results:\n## \n##   RMSE      Rsquared  MAE     \n##   56644.76  0.510273  38851.99\n## \n## Tuning parameter 'intercept' was held constant at a value of TRUE\n\n交叉验证的RMSE为56644.76美元（这是10个交叉验证折的平均RMSE）。当应用于未见过的数据时，该模型的预测平均与实际售价相差约56644.76美元。\n\n# 模型2交叉验证\nset.seed(123)\ncv_model2 &lt;- train(\n  Sale_Price ~ Gr_Liv_Area + Year_Built, \n  data = ames_train, \n  method = \"lm\",\n  trControl = trainControl(method = \"cv\", number = 10)\n)\n\n# 模型3交叉验证\nset.seed(123)\ncv_model3 &lt;- train(\n  Sale_Price ~ ., \n  data = ames_train, \n  method = \"lm\",\n  trControl = trainControl(method = \"cv\", number = 10)\n)\n\n# 提取样本外性能指标\nsummary(resamples(list(\n  model1 = cv_model1, \n  model2 = cv_model2, \n  model3 = cv_model3\n)))\n## \n## Call:\n## summary.resamples(object = resamples(list(model1 = cv_model1, model2\n##  = cv_model2, model3 = cv_model3)))\n## \n## Models: model1, model2, model3 \n## Number of resamples: 10 \n## \n## MAE \n##            Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\n## model1 34076.73 37656.23 39785.18 38851.99 40200.92 42058.68    0\n## model2 29227.14 30885.17 32003.59 31695.48 32710.41 33942.26    0\n## model3 14740.86 15377.88 17564.13 17559.41 19344.23 21180.02    0\n## \n## RMSE \n##            Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\n## model1 45604.65 55896.58 57000.74 56644.76 59544.08 66198.59    0\n## model2 37174.26 42650.00 46869.84 46865.68 51155.14 55780.47    0\n## model3 20737.09 24858.60 40515.19 41691.74 55969.21 69879.47    0\n## \n## Rsquared \n##             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\n## model1 0.4230788 0.4621034 0.5090642 0.5102730 0.5681246 0.5996400    0\n## model2 0.5829425 0.6075293 0.6865871 0.6631008 0.6976664 0.7254572    0\n## model3 0.5241423 0.6343368 0.7483874 0.7578796 0.9137443 0.9328876    0\n\n完整模型的交叉验证RMSE小于两个预测变量的模型和单预测变量模型。因此，包含所有可能主效应的模型表现最佳。"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#模型注意事项",
    "href": "bigdata/machlearn/regression.html#模型注意事项",
    "title": "有监督学习：回归模型",
    "section": "",
    "text": "如前所述，线性回归因其系数易于解释而成为一种流行的建模工具。然而，线性回归依赖于几个强假设，当我们在模型中包含更多预测变量时，这些假设常常被违反。违反这些假设可能导致对系数的错误解释和预测结果的偏差。\n\n线性关系：线性回归假设预测变量与响应变量之间存在线性关系。然而，非线性关系可以通过对响应变量和/或预测变量进行变换使其变为线性（或近似线性）。例如，左图显示了售价与房屋建造年份之间存在的非线性关系。然而，通过对售价进行对数变换，可以实现近似线性的关系，尽管对于较老的房屋仍存在一些非线性。\n\n\n\n\n\n\n\n\n\n\n\n残差方差恒定：线性回归假设误差项（\\(\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_p\\)）的方差是恒定的（此假设称为同方差性）。如果误差方差不恒定，系数的p值和置信区间将失效。与线性关系假设类似，非恒定方差通常可以通过变量变换或添加额外的预测变量来解决。例如，下图显示了model1和model3的残差与预测值的关系。model1显示出典型的异方差问题，表现为锥形模式。然而，model3的残差方差似乎接近相等。\n\n\n\n\n\n\n\n\n\n\n\n无自相关：线性回归假设误差是独立且不相关的。如果误差之间实际上存在相关性，那么系数的估计标准误差将出现偏差，导致预测区间的宽度比实际应有的更窄。左图显示了model1的残差（y轴）与观测ID（x轴）的关系。存在一个明显的模式，表明\\(\\epsilon_1\\)的信息提供了关于\\(\\epsilon_2\\)的信息。\n\n这种模式是由于数据按社区排序，而我们在该模型中未考虑社区因素。因此，同一社区内房屋的残差是相关的（同一社区的房屋通常大小相似，且特征往往类似）。由于model3（右图）包含了社区预测变量，误差中的相关性得到了减少。\n\n\n\n\n\n\n\n\n\n\n观测数多于预测变量：但当特征数量超过观测数量（\\(p &gt; n\\)）时，无法获得OLS估计。解决此问题的一种方法是逐一移除变量，直到\\(p &lt; n\\)。但是比较繁琐，后面介绍正则化回归，作为OLS的替代方法，可用于\\(p &gt; n\\)的情况。\n没有或较低的多重共线性：共线性是指两个或多个预测变量彼此密切相关的情况。共线性的存在会对OLS造成问题，因为很难区分共线性变量对响应的个别影响。事实上，共线性可能导致预测变量在实际上显著时被认为在统计上不显著。这显然会导致对系数的错误解释，并使识别重要预测变量变得困难。\n\n在Ames数据中，例如，Garage_Area和Garage_Cars这两个变量的相关性较强，且两者都与响应变量（Sale_Price）强相关。在包含这两个变量的完整模型中，我们看到Garage_Cars在统计上不显著，而Garage_Area显著：\n\n# 包含两个高度相关变量的拟合\nsummary(cv_model3) %&gt;%\n  broom::tidy() %&gt;%\n  filter(term %in% c(\"Garage_Area\", \"Garage_Cars\"))\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 Garage_Cars   3151.    1733.        1.82  0.0693\n## 2 Garage_Area     11.9      5.69      2.10  0.0359\n\n然而，移除Garage_Area后重新拟合完整模型，Garage_Cars的系数估计值将近增加了一倍，并变得在统计上显著：\n\n# 不包含Garage_Area的模型\nset.seed(123)\nmod_wo_Garage_Area &lt;- train(\n  Sale_Price ~ ., \n  data = select(ames_train, -Garage_Area), \n  method = \"lm\",\n  trControl = trainControl(method = \"cv\", number = 10)\n)\n\nsummary(mod_wo_Garage_Area) %&gt;%\n  broom::tidy() %&gt;%\n  filter(term == \"Garage_Cars\")\n## # A tibble: 1 × 5\n##   term        estimate std.error statistic    p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n## 1 Garage_Cars    5765.     1207.      4.78 0.00000194\n\n这反映了预测变量之间的关系导致的线性回归模型的不稳定性；这种不稳定性也会直接传递到模型预测中。一种选择是手动移除有问题的预测变量（逐一移除），直到所有变量的成对相关性低于某个预定阈值。然而，当预测变量数量较多时，会很繁琐。此外，当一个特征与两个或更多特征线性相关时，多重共线性可能会出现。在这些情况下，手动移除特定预测变量可能不可行，需要采用主成分回归和偏相关的降维方法。"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#主成分回归",
    "href": "bigdata/machlearn/regression.html#主成分回归",
    "title": "有监督学习：回归模型",
    "section": "",
    "text": "主成分分析（PCA）可用于将相关变量表示为较少数量的不相关特征（称为主成分），并将这些主成分用作线性回归模型中的预测变量。这种两步过程称为主成分回归（PCR）。\n在train()中指定method = \"pcr\"，即可在拟合回归模型之前对所有数值预测变量执行PCA。通常仅使用主成分中的一小部分作为预测变量，就可以显著提高性能。因此，可以将提取主成分的数量视为调参参数。以下代码执行了使用1到200个主成分的交叉验证PCR。可以看到，只提取五个主成分进入回归方程就可以显著降低预测误差，之后逐渐减少。\n\n# 对PCR模型执行10折交叉验证，调参主成分数量，从1到100\nset.seed(123)\ncv_model_pcr &lt;- train(\n  Sale_Price ~ ., \n  data = ames_train, \n  method = \"pcr\",\n  trControl = trainControl(method = \"cv\", number = 10),\n  preProcess = c(\"zv\", \"center\", \"scale\"),\n  tuneLength = 200\n)\n\n# RMSE最低的模型\ncv_model_pcr$bestTune\n##     ncomp\n## 195   195\n\n\n# RMSE最低的模型结果\ncv_model_pcr$results %&gt;%\n  dplyr::filter(ncomp == pull(cv_model_pcr$bestTune))\n##   ncomp     RMSE  Rsquared     MAE   RMSESD RsquaredSD    MAESD\n## 1   195 31807.17 0.8435625 19228.7 7980.335 0.07041646 1592.957\n\n\n# 绘制交叉验证的RMSE\nggplot(cv_model_pcr)\n\n\n\n\n\n\n\n\n通过使用PCR控制多重共线性，模型预测准确性相比之前获得的线性模型有了显著提升（交叉验证RMSE从约41691.74美元减少到31807.17美元），甚至低于之前的k近邻模型（33582.40）。\n值得注意的是，由于PCA在选择主成分时不考虑响应变量。因此，生成的新预测变量（主成分）是以减少预测变量空间的变异性为原则，并非最大化与响应变量间的关系。如果PCA处理的这种变异性恰好与响应变量的变异性相关，那么PCR会更容易识别出预测关系。但是，如果预测变量空间的变异性与响应变量的变异性无关，那么PCR可能难以识别实际存在的预测关系，甚至会导致预测准确性下降。"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#偏最小二乘",
    "href": "bigdata/machlearn/regression.html#偏最小二乘",
    "title": "有监督学习：回归模型",
    "section": "",
    "text": "偏最小二乘（PLS）可以视为一种有监督的降维过程。与PCR类似，PLS也为回归模型构建输入的线性组合，但与PCR不同的是，它利用了响应变量来辅助构建主成分，所以不仅能捕捉原始特征中大部分信息，而且能生成与响应变量相关的新特征。\n\n\n\nPCR与PLS之间的差别\n\n\n\n\n\n\n\n\n\n\n\n上图显示使用PCR时前两个主成分与响应变量关系很小，而使用PLS时前两个主成分与响应的关联性强得多。\nPLS是通过\\(y\\)对相应\\(x_j\\)的简单线性回归模型的系数（回归系数与相关系数成正比）来计算第一个主成分（\\(z_1\\)），所以PLS对与响应变量相关性最强的变量赋予最高权重。接下来，计算第二个主成分（\\(z_2\\)）时，先将每个变量回归到\\(z_1\\)上，用每个回归的残差（每个预测变量未被第一个主成分解释的剩余信息）来代替每个预测变量值，同样依据将残差与响应变量的相关性赋予权重计算第二个主成分。以此类推直到计算出所有\\(m\\)个主成分。更详细的内容在主成分分析降维部分介绍。\n与PCR类似，在train()中更改method参数可以拟合PLS模型。与PCR一样，使用的主成分数量是一个调参参数，由最大化预测准确性的模型确定。\n下面使用1到30个主成分的交叉验证PLS。可以看出，与PCR相比，预测误差略有下降，但是使用更少的主成分就达到了最小RMSE，因为这些主成分受响应变量的引导。\n\n# 对PLS模型执行10折交叉验证，调参主成分数量，从1到30\nset.seed(123)\ncv_model_pls &lt;- train(\n  Sale_Price ~ ., \n  data = ames_train, \n  method = \"pls\",\n  trControl = trainControl(method = \"cv\", number = 10),\n  preProcess = c(\"zv\", \"center\", \"scale\"),\n  tuneLength = 30\n)\n\n# RMSE最低的模型\ncv_model_pls$bestTune\n##   ncomp\n## 3     3\n\n\n# RMSE最低的模型结果\ncv_model_pls$results %&gt;%\n  dplyr::filter(ncomp == pull(cv_model_pls$bestTune))\n##   ncomp     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD\n## 1     3 31077.42 0.8493617 19137.51 7878.004 0.07079317 1784.467\n\n# 绘制交叉验证的RMSE\nggplot(cv_model_pls)"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#特征解释",
    "href": "bigdata/machlearn/regression.html#特征解释",
    "title": "有监督学习：回归模型",
    "section": "",
    "text": "一旦找到最大化预测准确性的模型，下一个目标是解释模型结构。解释模型结构需要先确定最具影响力的变量，可以使用vip::vip()提取并绘制最重要的变量。重要性度量从100（最重要）归一化到0（最不重要）。下图展示了最重要的四个变量分别是Gr_Liv_Area、First_Flr_SF、Garage_Cars和Garage_Area。\n\nvip(cv_model_pls, num_features = 20, method = \"model\")\n\n\n\n\n\n\n\n\n同时，由于线性回归模型假设单调线性关系。可以针对最重要的变量构建部分依赖图（PDPs）。PDPs绘制了指定特征在其边际分布上变化时平均预测值（\\(\\hat{y}\\)）的变化。当存在非线性关系时，PDPs变得更加有用。pdp包提供了方便的函数来计算和绘制PDPs。\n\np1 &lt;- pdp::partial(cv_model_pls, pred.var = \"Gr_Liv_Area\", grid.resolution = 20) %&gt;% \n  autoplot() +\n  scale_y_continuous(limits = c(0, 300000), labels = scales::dollar)\n\np2 &lt;- pdp::partial(cv_model_pls, pred.var = \"First_Flr_SF\", grid.resolution = 20) %&gt;% \n  autoplot() +\n  scale_y_continuous(limits = c(0, 300000), labels = scales::dollar)\n\np3 &lt;- pdp::partial(cv_model_pls, pred.var = \"Garage_Cars\", grid.resolution = 20) %&gt;% \n  autoplot() +\n  scale_y_continuous(limits = c(0, 300000), labels = scales::dollar)\n\np4 &lt;- pdp::partial(cv_model_pls, pred.var = \"Garage_Area\", grid.resolution = 4) %&gt;% \n  autoplot() +\n  scale_y_continuous(limits = c(0, 300000), labels = scales::dollar)\n\ngridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2)\n\n\n\n\n\n\n\n\n最重要的四个预测变量均与售价呈正相关；然而，我们看到最重要预测变量的斜率（\\(\\hat{\\beta}_i\\)）最陡，对于较不重要的变量逐渐减小。"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#工具和数据-1",
    "href": "bigdata/machlearn/regression.html#工具和数据-1",
    "title": "有监督学习：回归模型",
    "section": "2.1 工具和数据",
    "text": "2.1 工具和数据\n\n# 辅助包\nlibrary(dplyr)     # 用于数据处理\nlibrary(ggplot2)   # 用于出色的绘图\nlibrary(rsample)   # 用于数据拆分\n\n# 建模包\nlibrary(caret)     # 用于逻辑回归建模\n\n# 模型解释包\nlibrary(vip)       # 变量重要性\n\n使用员工流失数据，目标是预测员工是否流失的响应变量（编码为“是”/“否”）。\n\nlibrary(modeldata)\ndf &lt;- attrition %&gt;% mutate_if(is.ordered, factor, ordered = FALSE)\n\n# 为rsample::attrition数据创建训练集（70%）和测试集（30%）。\nset.seed(123)  # 为了可重现性\nchurn_split &lt;- initial_split(df, prop = .7, strata = \"Attrition\")\nchurn_train &lt;- training(churn_split)\nchurn_test  &lt;- testing(churn_split)"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#逻辑回归的目的",
    "href": "bigdata/machlearn/regression.html#逻辑回归的目的",
    "title": "有监督学习：回归模型",
    "section": "2.2 逻辑回归的目的",
    "text": "2.2 逻辑回归的目的\n假设有关于某地区居民参与公共福利项目的数据，想了解居民的年收入能否解释他们是否参与该项目。如果使用线性回归，对于接近零的年收入，预测出负的参与概率；如果年收入非常高，又会得到大于1的概率值。这些预测没有意义，因为无论年收入如何，真实的参与概率必须在0到1之间。而逻辑回归线是非线性的S形曲线，相当于将取值范围从负无穷到正无穷转化为0到1之间。\n\n\n\n\n\n\n\n\n\n为了避免线性模型在二元响应上的不足，使用一个函数来建模响应概率，该函数对所有\\(X\\)值输出在0到1之间。许多函数满足这一描述。在逻辑回归中，使用逻辑函数。\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} \\tag{5.1}\n\\]\n其中，\\(\\beta_i\\)参数表示与线性回归类似的系数，\\(p(X)\\)可以解释为正类（上述例子中的参与项目）出现的概率。\\(p(x)\\)的最小值在\\(\\lim_{a \\to -\\infty} \\left[ \\frac{e^a}{1 + e^a} \\right] = 0\\)时取得，最大值在\\(\\lim_{a \\to \\infty} \\left[ \\frac{e^a}{1 + e^a} \\right] = 1\\)时取得，这将输出概率限制在0到1之间。对方程进行重新整理，得到logit变换（逻辑回归因此得名）：\n\\[\ng(X) = \\ln \\left[ \\frac{p(X)}{1 - p(X)} \\right] = \\beta_0 + \\beta_1 X \\tag{5.2}\n\\]\n对\\(p(X)\\)应用logit变换会得到一个类似于简单线性回归模型中均值响应的线性方程。使用logit变换还可以为\\(\\beta_1\\)的大小提供直观的解释：对于\\(X\\)的每一单位增加，发生概率（例如参与项目）以\\(e^{\\beta_1}\\)的倍数递增。如果\\(X\\)是分类变量，存在类似的解释。"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#简单逻辑回归",
    "href": "bigdata/machlearn/regression.html#简单逻辑回归",
    "title": "有监督学习：回归模型",
    "section": "2.3 简单逻辑回归",
    "text": "2.3 简单逻辑回归\n拟合两个逻辑回归模型，以预测员工流失的概率。第一个模型基于员工的月收入（MonthlyIncome）预测流失概率，第二个模型基于员工是否加班（OverTime）。glm()函数拟合广义线性模型，语法类似于lm()，但是必须传递参数family = \"binomial\"，以告诉R运行逻辑回归。\n\nmodel1 &lt;- glm(Attrition ~ MonthlyIncome, family = \"binomial\", data = churn_train)\nmodel2 &lt;- glm(Attrition ~ OverTime, family = \"binomial\", data = churn_train)\n\n在后台，glm()使用最大似然估计（ML）来估计未知模型参数。使用ML估计拟合逻辑回归模型的基本逻辑是寻找\\(\\beta_0\\)和\\(\\beta_1\\)的估计值，使得每个员工的预测流失概率\\(\\hat{p}(X_i)\\)都尽可能接近员工的实际流失状态。似然函数的数学方程形式化：\n\\[\n\\ell(\\beta_0, \\beta_1) = \\prod_{i: y_i=1} p(X_i) \\prod_{i': y'_i=0} [1 - p(x'_i)]\n\\]\n估计值\\(\\hat{\\beta}_0\\)和\\(\\hat{\\beta}_1\\)被选择以最大化这个似然函数。结果是流失的预测概率。\n\n\n\n\n\n\n\n\n\n对于model1，月收入的估计系数为\\(\\hat{\\beta}_1 = -0.000130\\)，为负值，表明月收入增加与流失概率降低相关。类似地，对于model2，加班的员工与不加班的员工相比，流失概率增加。\n\ntidy(model1)\n## # A tibble: 2 × 5\n##   term           estimate std.error statistic      p.value\n##   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n## 1 (Intercept)   -0.886    0.157         -5.64 0.0000000174\n## 2 MonthlyIncome -0.000139 0.0000272     -5.10 0.000000344\n\ntidy(model2)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)    -2.13     0.119    -17.9  1.46e-71\n## 2 OverTimeYes     1.29     0.176      7.35 2.01e-13\n\n如前所述，使用exp()变换更容易解释系数：\n\nexp(coef(model1))\n##   (Intercept) MonthlyIncome \n##     0.4122647     0.9998614\n\nexp(coef(model2))\n## (Intercept) OverTimeYes \n##   0.1189759   3.6323389\n\n因此，在model1中，月收入每增加一美元，员工流失的概率以0.99986的倍数递增；而在model2中，加班的员工与不加班的员工相比，流失概率增加3.6倍。\n与线性回归类似，可以使用估计的标准误差来获得置信区间：\n\nconfint(model1)  # 对于概率，可以使用 `exp(confint(model1))`\n##                       2.5 %        97.5 %\n## (Intercept)   -1.1932606571 -5.761048e-01\n## MonthlyIncome -0.0001948723 -8.803311e-05\n\nconfint(model2)\n##                  2.5 %    97.5 %\n## (Intercept) -2.3695727 -1.902409\n## OverTimeYes  0.9463761  1.635373"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#多元逻辑回归",
    "href": "bigdata/machlearn/regression.html#多元逻辑回归",
    "title": "有监督学习：回归模型",
    "section": "2.4 多元逻辑回归",
    "text": "2.4 多元逻辑回归\n使用多个预测变量预测二元响应：\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1 X + \\dots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X + \\dots + \\beta_p X_p}}\n\\]\n将月收入和是否加班两个变量共同加入模型。结果显示，两个特征在0.05水平上均具有统计显著性。\n\nmodel3 &lt;- glm(\n  Attrition ~ MonthlyIncome + OverTime,\n  family = \"binomial\", \n  data = churn_train\n)\n\ntidy(model3)\n## # A tibble: 3 × 5\n##   term           estimate std.error statistic  p.value\n##   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)   -1.33     0.177         -7.54 4.74e-14\n## 2 MonthlyIncome -0.000147 0.0000280     -5.27 1.38e- 7\n## 3 OverTimeYes    1.35     0.180          7.50 6.59e-14"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#评估模型准确性-1",
    "href": "bigdata/machlearn/regression.html#评估模型准确性-1",
    "title": "有监督学习：回归模型",
    "section": "2.5 评估模型准确性",
    "text": "2.5 评估模型准确性\n现在关注模型的预测效果，使用caret::train()拟合三个10折交叉验证的逻辑回归模型。提取准确性指标（在本例中为分类准确性），可以看到cv_model1和cv_model2的平均准确率均为83.95%。然而，使用所有预测变量的cv_model3达到了87.16%的平均准确率。\n\nset.seed(123)\ncv_model1 &lt;- train(\n  Attrition ~ MonthlyIncome, \n  data = churn_train, \n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trainControl(method = \"cv\", number = 10)\n)\n\nset.seed(123)\ncv_model2 &lt;- train(\n  Attrition ~ MonthlyIncome + OverTime, \n  data = churn_train, \n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trainControl(method = \"cv\", number = 10)\n)\n\nset.seed(123)\ncv_model3 &lt;- train(\n  Attrition ~ ., \n  data = churn_train, \n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trainControl(method = \"cv\", number = 10)\n)\n\n# 提取样本外性能指标\nsummary(\n  resamples(\n    list(\n      model1 = cv_model1, \n      model2 = cv_model2, \n      model3 = cv_model3\n    )\n  )\n)$statistics$Accuracy\n##             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\n## model1 0.8349515 0.8349515 0.8398379 0.8395076 0.8431373 0.8446602    0\n## model2 0.8349515 0.8349515 0.8398379 0.8395076 0.8431373 0.8446602    0\n## model3 0.8058252 0.8529412 0.8786765 0.8715662 0.8907767 0.9320388    0\n\n可以通过评估混淆矩阵进一步了解模型的性能，使用caret::confusionMatrix()计算混淆矩阵。可以看到，虽然在预测非流失情况方面表现良好（高特异性），但模型在预测实际流失情况方面的表现特别差（低敏感性）。\n\n# 预测类别\npred_class &lt;- predict(cv_model3, churn_train)\n\n# 创建混淆矩阵\nconfusionMatrix(\n  data = relevel(pred_class, ref = \"Yes\"), \n  reference = relevel(churn_train$Attrition, ref = \"Yes\")\n)\n## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction Yes  No\n##        Yes  83  20\n##        No   82 843\n##                                           \n##                Accuracy : 0.9008          \n##                  95% CI : (0.8809, 0.9184)\n##     No Information Rate : 0.8395          \n##     P-Value [Acc &gt; NIR] : 8.982e-09       \n##                                           \n##                   Kappa : 0.5658          \n##                                           \n##  Mcnemar's Test P-Value : 1.542e-09       \n##                                           \n##             Sensitivity : 0.50303         \n##             Specificity : 0.97683         \n##          Pos Pred Value : 0.80583         \n##          Neg Pred Value : 0.91135         \n##              Prevalence : 0.16051         \n##          Detection Rate : 0.08074         \n##    Detection Prevalence : 0.10019         \n##       Balanced Accuracy : 0.73993         \n##                                           \n##        'Positive' Class : Yes             \n## \n\n在上面的混淆矩阵中，无信息率为0.839。这代表训练数据中非流失与流失的比例（table(churn_train$Attrition) %&gt;% prop.table()）。这意味着如果简单地对每个员工都预测“否”（盲猜），仍然可以获得83.9%的准确率。可以通过绘制ROC曲线比较简单模型（cv_model1）与完整模型（cv_model3）之间的性能差别。\n\nlibrary(ROCR)\n\n# 计算预测概率\nm1_prob &lt;- predict(cv_model1, churn_train, type = \"prob\")$Yes\nm3_prob &lt;- predict(cv_model3, churn_train, type = \"prob\")$Yes\n\n# 计算cv_model1和cv_model3的AUC指标\nperf1 &lt;- prediction(m1_prob, churn_train$Attrition) %&gt;%\n  performance(measure = \"tpr\", x.measure = \"fpr\")\nperf2 &lt;- prediction(m3_prob, churn_train$Attrition) %&gt;%\n  performance(measure = \"tpr\", x.measure = \"fpr\")\n\n# 绘制cv_model1和cv_model3的ROC曲线\nplot(perf1, col = \"black\", lty = 2)\nplot(perf2, add = TRUE, col = \"blue\")\nlegend(0.8, 0.2, legend = c(\"cv_model1\", \"cv_model3\"),\n       col = c(\"black\", \"blue\"), lty = 2:1, cex = 0.6)\n\n\n\n\n\n\n\n\n与线性回归类似，可以执行PLS逻辑回归，通过减少数值预测变量的维度来提高准确性。原数据集中有16个数值特征，最佳模型使用14个主成分，平均准确率为0.8716，与cv_model3的平均交叉验证准确率（0.8716）相等，并没有明显改善。\n\n# 对PLS模型执行10折交叉验证，调整使用的主成分数量\nset.seed(123)\ncv_model_pls &lt;- train(\n  Attrition ~ ., \n  data = churn_train, \n  method = \"pls\",\n  family = \"binomial\",\n  trControl = trainControl(method = \"cv\", number = 10),\n  preProcess = c(\"zv\", \"center\", \"scale\"),\n  tuneLength = 16\n)\n\n# 具有最高准确率的模型\ncv_model_pls$bestTune\n##    ncomp\n## 11    11\n\n# 最高准确率模型的结果\ncv_model_pls$results %&gt;%\n  dplyr::filter(ncomp == pull(cv_model_pls$bestTune))\n##   ncomp  Accuracy     Kappa AccuracySD   KappaSD\n## 1    11 0.8716041 0.3315369 0.02178929 0.1387885\n\n# 绘制交叉验证的准确率\nggplot(cv_model_pls)"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#特征解释-1",
    "href": "bigdata/machlearn/regression.html#特征解释-1",
    "title": "有监督学习：回归模型",
    "section": "2.6 特征解释",
    "text": "2.6 特征解释\n与线性回归类似，一旦确定了逻辑回归模型，就需要解释特征如何影响结果。使用vip::vip()，可以提取前20个有影响的变量。下图显示，加班是最重要的变量，其次是环境满意度和工作满意度。\n\nvip(cv_model3, num_features = 20)\n\n\n\n\n\n\n\n\n逻辑回归的线性关系发生在logit尺度上，在概率尺度上，关系是非线性的。可以展示了员工流失预测概率与重要的变量之间的关系（同时考虑模型中其他预测变量的平均效应），可以看到预测的流失概率如何随着影响预测变量的值变化。\n\np1 &lt;- pdp::partial(cv_model3, pred.var = \"OverTime\", \n                  prob = TRUE,  # 预测概率\n                  which.class = 2) %&gt;%  # 指定预测概率的类别 (2 = \"Yes\")，即是流失\n        autoplot() + ylim(c(0, 1)) \n\np2 &lt;- pdp::partial(cv_model3, pred.var = \"EnvironmentSatisfaction\", \n                   prob = TRUE,\n                   which.class = 2) %&gt;% \n        autoplot() + ylim(c(0, 1))\n\np3 &lt;- pdp::partial(cv_model3, pred.var = \"JobSatisfaction\", \n                   prob = TRUE,\n                   which.class = 2) %&gt;% \n        autoplot() + ylim(c(0, 1))\n\np4 &lt;- pdp::partial(cv_model3, pred.var = \"DistanceFromHome\", \n                   prob = TRUE,\n                   which.class = 2) %&gt;% \n        autoplot() + ylim(c(0, 1))\n\ngridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2)"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#工具和数据-2",
    "href": "bigdata/machlearn/regression.html#工具和数据-2",
    "title": "有监督学习：回归模型",
    "section": "3.1 工具和数据",
    "text": "3.1 工具和数据\n\n# 辅助包\nlibrary(recipes)  # 用于特征工程\n\n# 建模包\nlibrary(glmnet)   # 用于实现正则化回归\nlibrary(caret)    # 用于自动化调参过程\n\n# 模型解释包\nlibrary(vip)      # 用于变量重要性\n\n数据采用之前创建的ames_train和ames_test数据集和员工流失数据。"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#正则化回归的目的",
    "href": "bigdata/machlearn/regression.html#正则化回归的目的",
    "title": "有监督学习：回归模型",
    "section": "3.2 正则化回归的目的",
    "text": "3.2 正则化回归的目的\nOLS回归的目标是找到超平面（在二维空间中是一条直线），能够使得响应变量的观测值与预测值之间的平方误差和（SSE）最小。最小化的目标函数如下：\n\\[\n\\text{minimize} \\left( SSE = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\right)\n\\]\n但是，OLS目标函数的表现取决于几个关键假设：\n\n线性关系；\n观测数（\\(n\\)）大于特征数（\\(p\\)）（\\(n &gt; p\\)）；\n没有或几乎没有多重共线性。\n\n此外，还有误差的正态性和同方差假设。\n现实中的数据集（如文本分析等）是宽数据，包含较多的特征（\\(p &gt; n\\)）。随着\\(p\\)的增加，可能会违反OLS的关键假设，例如，观测数大于特征数，较大的多重共线性等。\n多重共线性的存在会造成系数估计被放大，系数分配不稳定，放大噪声的影响。 在OLS回归中，系数估计的公式为： \\[\\hat{\\beta} = (X^T X)^{-1} X^T y\\] 其中，\\(X\\) 是特征矩阵，\\(y\\) 是响应变量。 当特征之间高度相关时，\\(X^T X\\) 矩阵变得接近奇异（singular），即其行列式接近于零。这意味着矩阵的逆 \\((X^T X)^{-1}\\) 中的元素会变得非常大。大的矩阵元素会导致系数估计 \\(\\hat{\\beta}\\) 的值被放大。\n多重共线性使得模型难以区分高度相关的特征对响应变量的独立贡献。例如，年收入和教育水平都可能与参与福利项目的概率相关，但由于它们高度相关，模型可能“过度分配”贡献给其中一个特征，导致其系数变得异常大，而另一个特征的系数可能被压低或符号相反。\n当特征高度相关时，模型对数据中的微小噪声或随机波动变得非常敏感。OLS试图通过调整系数来拟合数据中的所有变异（包括噪声），这可能导致某些系数被放大到不合理的程度，以“解释”这些噪声。\n一类常规解决多重共线性的方法称为硬阈值特征选择，例如分步回归中的前向选择和后向消除策略。然而，这些方法计算效率低下，扩展性不佳，并且特征只有留在模型和移除模型两个选择（硬阈值）。相比之下，另一类方法称为软阈值，通过惩罚项逐渐将不相关特征的影响推向零，可以产生更准确且更容易解释的模型。\n针对存在多重共线性的宽数据，一种替代OLS回归的方法是使用正则化回归（通常也称为惩罚模型或收缩方法）约束所有系数估计的总的大小。这种约束有助于减少系数的幅度和波动，并降低模型的方差（代价是系数估计不再无偏，但这是合理的折衷）。\n正则化回归模型的目标函数类似于OLS，但增加了惩罚项\\(P\\)：\n\\[\n\\text{minimize} \\left( SSE + P \\right)\n\\] 三种常见的惩罚参数：\n\nRidge（岭回归）；\nLasso（或LASSO）；\nElastic net（弹性网），是ridge和lasso的组合。\n\n\nRidge惩罚\nRidge回归通过向目标函数添加\\(\\lambda \\sum_{j=1}^p \\beta_j^2\\)来控制估计系数的大小：\n\\[\n\\text{minimize} \\left( SSE + \\lambda \\sum_{j=1}^p \\beta_j^2 \\right)\n\\]\n该惩罚称为\\(L^2\\)（或欧几里得）范数，由调参参数\\(\\lambda\\)控制。当\\(\\lambda = 0\\)时，目标函数等同于OLS回归目标函数，仅最小化SSE。然而，当\\(\\lambda \\to \\infty\\)时，惩罚变大，迫使系数趋向于零。\n\n\n\n\n\n\n\n\n\n随着\\(\\lambda\\)从\\(0 \\to \\infty\\)，15个预测变量的Ridge回归系数。随着\\(\\lambda\\)变大，系数幅度受到更多约束。\n尽管这些系数已进行缩放和中心化，但是当\\(\\lambda\\)接近零时，一些系数相当大。当\\(\\lambda\\)较小时（即正则化力度较弱时），x1的系数是一个大的负值，并且在\\(\\lambda\\)变化时，系数的数值可能会不稳定（波动）。这种波动通常是由于特征之间的多重共线性导致的。当\\(\\lambda\\)增加到大约7时，正则化开始显著约束系数，x1的系数开始稳定地趋向于零。这表明Ridge回归通过施加更大的惩罚（更高的\\(\\lambda\\)值），限制了系数的大小，减少了由于多重共线性引起的系数波动。这种方差的减少通常会使模型更稳定，减少样本外的预测误差（尽管可能会引入一些偏差）。\nRidge回归不执行特征选择，会保留最终模型中的所有可用特征。因此，如果认为需要保留模型中的所有特征，只是减少不太重要的变量可能产生的噪声时（较小数据集存在严重多重共线性），Ridge模型是更好的选择。如果需要更高的解释性，并且许多特征是冗余的，那么Lasso或弹性网惩罚可能更可取。\n\n\nLasso惩罚\nLasso（最小绝对收缩和选择算子，least absolute shrinkage and selection operator）惩罚是Ridge惩罚的替代方法，仅需稍作修改。唯一的区别是将\\(L^2\\)范数替换为\\(L^1\\)范数：\\(\\lambda \\sum_{j=1}^p |\\beta_j|\\)：\n\\[\n\\text{minimize} \\left( SSE + \\lambda \\sum_{j=1}^p |\\beta_j| \\right) \\tag{6.4}\n\\]\n与Ridge惩罚将变量推向接近但不等于零不同，Lasso惩罚实际上会将系数完全推向零。Lasso惩罚不仅改善了模型，还执行了自动特征选择。\n\n\n\n\n\n\n\n\n\n上图中，当\\(\\lambda &lt; 0.01\\)时，所有15个变量都包含在模型中；当\\(\\lambda \\approx 0.5\\)时，保留了9个变量；当\\(\\log(\\lambda) = 1\\)时，仅保留了5个变量。因此，当数据集具有许多特征时，Lasso可用于识别和提取信号最大（且最一致）的特征。\n\n\n弹性网\n弹性网是Ridge和Lasso惩罚的泛化，结合了两者的惩罚：\n\\[\n\\text{minimize} \\left( SSE + \\lambda_1 \\sum_{j=1}^p \\beta_j^2 + \\lambda_2 \\sum_{j=1}^p |\\beta_j| \\right) \\tag{6.5}\n\\]\nLasso在处理高度相关特征时的特征选择过程具有一定的随机性。也就是说，Lasso可能在不同的运行或数据样本中选择不同的特征来保留或排除，而这种选择并不基于明确的规则或优先级。例如，两个高度相关的特征可能在理论上对响应变量的贡献相似，但Lasso可能随机选择其中一个保留，另一个排除。这种非系统化的选择可能导致模型的不稳定性，尤其是在特征选择对解释性至关重要的场景中。相对地，Ridge回归使用\\(L^2\\)范数惩罚（\\(\\lambda \\sum_{j=1}^p \\beta_j^2\\)），不会将系数完全推向零，而是将所有系数压缩到接近零但非零的值。对于高度相关的特征，Ridge回归倾向于将它们的系数压缩到相似的值，而不是随机选择一个保留、一个排除。弹性网结合了Ridge和Lasso的优点，通过在目标函数中同时加入\\(L^1\\)和\\(L^2\\)惩罚。\\(L^2\\)惩罚（来自Ridge）帮助系统化地处理高度相关的特征，降低系数方差，保持模型稳定性；而\\(L^1\\)惩罚（来自Lasso）则允许将不重要特征的系数推向零，实现特征选择。弹性网还可以通过调整参数alpha（公式未纳入）控制\\(L^1\\)和\\(L^2\\)惩罚的相对权重），在特征选择和处理多重共线性之间取得平衡。"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#实现",
    "href": "bigdata/machlearn/regression.html#实现",
    "title": "有监督学习：回归模型",
    "section": "3.3 实现",
    "text": "3.3 实现\n可以使用glmnet直接引擎实现正则化回归，此外h2o、elasticnet、penalized包也可以实现。glmnet包非常高效且快速，特别适用于大数据集（底层是Fortran语言）。它仅接受非公式XY接口，因此在建模之前，需要将特征和目标集分开。\n以下代码使用model.matrix对我们的特征集进行哑变量编码（对于大型数据集，可使用Matrix::sparse.model.matrix以提高效率）。我们还对响应变量进行了\\(\\log\\)变换，这不是必需的；然而，像正则化回归这样的参数模型对偏态响应值敏感，因此变换通常可以提高预测性能。\n\n# 创建训练特征矩阵\n# 使用 model.matrix(...)[, -1] 丢弃截距后\n# 获得原始回归模型所有特征（扩展因子和交互项）的数据矩阵\nX &lt;- model.matrix(Sale_Price ~ ., ames_train)[, -1]\n\n# 对响应变量进行 log 变换\nY &lt;- log(ames_train$Sale_Price)\n\n使用glmnet::glmnet()函数进行正则化回归，用alpha参数设定Ridge（alpha = 0）、Lasso（alpha = 1）或弹性网（0 &lt; alpha &lt; 1）模型；默认模型会进行系数归一化，否则，自然值较大的预测变量会受到更多惩罚。如果之前已经标准化了预测变量，可以通过standardize = FALSE关闭。\n\n# 对 Ames 数据应用 Ridge 回归\nridge &lt;- glmnet(\n  x = X,\n  y = Y,\n  alpha = 0\n)\n\nplot(ridge, xvar = \"lambda\")\n\n\n\n\n\n\n\n\n可以使用ridge$lambda查看精确\\(\\lambda\\)值，glmnet会自己应用100个基于数据推导的\\(\\lambda\\)值。可以使用coef()访问模型的系数。\n\n# 查看惩罚参数的 lambda 值\nridge$lambda %&gt;% head()\n## [1] 285.7769 260.3893 237.2570 216.1798 196.9749 179.4762\n\n\n# 小 lambda 导致大系数\ncoef(ridge)[c(\"Latitude\", \"Overall_QualVery_Excellent\"), 100]\n##                   Latitude Overall_QualVery_Excellent \n##                 0.60703722                 0.09344684\n\n\n# 大 lambda 导致小系数\ncoef(ridge)[c(\"Latitude\", \"Overall_QualVery_Excellent\"), 1]  \n##                   Latitude Overall_QualVery_Excellent \n##               6.115930e-36               9.233251e-37"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#调参",
    "href": "bigdata/machlearn/regression.html#调参",
    "title": "有监督学习：回归模型",
    "section": "3.4 调参",
    "text": "3.4 调参\n为了确定最佳的\\(\\lambda\\)值，可以使用k折交叉验证（CV）。glmnet::cv.glmnet()可以执行k折交叉验证，默认情况下执行10折交叉验证。\n\n# 对 Ames 数据应用 CV Ridge 回归\nridge &lt;- cv.glmnet(\n  x = X,\n  y = Y,\n  alpha = 0\n)\n\n# 对 Ames 数据应用 CV Lasso 回归\nlasso &lt;- cv.glmnet(\n  x = X,\n  y = Y,\n  alpha = 1\n)\n\n# 绘制结果\npar(mfrow = c(1, 2))\nplot(ridge, main = \"Ridge惩罚\\n\\n\")\nplot(lasso, main = \"Lasso惩罚\\n\\n\")\n\n\n\n\n\n\n\n\n图中展示了Ridge和Lasso模型的10折交叉验证MSE随\\(\\lambda\\)的变化。从右向左，即\\(\\lambda\\)增大的方向，每个图中的第一条虚线表示具有最小MSE的\\(\\lambda\\)，第二条表示最小MSE加上一个标准误差时的\\(\\lambda\\)。在两个模型中，随着惩罚\\(\\log(\\lambda)\\)变大，MSE略有减少，说明OLS模型可能会过拟合。但继续增加惩罚，MSE开始增加。图顶部的数字表示模型中的特征数量。\n接着获取参数的具体数值。\n\n# Ridge 模型\nmin(ridge$cvm)       # 最小 MSE\n## [1] 0.02198554\n# [1] 0.0218308\nridge$lambda.min     # 最小 MSE 对应的 lambda\n## [1] 0.1389619\n## [1] 0.1389619\n\nridge$cvm[ridge$lambda == ridge$lambda.1se]  # 1-SE 规则\n## [1] 0.02471237\n## [1] 0.0241608\nridge$lambda.1se  # 此 MSE 对应的 lambda\n## [1] 0.6156877\n## [1] 0.5609917\n\n\n# Lasso 模型\nmin(lasso$cvm)       # 最小 MSE\n## [1] 0.02346905\n## [1] 0.02360464\nlasso$lambda.min     # 最小 MSE 对应的 lambda\n## [1] 0.002727879\n## [1] 0.003606096\nlasso$nzero[lasso$lambda == lasso$lambda.min] # 最小 MSE 时的系数数量\n## s50 \n## 151\n## s47 \n## 135\n\nlasso$cvm[lasso$lambda == lasso$lambda.1se]  # 1-SE 规则\n## [1] 0.02745127\n## [1] 0.02690078\nlasso$lambda.1se  # 此 MSE 对应的 lambda\n## [1] 0.0120862\n## [1] 0.01326459\nlasso$nzero[lasso$lambda == lasso$lambda.1se] # 1-SE MSE 时的系数数量\n## s34 \n##  72\n## s33 \n##  69\n\n可以直观地评估这一点。通过绘制在\\(\\lambda\\)两个取值范围内的估计系数，可以直观评估在最大化预测准确性（保持MSE最小）的同时，可以多大程度地约束系数。Ridge和Lasso惩罚提供了相似的最小MSE，但是Ridge使用所有294个特征，而Lasso模型减少了特征。因此，虽然Lasso模型没有显著优于Ridge模型，但提供了更好的描述和解释预测变量的方式。\n\n# Ridge 模型\nridge_min &lt;- glmnet(\n  x = X,\n  y = Y,\n  alpha = 0\n)\n\n# Lasso 模型\nlasso_min &lt;- glmnet(\n  x = X,\n  y = Y,\n  alpha = 1\n)\n\npar(mfrow = c(1, 2))\n# 绘制 Ridge 模型\nplot(ridge_min, xvar = \"lambda\", main = \"Ridge penalty\\n\\n\")\nabline(v = log(ridge$lambda.min), col = \"red\", lty = \"dashed\")\nabline(v = log(ridge$lambda.1se), col = \"blue\", lty = \"dashed\")\n\n# 绘制 Lasso 模型\nplot(lasso_min, xvar = \"lambda\", main = \"Lasso penalty\\n\\n\")\nabline(v = log(lasso$lambda.min), col = \"red\", lty = \"dashed\")\nabline(v = log(lasso$lambda.1se), col = \"blue\", lty = \"dashed\")\n\n\n\n\n\n\n\n\n通过调整alpha参数实现弹性网\n\nlasso    &lt;- glmnet(X, Y, alpha = 1.0) \nelastic1 &lt;- glmnet(X, Y, alpha = 0.25) \nelastic2 &lt;- glmnet(X, Y, alpha = 0.75) \nridge    &lt;- glmnet(X, Y, alpha = 0.0)\n\npar(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1)\nplot(lasso, xvar = \"lambda\", main = \"Lasso (Alpha = 1)\\n\\n\\n\")\nplot(elastic1, xvar = \"lambda\", main = \"Elastic Net (Alpha = .25)\\n\\n\\n\")\nplot(elastic2, xvar = \"lambda\", main = \"Elastic Net (Alpha = .75)\\n\\n\\n\")\nplot(ridge, xvar = \"lambda\", main = \"Ridge (Alpha = 0)\\n\\n\\n\")\n\n\n\n\n\n\n\n\n此时需要同时调整\\(\\lambda\\)和alpha参数，可以使用caret包来自动化调参过程，以下代码在0到1之间的10个alpha值和10个lambda值上执行网格搜索。\n结果显示，最小化RMSE的模型使用了alpha = 0.1和\\(\\lambda = 0.02\\)。最小RMSE为0.1460178，对应的MSE为0.0213212。略优于之前生成的Ridge和Lasso模型。\n\n# 为了可重现性\nset.seed(123)\n\n# 网格搜索\ncv_glmnet &lt;- train(\n  x = X,\n  y = Y,\n  method = \"glmnet\",\n  preProc = c(\"zv\", \"center\", \"scale\"),\n  trControl = trainControl(method = \"cv\", number = 10),\n  tuneLength = 10\n)\n\n# 具有最低 RMSE 的模型\ncv_glmnet$bestTune\n##   alpha     lambda\n## 7   0.1 0.02006835\n##   alpha     lambda\n## 7   0.1 0.02006835\n\n# 最低 RMSE 模型的结果\ncv_glmnet$results %&gt;%\n  filter(alpha == cv_glmnet$bestTune$alpha, lambda == cv_glmnet$bestTune$lambda)\n##   alpha     lambda      RMSE  Rsquared      MAE     RMSESD RsquaredSD\n## 1   0.1 0.02006835 0.1460178 0.8749723 0.086671 0.03174582 0.04965123\n##         MAESD\n## 1 0.007970299\n##   alpha     lambda      RMSE  Rsquared        MAE     RMSESD RsquaredSD\n## 1   0.1 0.02006835 0.1460178 0.8749723 0.086671 0.03174582  0.04965123\n##         MAESD\n## 1 0.007970299\n\n# 绘制交叉验证 RMSE\nggplot(cv_glmnet)\n\n\n\n\n\n\n\n\n上图呈现10个alpha值（x轴）和10个lambda值（线条颜色）的10折交叉验证RMSE。\n为了与之前Ames数据集的预测模型相比较，需要对响应变量（Sale_Price）进行了\\(\\log\\)变换，变换后显示，正则化回归模型的RMSE为23281.59，优于之前获得的PLS模型的RMSE（31077.42），说明引入惩罚参数来约束系数相较于降维方法提供了较大的改进。\n\n# 在训练数据上预测销售价格\npred &lt;- predict(cv_glmnet, X)\n\n# 计算变换预测的 RMSE\nRMSE(exp(pred), exp(Y))\n## [1] 23281.59"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#特征解释-2",
    "href": "bigdata/machlearn/regression.html#特征解释-2",
    "title": "有监督学习：回归模型",
    "section": "3.5 特征解释",
    "text": "3.5 特征解释\n正则化模型的变量重要性与线性（或逻辑）回归的解释类似，重要性由标准化系数的绝对值大小决定。\n\nvip(cv_glmnet, num_features = 20, geom = \"point\")\n\n\n\n\n\n\n\n\n与线性回归和逻辑回归类似，特征与响应之间的关系是单调线性的。然而，由于对响应变量进行了\\(\\log\\)变换，估计的关系在原始响应尺度上仍是单调的但非线性的。下图展示了前四个最重要变量（即绝对系数最大的变量）与未变换的销售价格之间的关系。所有关系均为正向，随着这些特征值的增加，平均预测销售价格增加。\n图6.11：前四个最重要变量的部分依赖图。"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#类别目标特征的预测",
    "href": "bigdata/machlearn/regression.html#类别目标特征的预测",
    "title": "有监督学习：回归模型",
    "section": "3.6 类别目标特征的预测",
    "text": "3.6 类别目标特征的预测\n针对员工流失数据集，逻辑回归模型的交叉验证准确率为87.16%，正则化逻辑回归模型为87.94%，准确率提升了约0.8个百分点。\n\ndf &lt;- attrition %&gt;% mutate_if(is.ordered, factor, ordered = FALSE)\n\n# 为 rsample::attrition 数据创建训练（70%）和测试（30%）集。为了可重现性使用 set.seed\nset.seed(123)\nchurn_split &lt;- initial_split(df, prop = .7, strata = \"Attrition\")\ntrain &lt;- training(churn_split)\ntest  &lt;- testing(churn_split)\n\n# 训练逻辑回归模型\nset.seed(123)\nglm_mod &lt;- train(\n  Attrition ~ ., \n  data = train, \n  method = \"glm\",\n  family = \"binomial\",\n  preProc = c(\"zv\", \"center\", \"scale\"),\n  trControl = trainControl(method = \"cv\", number = 10)\n)\n\n# 训练正则化逻辑回归模型\nset.seed(123)\npenalized_mod &lt;- train(\n  Attrition ~ ., \n  data = train, \n  method = \"glmnet\",\n  family = \"binomial\",\n  preProc = c(\"zv\", \"center\", \"scale\"),\n  trControl = trainControl(method = \"cv\", number = 10),\n  tuneLength = 10\n)\n\n# 提取样本外性能指标\nsummary(resamples(list(\n  logistic_model = glm_mod, \n  penalized_model = penalized_mod\n)))$statistics$Accuracy\n##                      Min.   1st Qu.    Median      Mean   3rd Qu.      Max.\n## logistic_model  0.8058252 0.8529412 0.8786765 0.8715662 0.8907767 0.9320388\n## penalized_model 0.8349515 0.8671757 0.8823529 0.8793902 0.8980583 0.9223301\n##                 NA's\n## logistic_model     0\n## penalized_model    0\n\n当应用于具有大量特征的大型数据集时，正则化回归相较于传统GLM提供了许多显著的好处。它为处理\\(n &gt; p\\)问题提供了一个很好的选择，有助于减少多重共线性的影响，并可以执行自动特征选择。它还具有相对较少的超参数，这使得它们易于调参，计算效率高且内存高效。\n然而，正则化回归确实需要一些特征预处理。特别是，所有输入必须是数值型的；然而，一些包（例如caret和h2o）会自动处理此过程。它们无法自动处理缺失数据，需要在建模前移除或填补缺失值。与GLM类似，它们对特征和目标中的异常值也不够稳健。最后，正则化回归模型仍然假设单调线性关系（始终以线性方式增加或减少），是否包含特定的交互效应也取决于分析者。"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#工具和数据-3",
    "href": "bigdata/machlearn/regression.html#工具和数据-3",
    "title": "有监督学习：回归模型",
    "section": "4.1 工具和数据",
    "text": "4.1 工具和数据\n\n# 辅助包\nlibrary(dplyr)     # 用于数据处理\nlibrary(ggplot2)   # 用于出色的绘图\n\n# 建模包\nlibrary(earth)     # 用于拟合MARS模型\nlibrary(caret)     # 用于自动化调参过程\n\n# 模型解释包\nlibrary(vip)       # 用于变量重要性\nlibrary(pdp)       # 用于变量关系\n\n继续使用ames_train和ames_test数据集。"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#mars的基本原理",
    "href": "bigdata/machlearn/regression.html#mars的基本原理",
    "title": "有监督学习：回归模型",
    "section": "4.2 MARS的基本原理",
    "text": "4.2 MARS的基本原理\n线性模型对线性假设要求较强，如果假设不成立，会影响预测准确性。可以通过显式包含多项式项（例如，\\(x_i^2\\)）或阶梯函数来扩展线性模型以捕捉非线性关系。多项式回归是一种回归形式，其中\\(X\\)和\\(Y\\)之间的关系被建模为\\(X\\)的\\(d\\)次多项式。\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 \\dots + \\beta_d x_i^d + \\epsilon_i\n\\]\n另一种方法是使用阶梯函数。多项式函数施加了全局非线性关系，而阶梯函数将\\(X\\)的范围分成区间，并在每个区间内拟合一个简单常数（例如，平均响应值）。这相当于将连续特征转换为有序分类变量，从而将线性回归函数转换为方程：\n\\[\ny_i = \\beta_0 + \\beta_1 C_1(x_i) + \\beta_2 C_2(x_i) + \\beta_3 C_3(x_i) \\dots + \\beta_d C_d(x_i) + \\epsilon_i\n\\]\n其中，\\(C_1(x_i)\\)表示\\(x_i\\)值在\\(c_1 \\leq x_i &lt; c_2\\)范围，\\(C_2(x_i)\\)表示\\(x_i\\)值在\\(c_2 \\leq x_i &lt; c_3\\)范围，……，\\(C_d(x_i)\\)表示\\(x_i\\)值在\\(c_{d-1} \\leq x_i &lt; c_d\\)范围。\n下图对比了线性、多项式和阶梯函数对非线性、非单调模拟数据的拟合。\n\n\n\n\n\n\n\n\n\n多项式回归和阶梯函数要求用户明确识别并纳入哪些变量的高阶项，在变量\\(X\\)的哪些点上设置阶梯函数的切割点，对于包含众多特征的大数据集而言不太现实。多元自适应回归样条（MARS）提供了一种便捷的方法，通过评估类似于阶梯函数的切割点（节点）来捕捉数据中的非线性关系。该过程将每个预测变量的每个数据点作为节点，并使用候选特征创建线性回归模型。\n例如，上图中的非线性、非单调数据，其中\\(Y = f(X)\\)。MARS过程首先在\\(X\\)值的范围内寻找单个点，在此点上，\\(Y\\)和\\(X\\)之间的两个不同线性关系能够实现最小的误差（例如，最小的SSE）。结果被称为铰链函数\\(h(x-a)\\)，其中\\(a\\)是切割点值。对于单个节点，铰链函数为\\(h(x-1.183606)\\)，因此\\(Y\\)的两个线性模型为：\n\\[\ny = \\begin{cases}\n\\beta_0 + \\beta_1 (1.183606 - x) & x &lt; 1.183606, \\\\\n\\beta_0 + \\beta_2 (x - 1.183606) & x &gt; 1.183606\n\\end{cases}\n\\]\n一旦找到第一个节点，搜索继续寻找第二个节点，在\\(x=4.898114\\)处找到，得到\\(y\\)的三个线性模型：\n\\[\ny = \\begin{cases}\n\\beta_0 + \\beta_1 (1.183606 - x) & x &lt; 1.183606, \\\\\n\\beta_0 + \\beta_2 (x - 1.183606) & x &gt; 1.183606 \\& x &lt; 4.898114, \\\\\n\\beta_0 + \\beta_3 (4.898114 - x) & x &gt; 4.898114\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n这一过程持续进行，直到找到许多节点，生成一个（可能）高度非线性的预测方程。尽管包含许多节点可能使我们在训练数据上拟合出非常好的关系，但它可能无法很好地泛化到新的、未见过的数据。因此，一旦识别出完整的节点集，我们可以依次移除对预测准确性贡献不大的节点。这个过程称为“修剪”，可以使用与之前模型相同的交叉验证来找到最佳的节点数量。"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#拟合基本mars模型",
    "href": "bigdata/machlearn/regression.html#拟合基本mars模型",
    "title": "有监督学习：回归模型",
    "section": "4.3 拟合基本MARS模型",
    "text": "4.3 拟合基本MARS模型\n可以使用earth包（Enhanced Adaptive Regression Through Hinges）拟合直接引擎的MARS模型。默认情况下，earth::earth()会评估所有提供的特征上的所有潜在节点，然后根据训练数据的\\(R^2\\)预期变化小于0.001进行修剪。这一计算通过广义交叉验证（GCV）过程执行，这是一种线性模型的计算快捷方式，生成近似的留一法交叉验证误差指标。\n以下代码对Ames示例构建基本的MARS模型。结果显示了最终模型的GCV统计量、广义\\(R^2\\)（GRSq）等信息。\n\n# 拟合基本MARS模型\nmars1 &lt;- earth(\n  Sale_Price ~ .,  \n  data = ames_train   \n)\n\n# 打印模型摘要\nprint(mars1)\n## Selected 41 of 45 terms, and 29 of 308 predictors\n## Termination condition: RSq changed by less than 0.001 at 45 terms\n## Importance: Gr_Liv_Area, Year_Built, Total_Bsmt_SF, ...\n## Number of terms at each degree of interaction: 1 40 (additive model)\n## GCV 511589986    RSS 967008439675    GRSq 0.9207836    RSq 0.9268516\n\n结果显示模型从308个原始预测变量（包括了因子生成的哑变量）自动选择29个预测变量，通过节点分割和可能的交互效应从预测变量生成了45个初始阶段基函数项（包括截距项和铰链函数），最后通过修剪保留了41个基函数项。结果还描述了最终模型中每个交互阶数的基函数（项）的数量，1代表1个截距，40代表其他的基函数项都是1阶的，没有交互项。\n查看模型的前10个基函数项，可以看到Gr_Liv_Area在3197处有一个节点（\\(h(3197-Gr_Liv_Area)\\)的系数为-287.99），Year_Built在2002处有一个节点，等等。\n\nsummary(mars1) %&gt;% .$coefficients %&gt;% head(10)\n##                              Sale_Price\n## (Intercept)                227597.23167\n## h(Gr_Liv_Area-3194)          -287.99337\n## h(3194-Gr_Liv_Area)           -58.61227\n## h(Year_Built-2002)           3128.13737\n## h(2002-Year_Built)           -450.63697\n## h(Total_Bsmt_SF-2223)        -653.13097\n## h(2223-Total_Bsmt_SF)         -29.03002\n## h(1656-Bsmt_Unf_SF)            21.52273\n## Overall_QualVery_Excellent 119876.97697\n## Overall_QualExcellent       73257.41159\n\n可以通过绘图方法（plot）对MARS模型性能进行呈现。图中展示了模型选择图，绘制了模型中的基函数项数（x轴）的GCV \\(R^2\\)（左侧y轴和黑色实线）的关系，以及基函数项数与原始预测变量数（右侧y轴和黑色虚线）的关系，垂直虚线则指示最佳基函数项数，在此点上GCV \\(R^2\\)的边际增量小于0.001。\n\nplot(mars1, which = 1)\n\n\n\n\n\n\n\n\n除了修剪节点数量外，earth::earth()还能评估不同铰链函数之间的潜在交互效应（增加degree = 2参数）。结果显示模型包括二阶交互项，即两个铰链函数之间的交互项（例如，h(2002-Year_Built)*h(Gr_Liv_Area-2398)表示建于2002年之后且地面居住面积超过2398平方英尺的房屋的交互效应）。\n\n# 拟合基本MARS模型\nmars2 &lt;- earth(\n  Sale_Price ~ .,  \n  data = ames_train,\n  degree = 2\n)\n\n# 查看前10个系数项\nsummary(mars2) %&gt;% .$coefficients %&gt;% head(10)\n##                                           Sale_Price\n## (Intercept)                             3.473694e+05\n## h(Gr_Liv_Area-3194)                     2.302940e+02\n## h(3194-Gr_Liv_Area)                    -7.005261e+01\n## h(Year_Built-2002)                      5.242461e+03\n## h(2002-Year_Built)                     -7.360079e+02\n## h(Total_Bsmt_SF-2223)                   1.181093e+02\n## h(2223-Total_Bsmt_SF)                  -4.901140e+01\n## h(Year_Built-2002)*h(Gr_Liv_Area-2398)  9.035037e+00\n## h(Year_Built-2002)*h(2398-Gr_Liv_Area) -3.382639e+00\n## h(Bsmt_Unf_SF-625)*h(3194-Gr_Liv_Area) -1.145129e-02"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#调参-1",
    "href": "bigdata/machlearn/regression.html#调参-1",
    "title": "有监督学习：回归模型",
    "section": "4.4 调参",
    "text": "4.4 调参\nMARS模型有两个重要的调参参数：最大交互程度和最终模型中保留的项数。需要执行网格搜索以确定这两个超参数的最佳组合，以最小化预测误差（之前earth函数的修剪过程仅基于训练数据的近似CV模型性能（计算效率很高的GCV），而不是精确的k折CV过程）。\n\n# 创建调参网格，阶数最大为3，修剪节点2到100中的10个点\nhyper_grid &lt;- expand.grid(\n  degree = 1:3, \n  nprune = seq(2, 100, length.out = 10) %&gt;% floor()\n)\n\nhead(hyper_grid)\n##   degree nprune\n## 1      1      2\n## 2      2      2\n## 3      3      2\n## 4      1     12\n## 5      2     12\n## 6      3     12\n\n使用caret执行10折CV的网格搜索。结果显示最佳组合的模型包括二阶交互效应，并保留56个项，最佳模型的交叉验证RMSE为23363.42美元。\n\n# 交叉验证模型\nset.seed(123)  # 为了可重现性\ncv_mars &lt;- train(\n  x = subset(ames_train, select = -Sale_Price),\n  y = ames_train$Sale_Price,\n  method = \"earth\",\n  metric = \"RMSE\",\n  trControl = trainControl(method = \"cv\", number = 10),\n  tuneGrid = hyper_grid\n)\n\n# 查看结果\ncv_mars$bestTune\n##    nprune degree\n## 16     56      2\n\ncv_mars$results %&gt;%\n  filter(nprune == cv_mars$bestTune$nprune, degree == cv_mars$bestTune$degree)\n##   degree nprune     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD\n## 1      2     56 23363.42 0.9161586 15755.06 1861.729 0.01315104 789.6814\n\nggplot(cv_mars)\n\n\n\n\n\n\n\n\n可以在上述网格搜索的基础上进一步优化模型调参，例如在56附近比较保留45-65个项。"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#特征解释-3",
    "href": "bigdata/machlearn/regression.html#特征解释-3",
    "title": "有监督学习：回归模型",
    "section": "4.5 特征解释",
    "text": "4.5 特征解释\n通过earth::earth()拟合的MARS模型包括一个向后剔除特征的选择程序（修剪），该程序检查添加每个预测变量时GCV误差估计的减少量，将其作为变量重要性度量。另一种重要性度量是添加项时残差平方和（RSS）的变化。这两种方法之间的差异很小。\n\n# 变量重要性图\np1 &lt;- vip(cv_mars, num_features = 40, geom = \"point\", value = \"gcv\") + ggtitle(\"GCV\")\np2 &lt;- vip(cv_mars, num_features = 40, geom = \"point\", value = \"rss\") + ggtitle(\"RSS\")\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n变量重要性仅测量特征纳入模型时对预测误差的影响，并不测量特征所创建的铰链函数的影响。为了更好地理解这些特征与Sale_Price之间的关系，可以为每个特征单独创建部分依赖图（PDPs），也可以创建两个特征的联合依赖图。\n下面单独PDPs表明模型发现每个特征中的一个节点提供了最佳拟合。例如，对于面积超过节点的房屋，销售价格的边际增量高于面积未超过节点的房屋。类似地，对于2002年之后建造的房屋，房屋年龄对销售价格的边际效应大于2002年之前建造的房屋。交互图（最右侧图）展示了这两个特征组合时的更强效应。\n\n# 构建部分依赖图\np1 &lt;- partial(cv_mars, pred.var = \"Gr_Liv_Area\", grid.resolution = 10) %&gt;% \n  autoplot()\np2 &lt;- partial(cv_mars, pred.var = \"Year_Built\", grid.resolution = 10) %&gt;% \n  autoplot()\np3 &lt;- partial(cv_mars, pred.var = c(\"Gr_Liv_Area\", \"Year_Built\"), \n              grid.resolution = 10) %&gt;% \n  plotPartial(levelplot = FALSE, zlab = \"yhat\", drape = TRUE, colorkey = TRUE, \n              screen = list(z = -20, x = -60))\n\n# 并排显示图\ngridExtra::grid.arrange(p1, p2, p3, ncol = 3)"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#类别目标特征的预测-1",
    "href": "bigdata/machlearn/regression.html#类别目标特征的预测-1",
    "title": "有监督学习：回归模型",
    "section": "4.6 类别目标特征的预测",
    "text": "4.6 类别目标特征的预测\nMARS方法和算法可以扩展到处理分类问题和广义线性模型（GLM）。以员工流失数据为例。\n\n# 获取流失数据\ndf &lt;- attrition %&gt;% mutate_if(is.ordered, factor, ordered = FALSE)\n\n# 为 attrition 数据创建训练（70%）和测试（30%）集。\n# 使用 set.seed 确保可重现性\nset.seed(123)\nchurn_split &lt;- rsample::initial_split(df, prop = .7, strata = \"Attrition\")\nchurn_train &lt;- rsample::training(churn_split)\nchurn_test  &lt;- rsample::testing(churn_split)\n\n# 为了可重现性\nset.seed(123)\n\n# 交叉验证模型\ntuned_mars &lt;- train(\n  x = subset(churn_train, select = -Attrition),\n  y = churn_train$Attrition,\n  method = \"earth\",\n  trControl = trainControl(method = \"cv\", number = 10),\n  tuneGrid = hyper_grid\n)\n\n# 最佳模型\ntuned_mars$bestTune\n##   nprune degree\n## 3     23      1\n\n# 绘制结果\nggplot(tuned_mars)\n\n\n\n\n\n\n\n\n网格搜索中30种不同超参数组合的交叉验证准确率。最佳模型保留23个项，不包含交互效应。但是，MARS模型与之前的线性模型（逻辑回归和正则化回归）相比，没有看到整体准确率的改善。\n\n\n\n调优MARS和回归模型的交叉验证准确率结果\n\n\n\nMin.\n1st Qu.\nMedian\nMean\n3rd Qu.\nMax.\nNA's\n\n\n\n\nLogistic_model\n0.8058252\n0.8529412\n0.8786765\n0.8715662\n0.8907767\n0.9320388\n0\n\n\nElastic_net\n0.8349515\n0.8671757\n0.8823529\n0.8793902\n0.8980583\n0.9223301\n0\n\n\nMARS_model\n0.8431373\n0.8467304\n0.8550691\n0.8677300\n0.8810680\n0.9215686\n0\n\n\n\n\n\nMARS有几个优点。首先，MARS自然处理混合类型的预测变量（定量和定类）。MARS将定类预测变量的类别分成两组，考虑所有可能的二元划分。每组随后生成一对分段指示函数。MARS对特征工程（变量变换）的要求最低（例如，特征缩放）并会自动执行特征选择。由于MARS扫描每个预测变量以识别改善预测准确性的分割点，因此不会选择无信息的特征。此外，高度相关的预测变量对预测准确性的影响不像在OLS模型中那样大。\n然而，MARS模型的一个缺点是训练速度通常较慢。由于算法扫描每个预测变量的每个值以寻找潜在的切割点，随着样本数\\(n\\)和特征数\\(p\\)的增加，计算性能可能会下降。此外，尽管相关预测变量不一定妨碍模型性能，但它们可能使模型解释变得困难。当两个特征几乎完全相关时，算法通常会选择它在扫描特征时遇到的第一个特征。然后，由于它随机选择了前一个，后一个相关特征可能不会被包含，因为它没有额外的解释能力。"
  },
  {
    "objectID": "bigdata/machlearn/regression.html#参考书籍",
    "href": "bigdata/machlearn/regression.html#参考书籍",
    "title": "有监督学习：回归模型",
    "section": "参考书籍",
    "text": "参考书籍\n\nBradley Boehmke & Brandon Greenwell，Hands-On Machine Learning with R，CRC Press, 2020.\n\nPang-Ning Tan 数据挖掘导论（第2版），机械工业出版社，2019.\n\nIan Foster等 Big Data and Social Science: Data Science Methods and Tools for Research and Practice, CRC Press, 2021."
  },
  {
    "objectID": "bigdata/intro/introduction.html",
    "href": "bigdata/intro/introduction.html",
    "title": "公共管理大数据分析课程介绍",
    "section": "",
    "text": "随着数字政府、智慧城市和数据驱动治理的发展，公共管理领域面临着前所未有的数据挑战与分析需求。传统的政策分析方法已难以应对海量、多源、异构数据所承载的复杂治理问题。\n新型的“大数据”催生了一个全新的研究领域——数据科学。该领域开发了新的数据创建和收集方式，提出了新的分析技术，并提供了新的可视化和信息呈现方式。这些变化改变了公共管理研究者的工作性质，R语言和Python变得与SPSS和Stata一样，成为了公共管理研究重要的工具包。同时，研究的数据来源也在发生变化。公共管理研究越来越多地依赖“发现”数据，而非通过调查“制造”的数据；以调查数据为基础的文章在顶级学术期刊上发表的比例正在下降。此外，随着新工具的运用，研究工作流程变得更加自动化、可复制和可重现（Yarkoni等人，2019）。\n2025年3月24日，全国哲学社会科学工作办公室发布的《2025年国家社会科学基金年度项目申报公告》标志着中国STS研究发展的一个历史性里程碑。公告中，“科学技术与社会”（Science, Technology and Society，简称STS）被正式确立为一级学科条目（代码KXA-KXI）。\n本次国家社科基金共立项STS学科相关课题140项，其中重点项目9项、一般项目81项、青年项目58项、西部项目12项。从立项课题的主题分布来看，呈现出高度的时代性与聚焦性。“人工智能”，特别是 “生成式人工智能” 与 “大模型”，构成了本次立项的绝对核心议题，彰显了学术界对前沿颠覆性技术的高度敏感与积极回应。与此紧密耦合的是对 “治理”、“伦理”、“风险”与“安全” 等议题的系统性探讨，大量课题聚焦于算法偏见、数据安全、技术依赖、责任归因等复杂问题，反映出一种从技术社会影响的被动应对转向主动风险预判与伦理规制的学术范式转型。此外，研究议题广泛渗透至就业结构、教育变革、医疗健康、城市治理、农业现代化等具体社会场域，体现了STS研究深入社会肌理、服务民生的实践关怀。\n本课程旨在帮助研究生系统掌握大数据分析的基础理论、关键方法与实用工具，提升其在公共政策评估、社会治理优化、舆情研判、公共服务供给等方面的数据分析与实证研究能力，为其从事数据驱动的公共事务研究与实践奠定方法论基础，增强服务数字时代公共治理的能力。"
  },
  {
    "objectID": "bigdata/intro/introduction.html#课程目的",
    "href": "bigdata/intro/introduction.html#课程目的",
    "title": "公共管理大数据分析课程介绍",
    "section": "1.1 课程目的",
    "text": "1.1 课程目的\n本课程旨在引导学生系统掌握大数据在公共管理与公共政策分析中的基本原理、方法与应用技能，提升其运用数据驱动手段解决公共事务实践问题和分析研究公共管理学术问题的能力。通过本课程的学习，学生将能够：\n\n理解大数据的基本概念与公共管理的融合趋势，掌握数据驱动治理的核心理念与技术基础；\n提升大数据处理与可视化能力，熟练使用常用大数据分析工具（如Python/R等）进行数据清洗、建模与展示；\n掌握常见的大数据分析方法（如回归分析、分类与聚类、文本分析、空间与网络分析、因果推断等）及其在公共管理中的应用场景；\n培养具备问题导向的大数据分析思维，能够围绕公共管理和公共政策议题，通过多种渠道获取和融合数据，创造性地设计数据分析方案和构建大数据分析模型，提取和展示分析结果并进行政策建议和沟通；\n培养数据伦理与治理敏感性，认识在数据采集、处理与分析过程中可能面临的隐私、偏见与伦理问题，提升公共责任感。"
  },
  {
    "objectID": "bigdata/intro/introduction.html#课程内容",
    "href": "bigdata/intro/introduction.html#课程内容",
    "title": "公共管理大数据分析课程介绍",
    "section": "1.2 课程内容",
    "text": "1.2 课程内容\n\n课程计划\n\n课程介绍：课程结构、大数据的定义与特点、公共管理领域中的数据来源与特征、公共管理中的大数据应用、工具介绍\nR语言基础：基本设置、数据对象、函数、条件执行、循环、大数据编程的策略。\n数据收集与预处理：数据导入与获取、数据清洗与预处理等。\n数据描述与可视化：描述性统计和探索性分析、数据可视化工具与技术、地图与空间可视化等。\n机器学习基础：基本概念、建模过程、特征和目标工程等。\n有监督学习：回归模型：线性回归、逻辑回归、正则化回归、多元自适应回归样条。\n有监督学习：基于决策树的模型：决策树、分袋、随机森林、梯度提升。\n有监督学习：其他方法：KNN、深度学习、支持向量机、堆叠模型、可解释的机器学习。\n无监督学习：降维：主成分分析、广义低秩模型等。\n无监督学习：聚类：K-means、层次聚类等。\n文本分析与自然语言处理（NLP）：文本预处理（分词、词频、TF-IDF）、情感分析、主题建模（LDA）、文本分类、公共管理应用案例。\n空间数据分析基础：空间数据类型、空间数据对象、空间数据的处理、空间大数据的处理\n空间数据分析模型：点模式分析、空间插值、空间自相关、空间回归、空间计量经济学模型\n因果推断与政策评估：数据质量与推断误差、偏差的来源与处理、因果推断方法（双重差分法、倾向得分匹配、工具变量、合成控制）、公共管理应用案例\n期末项目展示与总结：学生展示公共管理相关大数据分析项目、课程总结与展望\n\nNote：课程计划会根据课程进度，适度调整。\n\n\n教学方式\n\n理论与实践比例：约40%理论（13小时）+ 60%实践（19小时），以项目驱动和案例教学为主，强化动手能力。\n\n理论教学：通过讲座和案例讨论，讲解大数据分析的核心方法、原理及公共管理应用场景。\n\n实践教学：通过编程实操、数据分析项目和可视化任务，培养学生处理真实公共管理数据的技能。"
  },
  {
    "objectID": "bigdata/intro/introduction.html#课程考核",
    "href": "bigdata/intro/introduction.html#课程考核",
    "title": "公共管理大数据分析课程介绍",
    "section": "1.3 课程考核",
    "text": "1.3 课程考核\n\n考核方式\n\n考核登记方式：百分制\n\n成绩组成：考勤占10%；平时作业40%；期末论文：50%\n\n考核标准：考勤10分，无故旷课一次扣5分，课前请假不扣分。平时作业40分，建议将平时作业作为期末论文项目的基础。期末论文占50分。\n\n时间要求：最后一次课进行课程论文报告并同时提交课程论文。\n\n课外学习内容：预习复习课程内容，阅读学习材料，学习相关软件的操作，完成练习，准备数据，进行数据分析，写作课程论文。"
  },
  {
    "objectID": "bigdata/intro/introduction.html#课程论文要求",
    "href": "bigdata/intro/introduction.html#课程论文要求",
    "title": "公共管理大数据分析课程介绍",
    "section": "1.4 课程论文要求",
    "text": "1.4 课程论文要求\n\n论文考察目的\n综合考察学生对大数据分析方法的理解与应用能力，重点在运用数据驱动的分析工具解决现实公共管理或政策问题，体现数据分析在实际治理场景中的应用价值。\n\n\n选题方向\n可围绕以下方向选题，或根据兴趣自拟题目。\n\n城市治理中的数据分析（如交通拥堵、垃圾分类、应急管理）\n政策效果评估（如教育公平、环保政策、社会救助）\n公共服务优化（如医疗资源配置、养老服务、大众交通）\n舆情分析与政府回应\n空间数据与公共资源布局\n开放数据与政府透明度分析\n利用文本分析研究政策文件或民众意见（如政府工作报告、留言数据）\n公共健康数据分析（如疫情传播、健康资源分布）\n\n\n\n论文结构建议\n\n引言：说明研究背景、问题与研究意义。\n文献综述：简要梳理相关研究成果，突出研究差异或创新点。\n研究设计：提出研究问题与假设，说明数据来源与分析方法。\n数据分析与结果展示：使用至少一种大数据分析方法（如分类、聚类、文本分析、网络分析等），通过图表和文字说明结果。\n讨论与政策建议：对分析结果进行解释，回应文献，提出可行的管理或政策建议。\n结论与不足：总结研究发现，指出研究局限与未来改进方向。\n\n\n\n具体要求\n\n字数要求：不少于 4000 字；\n数据要求：须使用真实数据（可来自政府开放平台、科研数据集、爬取的公开数据等），并附数据来源说明；\n方法要求：需至少运用一种大数据分析方法（可结合 Python、R 等工具）；\n独立完成：鼓励相互学习帮助；\n论文格式：\n\n中文宋体小四或英文 Times New Roman 12pt\n参考文献建议按照《公共管理学报》要求\n\n中间过程或不重要的细节可以放在附录中\n\n明确写出所建立的模型\n\n用公式编辑器编辑用到的数学公式，如果会用LaTex排版更好\n\n数字保留两位小数或者保留到你认为足够简单清晰的位数\n\n图表应简明扼要，展示论点，不是展示过程，不应过度使用\n\n图表应独立设定标题，图表分开标号，下方段落应有文字介绍\n\n建议挑出图表中的具体某个数字或信息进行举例，帮助理解\n\n表格采用三线表，不要每行每列都添加分隔线\n\n表格中有百分数，应在首行标注‘%’，不要每个单元都用。\n\n表格中每列数字的小数点应对齐。\n\n图表大小应合适，信息量小的图尽量小。\n\n可以用图或者表的地方，尽量用图。\n\n\n\n\n提交形式\n\n电子版材料： 包括下列4部分内容，打包成zip或rar文件，文件名是学号+姓名。\n\n\n说明文件（*.txt、*.docx或者*.pdf等，对期末论文项目涉及的文件做出说明，列明所有的文件内容）\n\n数据文件（*.csv, *.dta, *.xlsx等）\n\n程序脚本文件（*.R，*.py）\n\n论文（*.docx或者*.pdf）\n\n\n\n纸版论文：最后一次课提交，封面包括论文题目、姓名、学号，左上角装订。\n截止时间：纸版论文在最后一次课提交；电子版材料最后一次课后隔周中午12点，成绩以电子版为准！\n\n\n\n评估标准\n\n选题与问题意识（20%）\n数据处理与方法应用（30%）\n结果分析与政策建议（30%）\n写作规范与表达逻辑（10%）\n创新性与可操作性（10%）"
  },
  {
    "objectID": "bigdata/intro/introduction.html#什么是大数据",
    "href": "bigdata/intro/introduction.html#什么是大数据",
    "title": "公共管理大数据分析课程介绍",
    "section": "2.1 什么是大数据",
    "text": "2.1 什么是大数据\n关于大数据的定义很多。有将大数据定义为任何无法放入电脑的数据，也有将其定义为具有高容量、高速度和高多样性的数据。美国公共舆论研究协会（American Association of Public Opinion Research）的描述：“’大数据’是一个不精确的术语，描述了一组丰富而复杂的特性、实践、技术、伦理问题和与数据相关的结果”（Japec等人，2015）。\n出于应用目的，可以将大数据定义为由于其规模和复杂性而难以处理和难以直接从中提取价值的数据。大数据的处理之所以困难，一方面是因为数据常常来自非传统来源，导致数据结构不佳（例如，原始文本、网页、图像等）；另一方面是因为存储和加载处理大量数据需要专门的基础设施。此外，统计计算本身也成为一个挑战。\n\n处理数据来源、结构和格式的复杂性与多样性使得公共管理领域的研究变得越来越具挑战性。\n\n一方面，政府信息和流程的持续数字化推动了各种经济、社会、行政数据的生成和存储，使得此类数据在分析上变得更加可用。然而，另一方面，这种行政数字化的首要关注点通常是直接与信息交互并参与这些流程的政府部门与公众，而不是稍后可能有兴趣分析这些数据的研究者。因此，用于系统性地收集此类数据的接口通常不是为分析目的而优化的（例如，政务服务平台、线上报税、在线咨询投诉平台）。此外，数据可能以半结构化格式出现，例如网页（即超文本标记语言（HTML））、原始文本，甚至图像——每种格式都需要不同的导入/加载和预处理方法。本课程会在文本分析、空间数据分析介绍复杂多样大数据的导入、处理和分析方法。\n\n大 P 问题\n\n数据集的变量（列）数量接近甚至超过观测值（行）数量，这使得使用传统定量技术寻找预测模型变得困难。例如，某地方政府希望预测哪些居民可能参与某项社会福利项目，以优化资源分配。他们收集了居民数据，包括年龄、收入、家庭规模、教育水平、就业状态、居住区域、医疗记录、以往福利申请历史等。此外，还包括行为数据（如是否参加过社区活动）和环境变量（如附近公共服务设施数量）。假设有 10,000 名居民（观测值，行），但变量（列）达到 15,000 个（例如，细分的职业类别、历史申请的每种类型、区域特征的多种指标）。变量数（15,000）超过观测值数（10,000），传统线性回归无法有效处理，因为模型可能过拟合（捕捉噪声而非真实模式）或估计不稳定（多重共线性）。可能导致参数估计不可靠，预测准确性差。变量过多且可能高度相关（如收入和职业），需借助机器学习方法（如 lasso、岭回归、随机森林）来降维或正则化，以构建可靠的预测模型。本课程会在机器学习部分介绍其原理和应用。\n\n大 N 问题\n\n数据集拥有海量的观测值（行），以至于标准数据分析技术或一般计算机无法处理。例如，一个大城市（如北京市）的公共交通系统每天处理数百万次刷卡记录（地铁、公交）。假设城市有 2,000 万日活跃用户，每人每天平均产生 4 条记录（上下班往返），包含时间、站点、票价、支付方式等 30 个变量。单日数据集有 2,000 万 × 4 = 8,000 万行，5 年数据达 8,000 万 × 365 × 5 = 1,460 亿行，占用数TB存储空间。如果要分析高峰时段客流模式以优化调度，如此庞大的行数（1,460 亿）无法加载到标准计算机内存，传统工具（如 R 或 Python ）在单机上处理时会因内存不足崩溃。即使简单计算（如按站点统计日均客流）也可能耗时数小时，复杂分析（如预测拥堵）几乎不可行。标准工具（如 Excel、SPSS）或普通计算机因内存和计算能力限制无法胜任，需借助大数据技术（如分布式计算框架 Spark、Hadoop 或云平台）来存储和分析数据。本课程会在大数据分析工具部分介绍这些技术，同时在专题分析部分介绍这些技术的应用。"
  },
  {
    "objectID": "bigdata/intro/introduction.html#分析大数据的方法",
    "href": "bigdata/intro/introduction.html#分析大数据的方法",
    "title": "公共管理大数据分析课程介绍",
    "section": "2.2 分析大数据的方法",
    "text": "2.2 分析大数据的方法\n总体而言，有四种解决与分析大 N（大量观测值）和大 P（大量变量）数据中相关挑战的方法。\n\n统计与机器学习\n\n为什么不直接在大数据中抽样，获取一个合理的样本，然后采用传统的统计学方法进行分析？统计推断是在收集全体数据实际上不可行或成本过高的情况下回答实证问题的有效手段。如果已经能够获取大量的数据，在预期效果较大的情况下，使用成熟的“传统”统计学方法来恰当回答实证问题可能是有意义的。如果在大数据分析中预期效果（effect size）很小，则不能简单从大型数据集抽取随机样本，只有通过足够大的样本量（large N）才能获得足够的统计功效（statistical power），从而检测出显著差异并做出可靠决策。否则，小样本可能无法区分真实效果与随机噪声，尤其当决策的影响仅在少数情况下显现时。\n例如，研究者评估某项公共健康干预措施（如全国性疫苗推广计划）对降低特定疾病发病率的影响。假设干预预期效果很小——例如，只降低发病率0.5%（effect size小），因为大多数人已通过其他方式获得免疫。如果从全国数亿条健康记录中随机抽取小样本（如10万条），统计功效不足，无法可靠检测出这0.5%的差异（p值可能不显著，看起来像零效果）。在少数高风险人群中，效果才显现（如老人或偏远地区居民）。因此，需要分析整个大 N 数据集（数亿行记录）来积累足够的统计功率，区分真实政策效果与噪声，从而为政府决策提供依据（如是否继续推广疫苗）。这种小效果常见于政策评估，如果抽样，可能会错失微弱但重要的信号，导致无效结论。但是，已有成熟的机器学习方法（例如分布式随机森林）可以更好地解决这些问题。机器学习方法及应用是本课程介绍的重点内容，帮助解决传统统计方法在分析大数据中面临的困境。\n\n编写高效代码\n\n在处理中小规模数据集时，数据分析脚本是否高效编写可能无关紧要。然而，面对大型数据集，运行脚本可能会变得不顺畅，或者完全无法顺利运行。因此，需要了解如何在 R 中编写高效快速代码。本课程会在R语言基础部分，介绍进行R语言的高效编程的原则。\n\n有效利用本地计算资源\n\n通常电脑操作系统会自动采用一些策略分配和使用可用硬件资源来完成数据分析任务。同时，一些R语言包提供了一些多核处理和虚拟内存的高效使用的策略。这些策略也有成本，只有在数据集达到一定规模后，才能节省时间。因此，研究者需要了解决定如何更有效地利用本地计算环境来加速特定分析任务。本课程会针对不同专题（可视化、文本分析、空间数据等），介绍有效利用本地计算资源针对不同类型数据完成不同分析任务的策略。\n\n扩展和分布\n\n如果在采取上述方法后，依然无法在本地完成任务，则需要考虑扩展（scale up）或分布（scale out）可用计算资源。扩展指在云端租用虚拟服务器，分布意味着协同使用多台机器（集群计算机、分布式系统）。扩展通常不需要熟悉专用软件，可以在云端更大的机器上运行与本地测试相同的脚本。分布需要熟悉一些软件组件。本课程会简要介绍如何部署扩展和利用相关软件包进行分布。\n方法选择的顺序是：能否抽样后用传统方法，是否有机器学习等新技术，代码是否高效，本地计算资源是否有效，能够利用扩展和分布的计算资源解决。\n\n解决大P问题的示例\n\n假设某电子商务平台打算根据用户和上网浏览会话特征预测购买行为（即用户在某次会话中实际购买某物的概率）。因变量 purchase 是二值变量，若购买则为 1，否则为 0。其他变量包含用户和会话信息（用户位于何处？使用哪种浏览器？等等）。\n\n# 导入数据\nga &lt;- read.csv(\"ga.csv\")\nhead(ga[, c(\"source\", \"browser\", \"city\", \"purchase\")])\n\n    source browser          city purchase\n1   google  Chrome      San Jose        1\n2 (direct)    Edge     Charlotte        1\n3 (direct)  Safari San Francisco        1\n4 (direct)  Safari   Los Angeles        1\n5 (direct)  Chrome       Chicago        1\n6 (direct)  Chrome     Sunnyvale        1\n\n# 将source变量里面的每个网站转化成二值的虚拟变量，并添加因变量purchase\nmm &lt;- cbind(ga$purchase,\n            model.matrix(purchase~source, data=ga,)[,-1])\nmm_df &lt;- as.data.frame(mm)\n# 整理变量名\nnames(mm_df) &lt;- c(\"purchase\",\n                  gsub(\"source\", \"\", names(mm_df)[-1]))\n# 运行logit回归\nmodel1 &lt;- glm(purchase ~ .,\n              data=mm_df, family=binomial)\n\nmodel1_sum &lt;- summary(model1)\n# 筛选显著的变量\npvalues &lt;- model1_sum$coefficients[,\"Pr(&gt;|z|)\"]\nvars &lt;- names(pvalues[which(pvalues&lt;0.05)][-1])\nvars\n\n [1] \"bing\"                    \"dfa\"                    \n [3] \"docs.google.com\"         \"facebook.com\"           \n [5] \"google\"                  \"google.com\"             \n [7] \"m.facebook.com\"          \"Partners\"               \n [9] \"quora.com\"               \"siliconvalley.about.com\"\n[11] \"sites.google.com\"        \"t.co\"                   \n[13] \"youtube.com\"            \n\n# 确定最终的模型\nfinalmodel &lt;- glm(purchase ~.,\n                  data = mm_df[, c(\"purchase\", vars)],\n                  family = binomial)\n\nsummary(finalmodel)$coef[,c(\"Estimate\", \"Pr(&gt;|z|)\")]\n\n                          Estimate      Pr(&gt;|z|)\n(Intercept)             -1.3831286  0.000000e+00\nbing                    -1.4646836  4.415693e-03\ndfa                     -0.1865031  1.271359e-01\ndocs.google.com         -2.0180688  4.713922e-02\nfacebook.com            -1.1663166  3.873468e-04\ngoogle                  -1.0148614 6.320540e-168\ngoogle.com              -2.9606768  3.193285e-05\nm.facebook.com          -3.6920452  2.330779e-04\nPartners                -4.3747211  3.942196e-14\nquora.com               -3.1277309  1.868612e-03\nsiliconvalley.about.com -2.2456469  1.241664e-04\nsites.google.com        -0.5968274  1.355800e-03\nt.co                    -2.0508586  4.316117e-03\nyoutube.com             -6.9935344  4.197158e-23\n\n\n在自变量数目比较多的情况下直接采用logistic回归构建预测模型会存在问题。初始模型有62个解释变量（加上截距），对系数的62个假设检验中，弃真错误概率5%，那么平均而言会错误拒绝3个“无预测效果”的原假设，尽管实际上没有预测效果。如果解释变量增加，错误数目还会增大。此外，原始变量间存在较大的相关性，添加或移除一个变量可能显著影响模型的预测能力。最终模型估计中筛选的最终变量dfa在模型中又变得不显著了。\n另一种方法是基于所有可能的协变量组合估计模型（完整的组合是2的61次方！），然后根据某种样本外预测性能指标选择最终模型。显然，这种方法计算时间会很长。\nlasso 估计提供了方便且高效的方法来获得一系列候选模型。lasso 的核心思想是在估计过程中惩罚模型复杂性，复杂性是造成模型不稳定的主要原因。然后，可以根据 k 折交叉验证的样本外预测方法，从候选模型序列中选择最终模型。下面基于 gamlr 包采用lasso 用于生成候选模型序列，基于 k 折交叉验证选择最佳模型。\n\nlibrary(gamlr)\n# 创建模型矩阵\nmm &lt;- model.matrix(purchase~source, data = ga)\n\n# 更节省内存的方式，创建稀疏矩阵（因为模型矩阵中存在大量的0）\nmm_sparse &lt;- sparse.model.matrix(purchase~source, data = ga)\n# 比较两种方式占用内存大小\nas.numeric(object.size(mm)/object.size(mm_sparse))\n\n[1] 7.524862\n\n# 运行带 k 折交叉验证 lasso 估计，通过惩罚降低模型复杂性\ncvpurchase &lt;- cv.gamlr(mm_sparse, ga$purchase, family=\"binomial\")\n\nlibrary(PRROC)\n# 采用最佳模型进行预测\n# 模型选择依据样本外偏差最小的原则，可以避免过度拟合\npred &lt;- predict(cvpurchase$gamlr, mm_sparse, type=\"response\")\n# 计算 tpr, fpr; 绘制 ROC 曲线\ncomparison &lt;- roc.curve(scores.class0 = pred,\n                        weights.class0=ga$purchase,\n                        curve=TRUE)\nplot(comparison)\n\n\n\n\n\n\n\n\n上面的例子通过稀疏矩阵节省内存使用，通过lasso估计技术提供了合理的方法选择预测模型，解决了大P问题。\n\n解决大N问题的示例\n\n线性回归是统计分析的重要方法。但在面对观测量巨大的大数据集，以最小二乘法估计回归系数可能会运行缓慢，或者无法运行。而新的算法Uluru能够替代最小二乘法，提供更有效率的解决方案。下面的例子比较两种算法的运算时间，估计的回归系数与真实值之间的差距。整个模拟数据的观测量是1千万个，分成不同大小的子样本量（没间隔50万个）来检验。\n\n\n           [,1]\n[1,]  1.9980919\n[2,]  1.4995462\n[3,]  4.0000350\n[4,] -3.4996061\n[5,]  0.5001466\n\n\n           [,1]\n[1,]  2.0319591\n[2,]  1.4982386\n[3,]  3.9973941\n[4,] -3.4999876\n[5,]  0.5010381\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n水平红线表示通过 OLS 估计的计算时间；绿色点表示通过 Uluru 算法估计的计算时间。即使对于较大的子样本，计算时间也显著低于 OLS。\n水平红线表示使用 OLS 时估计系数的大小。水平绿线表示实际系数的大小。绿色点表示 Uluru 算法在不同子样本大小下估计的同一系数大小。即使是相对较小的子样本也能提供非常接近 OLS 估计的结果。综合来看，该示例说明了适用于大数据的优化算法可以提供非常接近传统方法的结果，同时具有更高的计算效率，也更节省资源。"
  },
  {
    "objectID": "bigdata/intro/introduction.html#公共管理领域的主要数据来源",
    "href": "bigdata/intro/introduction.html#公共管理领域的主要数据来源",
    "title": "公共管理大数据分析课程介绍",
    "section": "3.1 公共管理领域的主要数据来源",
    "text": "3.1 公共管理领域的主要数据来源\n公共管理的数据来源极其广泛，可以概括为六大类：政府统计与行政数据、调查数据、公共事务运行数据、大数据与新兴数据源、国际与区域组织数据、地理空间数据。\n\n政府统计与行政数据\n\n各类人口、经济、社会、环境统计年鉴。\n行政部门业务数据（如社保、医保、税收、教育、交通、户籍管理等）。\n行政执法与司法数据（行政处罚记录、法院判决书、案件受理数据等）。\n政策文件、预算与决算报告、审计报告。\n\n调查数据\n\n大规模社会调查（如人口普查、住户调查、就业与收入调查）。\n政策评估和民意调查。\n学术机构、智库、第三方调查公司收集的数据（CGSS、CFPS、CHIP、CHARLS）。\n\n公共事务运行数据\n\n公共服务绩效考核数据（如公共交通运行效率、医疗服务质量）。\n城市管理数据（垃圾分类、环境监测、公共安全监控）。\n公共工程与项目执行过程数据。\n\n大数据与新兴数据源\n\n网络与社交媒体数据（政务平台的网民留言、微博等社交媒体上的公众舆论、网络新闻热点等）。\n移动通信与位置数据（GPS、地铁刷卡、共享单车使用数据）。\n互联网平台数据（电商、外卖、出行平台产生的交易与行为记录）。\n物联网与传感器数据（环境监测、智慧城市中的交通摄像头、传感器网络）。\n\n国际与区域组织数据\n\n联合国、世界银行、OECD等机构数据库。\n区域合作组织数据（如欧盟统计局、东盟数据中心）。\n\n地理空间数据\n\n地理空间数据横跨多个类别，在实际公共管理研究中，往往是作为 跨领域的支撑性数据层 存在，为各类统计、运行、调查和大数据提供地理定位与空间分析基础。\n卫星遥感影像、电子地图、地质地貌、土地利用数据等，用于城市规划、灾害预警、资源调查。\n\n\n\n\n\n\n\n\n\n\n数据来源\n主要特征\n应用场景\n\n\n\n\n政府统计与行政数据\n权威性强、周期性更新、结构化程度高，但更新滞后\n政策制定、人口与经济分析、预算管理、绩效评估\n\n\n调查数据\n具有代表性、针对性强，样本有限，成本高\n政策评估、公众满意度调查、社会治理研究\n\n\n公共事务运行数据\n贴近实际、连续性强，可能存在部门间割裂\n公共服务绩效考核、城市管理、应急管理\n\n\n大数据与新兴数据源（社交媒体、移动通信、平台交易、物联网等）\n实时性强、体量大、非结构化多，需处理隐私问题\n舆情监测、智慧城市治理、应急响应、交通与环境监控\n\n\n国际与区域组织数据\n标准化程度高、跨国可比性强，宏观性突出\n跨国比较研究、国际合作、可持续发展目标（SDGs）评估\n\n\n地理空间数据\n带有位置属性、跨领域、可与其他数据叠加分析\n城市规划、区域治理、环境监测、应急调度"
  },
  {
    "objectID": "bigdata/intro/introduction.html#公共管理数据的核心特征",
    "href": "bigdata/intro/introduction.html#公共管理数据的核心特征",
    "title": "公共管理大数据分析课程介绍",
    "section": "3.2 公共管理数据的核心特征",
    "text": "3.2 公共管理数据的核心特征\n公共管理数据除了具备一般大数据的“4V”特征（Volume大量、Velocity高速、Variety多样、Value低价值密度）外，还具有其独特的领域特征：\n\n多源性与异质性\n\n数据来源跨越政府部门、社会组织、企业和公众。\n格式多样：结构化（统计表）、半结构化（日志、JSON）、非结构化（文本、图片、视频）。\n\n公共性与开放性\n\n涉及公共利益，具有共享和开放潜力。\n近年来各国推动“政府数据开放”，增强透明度和公民参与。\n\n动态性与实时性\n\n大数据尤其强调实时更新（如交通流量、疫情监测）。\n传统统计数据则更新周期较长。\n\n空间性与区域差异\n\n很多公共管理数据带有地理属性（GIS数据、城乡差异）。\n适合进行空间分析、区域治理比较。\n\n敏感性与隐私性\n\n涉及个人信息（户籍、医保、社保）、组织运营信息。\n需要数据安全与隐私保护机制。\n\n复杂性与关联性\n\n不同数据之间存在跨领域联系（如教育与就业、环保与健康）。\n分析中需多维度交叉验证，构建综合性模型。\n\n质量参差不齐\n\n存在数据缺失、偏差、更新滞后等问题。\n行政数据与现实情况可能存在差距。"
  },
  {
    "objectID": "bigdata/intro/introduction.html#公共管理实践中的大数据应用",
    "href": "bigdata/intro/introduction.html#公共管理实践中的大数据应用",
    "title": "公共管理大数据分析课程介绍",
    "section": "4.1 公共管理实践中的大数据应用",
    "text": "4.1 公共管理实践中的大数据应用\n在公共管理中，大数据的应用已经逐渐从“辅助决策”走向“驱动治理创新”。它不仅改变了政府的治理方式，也提升了公共服务的精准性和效率。\n\n社会治理与公共安全\n\n应用场景：智慧警务（人脸识别、视频监控、预测性警务）、治安风险预测、灾害应急预警。\n价值：通过对人流、物流、网络舆情的实时分析，提高社会治理的前瞻性和精准性。\n\n公共服务优化\n\n应用场景：医疗健康（电子病历大数据、疫情传播监测）、教育资源配置（学生学业数据分析）、交通出行（智慧交通、实时路况调度）。\n价值：实现服务的个性化和差异化，提高资源配置效率。\n\n政策制定与评估\n\n应用场景：利用多源数据分析社会需求与政策效果，如住房保障政策效果评估、精准扶贫效果监测。\n价值：增强政策的科学性和针对性，推动基于证据的政策制定（evidence-based policy）。\n\n经济与社会运行监测\n\n应用场景：宏观经济运行监测（电商交易数据、金融交易数据）、就业与劳动市场监控、环境污染与碳排放监控。\n价值：弥补传统统计的滞后性，形成实时监测与预警机制。\n\n政务透明与公众参与\n\n应用场景：政府数据开放平台、在线政务服务平台、舆情监测与反馈渠道。\n价值：提高政府透明度，促进公众监督和协同治理。"
  },
  {
    "objectID": "bigdata/intro/introduction.html#公共管理研究中的大数据分析方法应用",
    "href": "bigdata/intro/introduction.html#公共管理研究中的大数据分析方法应用",
    "title": "公共管理大数据分析课程介绍",
    "section": "4.2 公共管理研究中的大数据分析方法应用",
    "text": "4.2 公共管理研究中的大数据分析方法应用\n\n文本挖掘与自然语言处理（NLP）\n\n\n应用：利用政策文件、新闻报道、预算文本等非结构化数据进行分析。\n案例：\n\nGreer et al. (2023)：通过计算方法分析地方政府预算叙事，揭示政府如何通过语言传递“韧性”信号（computational text analysis）。\nChen et al. (2023)：分析媒体声誉维度对机构撤销的影响，体现了基于媒体文本的声誉数据挖掘。\nBoon et al. (2025)：利用媒体报道数据和文本特征，研究机构结构改革与声誉的关系。\n\n方法特点：能捕捉到话语框架、媒体声誉、政策叙事对公共治理的影响。\n\n\n机器学习与因果推断\n\n\n应用：通过机器学习和计量方法发现因果关系、预测政策效果。\n案例：\n\n韩先锋 & 李佳佳 (2025)：使用“双重机器学习”方法，研究数智化对城市绿色增长的因果效应。\nChen et al. (2024)：利用可解释的机器学习和多目标优化，进行北京城市绿色基础设施规划。\nBa et al. (2025)：通过多维度政策引文特征和机器学习，揭示政策采纳中的决策逻辑。\n\n方法特点：在大规模数据中寻找因果效应，结合可解释性方法提高政策可用性。\n\n\n网络分析（Network Analysis）\n\n\n应用：刻画治理平台、协作网络、政策引用网络中的关系结构。\n案例：\n\nZufall et al. (2025)：研究环境治理平台中的委托代理矛盾，数据分析方法涉及治理网络关系。\nAmbrose & Siddiki (2024)：分析协同治理安排中的持续参与因素，运用网络数据识别合作动力。\nWang et al. (2024)：利用咨询网络分析环境影响评估中的机构互动。\nBa et al. (2025)：基于政策引文网络，研究政策采纳过程。\n\n方法特点：揭示公共管理中的多主体互动模式与治理网络结构。\n\n\n空间大数据与GIS分析\n\n\n应用：结合遥感数据、地理数据进行城市规划、环境治理和公共服务公平性研究。\n案例：\n\nChen et al. (2024)：利用空间大数据和优化算法进行城市绿色基础设施规划。\nChai-allah et al. (2025)：利用众包图片和机器学习，分析公众对景观和物种的偏好。\n王振坡等 (2025)：基于调查和空间通勤数据，研究低碳出行方式选择。\n\n方法特点：能体现空间异质性与公共服务分布差异，支持精细化治理。\n\n\n情感与舆情分析\n\n\n应用：通过媒体、社交平台数据捕捉公众情绪和政策态度。\n案例：\n\nBoon et al. (2025)、Chen et al. (2023)：通过媒体数据分析政府机构声誉，间接反映社会舆情。\nChai-allah et al. (2025)：通过图像和用户数据反映公众对自然景观的偏好，也是一种情感/态度分析。\n\n方法特点：能揭示社会信任、声誉、公众情绪对治理成效的影响。\n\n\n规范性与优化分析（Prescriptive Analytics）\n\n\n应用：通过模拟与优化为政策设计提供最优方案。\n案例：\n\nChen et al. (2024)：结合多目标优化算法，提出城市绿色基础设施的最优布局。\n杨丹等 (2025)：利用实证数据评估区域品牌对农民收入的效应，为乡村振兴提供政策依据。\n\n方法特点：不仅揭示现状，还能为政策制定提供可操作性方案。"
  },
  {
    "objectID": "bigdata/visualization/visualization.html",
    "href": "bigdata/visualization/visualization.html",
    "title": "数据描述与可视化",
    "section": "",
    "text": "可视化数据之前，需要准备数据，包括导入数据并进行清洗。\n\n\nR 几乎可以导入任何格式的数据，包括文本、Excel、SPSS和STATA等统计软件等。\n\n\nreadr包提供了将分隔文本文件导入 R 数据框的函数选择。\n\nlibrary(readr)\n\n# 导入逗号分割的数据\n#cgss2017 &lt;- read_csv(\"cgss2017.csv\")\n\n# 导入制表符分割的数据\n#cgss2017 &lt;- read_tsv(\"cgss2017.txt\")\n\n\n\n\nreadxl包可以从 Excel 文件导入数据，支持 xls 和 xlsx 格式。\n\nlibrary(readxl)\n\n# 从excel工作表导入数据\n#cgss2017 &lt;- read_excel(\"cgss2017.xlsx\", sheet=1)\n\n由于excel文件可以包含多个工作表，因此您可以使用sheet选项指定所需的工作表。默认值为sheet=1。\n\n\n\nhaven包提供了从各种统计包导入数据的功能。示例采用的CGSS2015子数据集\n\nlibrary(haven)\n\n# 导入stata数据\n#cgss2017 &lt;- read_dta(\"cgss2017.dta\")\n\n# 导入SPSS数据\ncgss &lt;- read_sav(\"cgss2015subset.sav\")\n\n# 导入SAS数据\n#cgss2017 &lt;- read_sas(\"cgss2017.sas7bdat\")\n\n\n\n\n\n数据清理是数据分析中最耗时的任务。以下列出了最重要的步骤。虽然方法有很多，但使用dplyr和tidyr包是最快捷、最容易学习的方法之一。\n\n\n\n包\n函数\n用途\n\n\n\n\ndplyr\nselect\n选择变量/列\n\n\ndplyr\nfilter\n选择观察值/行\n\n\ndplyr\nmutate\n转换或重新编码变量\n\n\ndplyr\nsummarize\n汇总数据\n\n\ndplyr\ngroup_by\n识别需要进一步处理的子组\n\n\ntidyr\ngather\n将宽格式数据集转换为长格式\n\n\ntidyr\nspread\n将长格式数据集转换为宽格式\n\n\n\n\n\n该select函数从数据集选取指定的变量或列。\n\nlibrary(dplyr)\n\n# 从数据框cgss中选择变量id，sex和age构成新数据集\nnewdata &lt;- select(cgss, id, sex, age)\n\n# 选择变量id以及hp1与hp4之间的所有变量 \nnewdata &lt;- select(cgss, id, hp1:hp4)\n\n# 选择除了edu和lnincome以外的所有变量\nnewdata &lt;- select(cgss, -edu, -lnincome)\n\n\n\n\n该filter函数选择符合特定条件的观测（行）。可以使用&(AND) 和|(OR) 逻辑符号组合多个条件。\n\nlibrary(dplyr)\n\n# 为了方便展示，将带有值标签的变量转换为因子(使用值标签作为取值)   \ncgss &lt;- cgss %&gt;% mutate(across(where(is.labelled), as_factor))\n\n# 选择女性观测\nnewdata &lt;- filter(cgss, sex == \"女\")\n\n# 选择来自山东的女性\nnewdata &lt;- filter(cgss, sex == \"女\" & province == \"山东省\")\n\n# 选择来自东北三省的观测\nnewdata &lt;- filter(cgss, \n                  province == \"辽宁省\" | \n                  province == \"吉林省\" | \n                  province == \"黑龙江省\")\n\n# 更简洁的代码\nnewdata &lt;- filter(cgss, \n                  province %in% \n                    c(\"辽宁省\", \"吉林省\", \"黑龙江省\"))\n\n\n\n\nmutate函数可以创建新变量或转换现有变量。\n\nlibrary(dplyr)\n\n# 将身高从厘米转化成英寸，将体重从斤转化成磅 \nnewdata &lt;- mutate(cgss, \n                  height = height * 0.394,\n                  weight   = (weight/2) * 2.205)\n\n# 如果收入高于8万元，则变量incomecat取值为\"高\"，否则取值为\"低\"\n\nnewdata &lt;- mutate(cgss, \n                  incomecat = ifelse(income &gt; 80000, \n                                     \"高\", \n                                     \"低\"))\n                  \n# 将教育程度不是初中、普通高中、中专的转化为其他类别\nnewdata &lt;- mutate(cgss, \n                  edu = ifelse(edu %in% \n                                     c(\"初中\", \"普通高中\", \"中专\"),\n                                     edu,\n                                     \"其他\"))\n                  \n# 将身高大于200或小于75设定为缺失值\nnewdata &lt;- mutate(cgss, \n                  height = ifelse(height &lt; 75 | height &gt; 200,\n                                     NA,\n                                     height))\n\n\n\n\nsummarize函数可用于描述性统计的数据汇总。可与by_group函数结合使用，用于按组计算统计值。na.rm=TRUE选项用于在计算平均值之前删除缺失值。\n\nlibrary(dplyr)\n\n# 计算身高和体重的平均值\nnewdata &lt;- summarize(cgss, \n                     mean_ht = mean(height, na.rm=TRUE), \n                     mean_weight = mean(weight, na.rm=TRUE))\nnewdata\n\n# A tibble: 1 × 2\n  mean_ht mean_weight\n    &lt;dbl&gt;       &lt;dbl&gt;\n1    164.        122.\n\n# 分性别组计算身高和体重的平均值\nnewdata &lt;- group_by(cgss, sex)\nnewdata &lt;- summarize(newdata, \n                     mean_ht = mean(height, na.rm=TRUE), \n                     mean_wt = mean(weight, na.rm=TRUE))\nnewdata\n\n# A tibble: 2 × 3\n  sex   mean_ht mean_wt\n  &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 男       170.    133.\n2 女       159.    113.\n\n\n\n\n\ndplyr和tidyr软件包允许使用管道运算符以紧凑的格式编写代码%&gt;%，运算符%&gt;%将左边的结果传递给右边的函数的第一个参数。例如：\n\nlibrary(dplyr)\n\nnewdata &lt;- filter(cgss, \n                  sex == \"女\")\nnewdata &lt;- group_by(newdata, province)\nnewdata &lt;- summarize(newdata, \n                     mean_ht = mean(height, na.rm = TRUE))\n\n# 采用管道操作符更简洁，也更符合人类思维\nnewdata &lt;- cgss %&gt;%\n  filter(sex == \"女\") %&gt;%\n  group_by(province) %&gt;%\n  summarize(mean_ht = mean(height, na.rm = TRUE))\n\n\n\n\n在 R 中，日期值以字符的形式输入。例如，记录 3 个人出生日期的简单数据集。\n\ndf &lt;- data.frame(\n  dob = c(\"11/10/1963\", \"Jan-23-91\", \"12:1:2001\")\n)\n\nstr(df) \n\n'data.frame':   3 obs. of  1 variable:\n $ dob: chr  \"11/10/1963\" \"Jan-23-91\" \"12:1:2001\"\n\n\n将字符变量转换为日期变量的方法有很多。最简单的方法之一是使用lubridate包中提供的函数。这些函数包括ymd、dmy和 ，mdy分别用于导入年-月-日、日-月-年和月-日-年的格式。\n\nlibrary(lubridate)\n# 将dob变量值从字符转化为日期型数据\ndf$dob &lt;- mdy(df$dob)\nstr(df)\n\n'data.frame':   3 obs. of  1 variable:\n $ dob: Date, format: \"1963-11-10\" \"1991-01-23\" ...\n\n\n这些值在R的内部记录为自 1970 年 1 月 1日以来的天数，可以方便地执行日期运算，提取日期元素（月、日、年），重新格式化（例如，1963年10月11日）。 日期变量对于制作时间相关图表非常重要。\n\n\n\n有些图表要求数据为宽格式，而有些图表则要求数据为长格式，示例如下。\n将宽数据集转换为长数据集\n\nlibrary(tidyr)\nwide_data &lt;- data.frame(id = c(\"01\", \"02\", \"03\"),\n                        name = c(\"张三\", \"李四\", \"王五\"),\n                        sex = c(\"男\", \"男\", \"女\"),\n                        height = c(70, 72, 62),\n                        weight = c(180, 195, 130))\nknitr::kable(wide_data, caption = \"宽数据\")\n\n\n宽数据\n\n\nid\nname\nsex\nheight\nweight\n\n\n\n\n01\n张三\n男\n70\n180\n\n\n02\n李四\n男\n72\n195\n\n\n03\n王五\n女\n62\n130\n\n\n\n\nlong_data &lt;- pivot_longer(wide_data,\n                          cols = c(\"height\", \"weight\"),\n                          names_to = \"variable\", \n                          values_to =\"value\")\n\n将长数据转化为宽数据\n\nlibrary(tidyr)\nknitr::kable(long_data, caption = \"长数据\")\n\n\n长数据\n\n\nid\nname\nsex\nvariable\nvalue\n\n\n\n\n01\n张三\n男\nheight\n70\n\n\n01\n张三\n男\nweight\n180\n\n\n02\n李四\n男\nheight\n72\n\n\n02\n李四\n男\nweight\n195\n\n\n03\n王五\n女\nheight\n62\n\n\n03\n王五\n女\nweight\n130\n\n\n\n\nwide_data &lt;- pivot_wider(long_data,\n                         names_from = \"variable\",\n                         values_from = \"value\")\n\n\n\n\n真实数据很可能包含缺失值。处理缺失数据有三种基本方法：特征选择、列删除和插补。ggplot2包中的msleep数据集描述了哺乳动物的睡眠习惯，并且在多个变量上存在缺失值。\n\n特征选择 在特征选择中，可以删除包含太多缺失值的变量（列）。\n\n\ndata(msleep, package=\"ggplot2\")\n\n# 每个变量中缺失值的比例\npctmiss &lt;- colSums(is.na(msleep))/nrow(msleep)\nround(pctmiss, 2)\n\n        name        genus         vore        order conservation  sleep_total \n        0.00         0.00         0.08         0.00         0.35         0.00 \n   sleep_rem  sleep_cycle        awake      brainwt       bodywt \n        0.27         0.61         0.00         0.33         0.00 \n\n\n61% 的 sleep_cycle 值缺失。可以决定将其删除。\n\n按列删除\n整行删除包含缺失值的观测。\n\n\nnewdata &lt;- select(msleep, genus, vore, conservation)\nnewdata &lt;- na.omit(newdata)\n\n\n补值\n插补涉及用“合理”的猜测值（假设缺失值不存在时的值）来替换缺失值。有几种方法，详见VIM、mice、Amelia和missForest等包。这里将使用VIMkNN()包中的函数，用插补值替换缺失值。\n\n\n# 用5个最近邻的值插补缺失值\nlibrary(VIM)\nnewdata &lt;- kNN(msleep, k=5)\n\n基本上，对于每个有缺失值的案例，都会选择k个最相似的、没有缺失值的案例。如果缺失值为数值型，则使用这k 个案例的中位数作为插补值。如果缺失值为类别值，则使用这k 个案例中出现频率最高的值。该过程会迭代所有观测和变量，直到结果收敛（趋于稳定）。\n重要提示：缺失值可能会对研究结果造成偏差（有时甚至非常严重）。如果有大量缺失数据，在删除观测或填补缺失值之前，要慎重考虑其合理性。"
  },
  {
    "objectID": "bigdata/visualization/visualization.html#数据准备",
    "href": "bigdata/visualization/visualization.html#数据准备",
    "title": "数据描述与可视化",
    "section": "",
    "text": "可视化数据之前，需要准备数据，包括导入数据并进行清洗。\n\n\nR 几乎可以导入任何格式的数据，包括文本、Excel、SPSS和STATA等统计软件等。\n\n\nreadr包提供了将分隔文本文件导入 R 数据框的函数选择。\n\nlibrary(readr)\n\n# 导入逗号分割的数据\n#cgss2017 &lt;- read_csv(\"cgss2017.csv\")\n\n# 导入制表符分割的数据\n#cgss2017 &lt;- read_tsv(\"cgss2017.txt\")\n\n\n\n\nreadxl包可以从 Excel 文件导入数据，支持 xls 和 xlsx 格式。\n\nlibrary(readxl)\n\n# 从excel工作表导入数据\n#cgss2017 &lt;- read_excel(\"cgss2017.xlsx\", sheet=1)\n\n由于excel文件可以包含多个工作表，因此您可以使用sheet选项指定所需的工作表。默认值为sheet=1。\n\n\n\nhaven包提供了从各种统计包导入数据的功能。示例采用的CGSS2015子数据集\n\nlibrary(haven)\n\n# 导入stata数据\n#cgss2017 &lt;- read_dta(\"cgss2017.dta\")\n\n# 导入SPSS数据\ncgss &lt;- read_sav(\"cgss2015subset.sav\")\n\n# 导入SAS数据\n#cgss2017 &lt;- read_sas(\"cgss2017.sas7bdat\")\n\n\n\n\n\n数据清理是数据分析中最耗时的任务。以下列出了最重要的步骤。虽然方法有很多，但使用dplyr和tidyr包是最快捷、最容易学习的方法之一。\n\n\n\n包\n函数\n用途\n\n\n\n\ndplyr\nselect\n选择变量/列\n\n\ndplyr\nfilter\n选择观察值/行\n\n\ndplyr\nmutate\n转换或重新编码变量\n\n\ndplyr\nsummarize\n汇总数据\n\n\ndplyr\ngroup_by\n识别需要进一步处理的子组\n\n\ntidyr\ngather\n将宽格式数据集转换为长格式\n\n\ntidyr\nspread\n将长格式数据集转换为宽格式\n\n\n\n\n\n该select函数从数据集选取指定的变量或列。\n\nlibrary(dplyr)\n\n# 从数据框cgss中选择变量id，sex和age构成新数据集\nnewdata &lt;- select(cgss, id, sex, age)\n\n# 选择变量id以及hp1与hp4之间的所有变量 \nnewdata &lt;- select(cgss, id, hp1:hp4)\n\n# 选择除了edu和lnincome以外的所有变量\nnewdata &lt;- select(cgss, -edu, -lnincome)\n\n\n\n\n该filter函数选择符合特定条件的观测（行）。可以使用&(AND) 和|(OR) 逻辑符号组合多个条件。\n\nlibrary(dplyr)\n\n# 为了方便展示，将带有值标签的变量转换为因子(使用值标签作为取值)   \ncgss &lt;- cgss %&gt;% mutate(across(where(is.labelled), as_factor))\n\n# 选择女性观测\nnewdata &lt;- filter(cgss, sex == \"女\")\n\n# 选择来自山东的女性\nnewdata &lt;- filter(cgss, sex == \"女\" & province == \"山东省\")\n\n# 选择来自东北三省的观测\nnewdata &lt;- filter(cgss, \n                  province == \"辽宁省\" | \n                  province == \"吉林省\" | \n                  province == \"黑龙江省\")\n\n# 更简洁的代码\nnewdata &lt;- filter(cgss, \n                  province %in% \n                    c(\"辽宁省\", \"吉林省\", \"黑龙江省\"))\n\n\n\n\nmutate函数可以创建新变量或转换现有变量。\n\nlibrary(dplyr)\n\n# 将身高从厘米转化成英寸，将体重从斤转化成磅 \nnewdata &lt;- mutate(cgss, \n                  height = height * 0.394,\n                  weight   = (weight/2) * 2.205)\n\n# 如果收入高于8万元，则变量incomecat取值为\"高\"，否则取值为\"低\"\n\nnewdata &lt;- mutate(cgss, \n                  incomecat = ifelse(income &gt; 80000, \n                                     \"高\", \n                                     \"低\"))\n                  \n# 将教育程度不是初中、普通高中、中专的转化为其他类别\nnewdata &lt;- mutate(cgss, \n                  edu = ifelse(edu %in% \n                                     c(\"初中\", \"普通高中\", \"中专\"),\n                                     edu,\n                                     \"其他\"))\n                  \n# 将身高大于200或小于75设定为缺失值\nnewdata &lt;- mutate(cgss, \n                  height = ifelse(height &lt; 75 | height &gt; 200,\n                                     NA,\n                                     height))\n\n\n\n\nsummarize函数可用于描述性统计的数据汇总。可与by_group函数结合使用，用于按组计算统计值。na.rm=TRUE选项用于在计算平均值之前删除缺失值。\n\nlibrary(dplyr)\n\n# 计算身高和体重的平均值\nnewdata &lt;- summarize(cgss, \n                     mean_ht = mean(height, na.rm=TRUE), \n                     mean_weight = mean(weight, na.rm=TRUE))\nnewdata\n\n# A tibble: 1 × 2\n  mean_ht mean_weight\n    &lt;dbl&gt;       &lt;dbl&gt;\n1    164.        122.\n\n# 分性别组计算身高和体重的平均值\nnewdata &lt;- group_by(cgss, sex)\nnewdata &lt;- summarize(newdata, \n                     mean_ht = mean(height, na.rm=TRUE), \n                     mean_wt = mean(weight, na.rm=TRUE))\nnewdata\n\n# A tibble: 2 × 3\n  sex   mean_ht mean_wt\n  &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 男       170.    133.\n2 女       159.    113.\n\n\n\n\n\ndplyr和tidyr软件包允许使用管道运算符以紧凑的格式编写代码%&gt;%，运算符%&gt;%将左边的结果传递给右边的函数的第一个参数。例如：\n\nlibrary(dplyr)\n\nnewdata &lt;- filter(cgss, \n                  sex == \"女\")\nnewdata &lt;- group_by(newdata, province)\nnewdata &lt;- summarize(newdata, \n                     mean_ht = mean(height, na.rm = TRUE))\n\n# 采用管道操作符更简洁，也更符合人类思维\nnewdata &lt;- cgss %&gt;%\n  filter(sex == \"女\") %&gt;%\n  group_by(province) %&gt;%\n  summarize(mean_ht = mean(height, na.rm = TRUE))\n\n\n\n\n在 R 中，日期值以字符的形式输入。例如，记录 3 个人出生日期的简单数据集。\n\ndf &lt;- data.frame(\n  dob = c(\"11/10/1963\", \"Jan-23-91\", \"12:1:2001\")\n)\n\nstr(df) \n\n'data.frame':   3 obs. of  1 variable:\n $ dob: chr  \"11/10/1963\" \"Jan-23-91\" \"12:1:2001\"\n\n\n将字符变量转换为日期变量的方法有很多。最简单的方法之一是使用lubridate包中提供的函数。这些函数包括ymd、dmy和 ，mdy分别用于导入年-月-日、日-月-年和月-日-年的格式。\n\nlibrary(lubridate)\n# 将dob变量值从字符转化为日期型数据\ndf$dob &lt;- mdy(df$dob)\nstr(df)\n\n'data.frame':   3 obs. of  1 variable:\n $ dob: Date, format: \"1963-11-10\" \"1991-01-23\" ...\n\n\n这些值在R的内部记录为自 1970 年 1 月 1日以来的天数，可以方便地执行日期运算，提取日期元素（月、日、年），重新格式化（例如，1963年10月11日）。 日期变量对于制作时间相关图表非常重要。\n\n\n\n有些图表要求数据为宽格式，而有些图表则要求数据为长格式，示例如下。\n将宽数据集转换为长数据集\n\nlibrary(tidyr)\nwide_data &lt;- data.frame(id = c(\"01\", \"02\", \"03\"),\n                        name = c(\"张三\", \"李四\", \"王五\"),\n                        sex = c(\"男\", \"男\", \"女\"),\n                        height = c(70, 72, 62),\n                        weight = c(180, 195, 130))\nknitr::kable(wide_data, caption = \"宽数据\")\n\n\n宽数据\n\n\nid\nname\nsex\nheight\nweight\n\n\n\n\n01\n张三\n男\n70\n180\n\n\n02\n李四\n男\n72\n195\n\n\n03\n王五\n女\n62\n130\n\n\n\n\nlong_data &lt;- pivot_longer(wide_data,\n                          cols = c(\"height\", \"weight\"),\n                          names_to = \"variable\", \n                          values_to =\"value\")\n\n将长数据转化为宽数据\n\nlibrary(tidyr)\nknitr::kable(long_data, caption = \"长数据\")\n\n\n长数据\n\n\nid\nname\nsex\nvariable\nvalue\n\n\n\n\n01\n张三\n男\nheight\n70\n\n\n01\n张三\n男\nweight\n180\n\n\n02\n李四\n男\nheight\n72\n\n\n02\n李四\n男\nweight\n195\n\n\n03\n王五\n女\nheight\n62\n\n\n03\n王五\n女\nweight\n130\n\n\n\n\nwide_data &lt;- pivot_wider(long_data,\n                         names_from = \"variable\",\n                         values_from = \"value\")\n\n\n\n\n真实数据很可能包含缺失值。处理缺失数据有三种基本方法：特征选择、列删除和插补。ggplot2包中的msleep数据集描述了哺乳动物的睡眠习惯，并且在多个变量上存在缺失值。\n\n特征选择 在特征选择中，可以删除包含太多缺失值的变量（列）。\n\n\ndata(msleep, package=\"ggplot2\")\n\n# 每个变量中缺失值的比例\npctmiss &lt;- colSums(is.na(msleep))/nrow(msleep)\nround(pctmiss, 2)\n\n        name        genus         vore        order conservation  sleep_total \n        0.00         0.00         0.08         0.00         0.35         0.00 \n   sleep_rem  sleep_cycle        awake      brainwt       bodywt \n        0.27         0.61         0.00         0.33         0.00 \n\n\n61% 的 sleep_cycle 值缺失。可以决定将其删除。\n\n按列删除\n整行删除包含缺失值的观测。\n\n\nnewdata &lt;- select(msleep, genus, vore, conservation)\nnewdata &lt;- na.omit(newdata)\n\n\n补值\n插补涉及用“合理”的猜测值（假设缺失值不存在时的值）来替换缺失值。有几种方法，详见VIM、mice、Amelia和missForest等包。这里将使用VIMkNN()包中的函数，用插补值替换缺失值。\n\n\n# 用5个最近邻的值插补缺失值\nlibrary(VIM)\nnewdata &lt;- kNN(msleep, k=5)\n\n基本上，对于每个有缺失值的案例，都会选择k个最相似的、没有缺失值的案例。如果缺失值为数值型，则使用这k 个案例的中位数作为插补值。如果缺失值为类别值，则使用这k 个案例中出现频率最高的值。该过程会迭代所有观测和变量，直到结果收敛（趋于稳定）。\n重要提示：缺失值可能会对研究结果造成偏差（有时甚至非常严重）。如果有大量缺失数据，在删除观测或填补缺失值之前，要慎重考虑其合理性。"
  },
  {
    "objectID": "bigdata/visualization/visualization.html#ggplot2-软件包简介",
    "href": "bigdata/visualization/visualization.html#ggplot2-软件包简介",
    "title": "数据描述与可视化",
    "section": "ggplot2 软件包简介",
    "text": "ggplot2 软件包简介\n本部分简要概述了ggplot2包的工作原理，该软件包具有强大的绘图功能。\n\n示例\nggplot2包中的函数可以分层分步构建图形，先从简单的图形开始，逐个添加其他元素，从而构建复杂的图形。\n在使用 ggplot2 制图时，只需使用ggplot和geom两个函数。其他函数是可选的，可以按需求添加。\n\n\nggplot\nggplot函数指定要使用的数据框、变量到图形视觉属性的映射。这些映射位于aes函数内部。\n\nlibrary(ggplot2)\n# 为了制图美观，剔除收入过高的人群\ncgss &lt;- cgss %&gt;% filter(income &lt;=200000)\n\n# 确定数据集和映射\nggplot(data = cgss,\n       mapping = aes(x = eduyear, y = income))\n\n\n\n\n\n\n\n\n图表是空的,因为只是指定映射，教育年限变量映射到x轴，收入映射到y轴，但还没有指定图上要显示的内容。\n\n\n几何对象\n几何对象 (Geoms) 是可以放置在图上的几何形状（点、线、条形等），它们是以geom_开头的函数。先看看用geom_point函数添加点，创建散点图。\n在 ggplot2 制图时，可以使用+号（容易忘记添加）将不同的函数链接在一起，一步一步构建复杂的图。\n\nggplot(data = cgss,\n       mapping = aes(x = eduyear, y = income)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n几何对象函数中可以指定参数（选项）控制其外观。下面的geom_point函数的选项包括color、size和alpha，分别控制点的颜色、大小和透明度。透明度的范围从 0（完全透明）到 1（完全不透明）。增加透明度有助于可视化重叠点密度。\n\n\nggplot(data = cgss,\n       mapping = aes(x = eduyear, y = income)) +\n  geom_point(color = \"cornflowerblue\",\n             alpha = .7,\n             size = 2)\n\n\n\n\n\n\n\n\n\n添加一条最佳拟合线，可以使用geom_smooth函数来实现。选项控制线的类型（线性、二次、非参数）、线的粗细、线的颜色以及是否包含置信区间。下面添加线性回归（method = lm）线（其中lm代表线性回归模型），默认带置信区间。\n\n\nggplot(data = cgss,\n       mapping = aes(x = eduyear, y = income)) +\n  geom_point(color = \"cornflowerblue\",\n             alpha = .5,\n             size = 2) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\n\n分组\n除了将变量映射到x轴和y轴（只能有两个变量维度）之外，还可以将变量映射到几何对象的颜色、形状、大小、透明度和其他视觉特征，体现多个变量维度，实现将观测更多的状态叠加到单个图形中。\n\n# 采用颜色体现性别差异\nggplot(data = cgss,\n       mapping = aes(x = eduyear, \n                     y = income,\n                     color = sex)) +\n  geom_point(alpha = .5,\n             size = 2) +\n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              linewidth = 1.5)\n\n\n\n\n\n\n\n\ncolor = sex选项位于aes函数中，将变量sex映射到图形的视觉特征color上。\n\n\n刻度\n\n先解决中文支持的问题\n\n\nlibrary(showtext)\nlibrary(sysfonts)\n\n# 加载黑体，最好找到字体的路径再加载 mac可用fc-list :lang=zh命令查看\nfont_add(\"Heiti SC\", \"/System/Library/Fonts/STHeiti Medium.ttc\")  \n\n# 启用 showtext 自动渲染\nshowtext_auto()\n\n刻度控制变量通过以scale_开头的刻度函数来修改x轴和y轴的缩放比例和单位，以及图例所使用的颜色等刻度视觉特征。\n\nggplot(data = cgss,\n       mapping = aes(x = eduyear, \n                     y = income,\n                     color = sex)) +\n  geom_point(alpha = .5,\n             size = 2) +\n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              linewidth = 1.5) +\n  scale_x_continuous(breaks = seq(0, 20, 5)) +\n  scale_y_continuous(breaks = seq(0, 200000, 50000),\n                     label = scales::label_currency(prefix = \"￥\",suffix = \"元\")) +\n  scale_color_manual(values = c(\"indianred3\", \n                                \"cornflowerblue\"))\n\n\n\n\n\n\n\n\n\n\n切片\n切片会为给定变量（或成对的变量）的每个级别生成一个图表，方便对比。切片以facet_开头的函数创建。下面采用region变量的取值进行城乡切片。\n\nggplot(data = cgss,\n       mapping = aes(x = eduyear, \n                     y = income,\n                     color = sex)) +\n  geom_point(alpha = .5,\n             size = 2) +\n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              linewidth = 1.5) +\n  scale_x_continuous(breaks = seq(0, 20, 5)) +\n  scale_y_continuous(breaks = seq(0, 200000, 50000),\n                     label = scales::label_currency(prefix = \"￥\")) +\n  scale_color_manual(values = c(\"indianred3\", \n                                \"cornflowerblue\")) +\n  facet_wrap(~region)\n\n\n\n\n\n\n\n\n\n\n标签\n信息丰富的标签可以帮助图表的理解。labs功能为轴和图例提供自定义标签，还可以添加自定义标题、副标题和说明等。\n\nggplot(data = cgss,\n       mapping = aes(x = eduyear, \n                     y = income,\n                     color = sex)) +\n  geom_point(alpha = .5,\n             size = 2) +\n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              linewidth = 1.5) +\nscale_x_continuous(breaks = seq(0, 20, 5)) +\n  scale_y_continuous(breaks = seq(0, 200000, 50000),\n                     label = scales::label_currency(prefix = \"￥\")) +\n  scale_color_manual(values = c(\"indianred3\", \n                                \"cornflowerblue\")) +\n  facet_wrap(~region) +\n  # 增加制图标题，坐标标签等\n  labs(title = \"教育回报的影响因素\",\n       subtitle = \"CGSS2017\",\n       caption = \"来源：CGSS网站\",\n       x = \" 教育年限（年）\",\n       y = \"个人收入（元）\",\n       color = \"城乡差别\") \n\n\n\n\n\n\n\n\n\n\n主题\n最后，可以使用以theme_开头的主题函数来微调图表的外观。主题函数控制图表的背景颜色、字体、网格线、图例位置以及其他与数据无关的功能。下面是简洁主题的运用。\n\nggplot(data = cgss,\n       mapping = aes(x = eduyear, \n                     y = income,\n                     color = sex)) +\n  geom_point(alpha = .5,\n             size = 2) +\n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              linewidth = 1.5) +\nscale_x_continuous(breaks = seq(0, 20, 5)) +\n  scale_y_continuous(breaks = seq(0, 200000, 50000),\n                     label = scales::label_currency(prefix = \"￥\")) +\n  scale_color_manual(values = c(\"indianred3\", \n                                \"cornflowerblue\")) +\n  facet_wrap(~region) +\n  # 增加制图标题，坐标标签等\n  labs(title = \"教育回报的影响因素\",\n       subtitle = \"CGSS2017\",\n       caption = \"来源：CGSS网站\",\n       x = \" 教育年限（年）\",\n       y = \"个人收入（元）\",\n       color = \"城乡差别\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n图表展现的结果：\n\n教育年限与个人收入之间存在正向线性关系。但是男性的教育回报更高。\n\n在较高教育水平上，城市男性的个人收入更高。\n\n性别与城乡之间可能存在交互作用。城市似乎缩小了性别之间的教育回报的差异。\n\n在较高教育水平上，两个性别的个人收入都存在一些非常高的异常值。\n\n这些仅仅是初步发现，样本量有限，并且未采用统计检验来评估差异是否由偶然变异造成。\n\n\n\n\n放置data和mapping选项\n使用 ggplot2 创建的绘图始终以ggplot函数开头。在上面的示例中，data和mapping选项位于此函数中。在这种情况下，它们适用于其geom_后的每个函数。但是，也可以将这些选项直接放在geom函数中。在此情况下，它们仅适用于该特定的几何对象。\n\n# 颜色映射放在ggplot函数中\nggplot(cgss,\n       aes(x = eduyear, \n           y = income,\n           color = sex)) +\n  geom_point(alpha = .5,\n             size = 2) +\n  geom_smooth(method = \"lm\",\n              se = FALSE, \n              linewidth = 1.5)\n\n\n\n\n\n\n\n\n颜色映射适用于点和趋势线。\n\n# 颜色映射放在geom_point函数中\nggplot(cgss,\n       aes(x = eduyear, \n           y = income)) +\n  geom_point(aes(color = sex),\n             alpha = .5,\n             size = 2) +\n  geom_smooth(method = \"lm\",\n              se = FALSE, \n              linewidth = 1.5)\n\n\n\n\n\n\n\n\n颜色映射只适用于点的分类，不适用趋势线，所以只有一条趋势线。\n\n\n图作为对象\nggplot2 图表可以保存为 R 对象（如同数据框），进一步操作，然后绘制或存取。\n\n# 创建散点图并保存为对象\nmyplot &lt;- ggplot(data = cgss,\n                  aes(x = eduyear, y = income)) +\n             geom_point()\n\n# 绘制\nmyplot\n\n\n\n\n\n\n\n# 更改点的大小和颜色并绘制\n\nmyplot &lt;- myplot + geom_point(size = 2, color = \"blue\")\nmyplot\n\n\n\n\n\n\n\n# 添加标题和趋势线后绘制，但不保存到对象\nmyplot + geom_smooth(method = \"lm\") +\n  labs(title = \"示例图\")\n\n\n\n\n\n\n\n# 采用黑白主题绘制，不保存\nmyplot + theme_bw()"
  },
  {
    "objectID": "bigdata/visualization/visualization.html#单变量图",
    "href": "bigdata/visualization/visualization.html#单变量图",
    "title": "数据描述与可视化",
    "section": "单变量图",
    "text": "单变量图\n\n定类变量\n单个定类变量分布通常用条形图来展示。\n\n条形图\n\nggplot(cgss, aes(x = health)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n\n可以添加选项来修改条形填充和边框的颜色、绘图标签和标题等。\n\n\nggplot(cgss, aes(x=health)) + \n  geom_bar(fill = \"cornflowerblue\", \n           color=\"black\") +\n  labs(x = \"健康状况\", \n       y = \"频次\", \n       title = \"调查人群健康状况\")\n\n\n\n\n\n\n\n\n\n百分比条形图，代码aes(x=health)实际上是aes(x = health, y = after_stat(count))，其中count是一个特殊变量，表示每个类别中的频次。可以通过给y明确指定如何计算百分比。\n\n\nggplot(cgss, \n       aes(x = health, y = after_stat(count/sum(count)))) + \n  geom_bar() +\n  labs(x = \"健康状况\", \n       y = \"频率\", \n       title = \"调查人群健康状况\") +\n# 采用刻度函数添加百分比符号\n    scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\n\n\n\n排序类别\n某些情况下希望按频次对条形图进行排序，可以使用reorder函数按频次对类别进行排序。选项stat=“identity”告诉绘图函数不要计数，因为它们是直接提供的。\n\n\n# 计算健康状态每个类别的频次\nplotdata &lt;- cgss %&gt;%\n count(health)\n#使用新数据集来创建图表\nggplot(plotdata, \n       aes(x = reorder(health, n), y = n)) + \n  geom_bar(stat=\"identity\") +\n  labs(x = \"健康状况\", \n       y = \"频次\", \n       title  = \"调查人群健康状况\")\n\n\n\n\n\n\n\n\n图表条形按升序排列。使用reorder(health, -n)可按降序排列。\n\n标签，带有数字标签的条形图\n\n\nggplot(plotdata, \n       aes(x = health, y = n)) + \n  geom_bar(stat=\"identity\") +\n# 添加数值标签\n  geom_text(aes(label = n), vjust=-0.5) +\n  labs(x = \"健康状况\", \n       y = \"频次\", \n       title  = \"调查人群健康状况\")\n\n\n\n\n\n\n\n\n此处geom_text添加标签，并用vjust控制垂直对齐。\n\nplotdata &lt;- cgss %&gt;%\n  count(health) %&gt;%\n  mutate(pct = n / sum(n),\n         pctlabel = paste0(round(pct*100), \"%\"))\n\nggplot(plotdata, \n       aes(x = reorder(health, -pct), y = pct)) + \n  geom_bar(stat=\"identity\", fill=\"indianred3\", color=\"black\") +\n  geom_text(aes(label = pctlabel), vjust=-0.25) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"健康状况\", \n       y = \"频率\", \n       title  = \"调查人群健康状况\")\n\n\n\n\n\n\n\n\n\n重叠标签的调整\n如果类别过多或标签很长，类别标签可能会重叠，可以采用旋转坐标解决。\n\n\nggplot(cgss, aes(x = province)) + \n  geom_bar() +\n  labs(x = \"省份\",\n       y = \"频次\",\n       title = \"调查人群分布\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n此外，可以ggpieggpie包中的函数创建饼图；结合ggplot2，使用treemapify包创建树状图，使用waffle包创建华夫饼图。\n\nlibrary(ggpie)\nggpie(cgss, group_key = \"health\", count_type = \"full\", label_info = \"ratio\")\n\n\n\n\n\n\n\nlibrary(treemapify)\n\nplotdata &lt;- cgss %&gt;%\n  count(edu)\n\nggplot(plotdata, \n       aes(fill = edu, area = n)) +\n  geom_treemap() + \n  labs(title = \"调查人群教育程度\")\n\n\n\n\n\n\n\nlibrary(waffle)\nggplot(plotdata, aes(fill = edu, values=n)) +\n  geom_waffle(na.rm=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n尺度变量\n单个尺度变量的分布通常用直方图、核密度图或点图来绘制。\n\n直方图\n\nggplot(cgss, aes(x = age)) +\n  geom_histogram() + \n  labs(title = \"调查人群年龄分布\",\n       x = \"年龄\")\n\n\n\n\n\n\n\n\n\n可以使用两个选项修改直方图颜色，fill为条形填充颜色，color为条形周围的边框颜色。\n\n\nggplot(cgss, aes(x = age)) +\n  geom_histogram(fill = \"cornflowerblue\", \n                 color = \"white\") + \n  labs(title = \"调查人群年龄分布\",\n       x = \"年龄\")\n\n\n\n\n\n\n\n\n\n分组和组距\n直方图最重要的选项是分组（bins），它控制将尺度变量划分成的多个分组（即图中的条形数）。默认值为 30，但尝试较小或较大的数字有助于更好地了解分布的形状。还可以指定组距binwidth间接控制分组个数。\n\n\nggplot(cgss, aes(x = age)) +\n  geom_histogram(fill = \"cornflowerblue\", \n                 color = \"white\",\n                 bins = 20) + \n  labs(title = \"调查人群年龄分布\",\n       x = \"年龄\")\n\n\n\n\n\n\n\n\n与条形图一样，y轴可以表示计数或总数的百分比。\n\nlibrary(scales)\nggplot(cgss, \n       aes(x = age, y= after_stat(count/sum(count)))) +\n  geom_histogram(fill = \"cornflowerblue\", \n                 color = \"white\", \n                 binwidth = 5) + \n  labs(title=\"调查人群年龄分布\", \n       y = \"比例\",\n       x = \"年龄\") +\n  scale_y_continuous(labels = percent)\n\n\n\n\n\n\n\n\n\n\n核密度图\n从技术上讲，核密度估计是一种非参数方法，用于估计连续随机变量的概率密度函数。绘制一个平滑的直方图，其中曲线下的面积等于1。\n\nggplot(cgss, aes(x = age)) +\n  geom_density() + \n  labs(title = \"调查人群年龄分布\")\n\n\n\n\n\n\n\n\n\n填充颜色\n\n\nggplot(cgss, aes(x = age)) +\n  geom_density(fill = \"indianred3\") + \n  labs(title = \"调查人群年龄分布\")\n\n\n\n\n\n\n\n\n\n平滑参数\n平滑度由带宽参数bw控制。值越大，平滑度越高，值越小，平滑度越低。\n\n\nggplot(cgss, aes(x = age)) +\n  geom_density(fill = \"deepskyblue\", \n               bw = 1) + \n  labs(title = \"调查人群年龄分布\",\n       subtitle = \"bandwidth = 1\")\n\n\n\n\n\n\n\n\n\n\n点状图\n直方图的一种替代方案是点图。同样，尺度变量被分成多个组，但每个观测值都用一个点来表示，而不是用汇总条形图。默认情况下，点的宽度与分组的宽度相对应，并且点是堆叠的，每个点代表一个观测值。当观测值数量较少（例如少于 150 个）时，这种方法效果比较好。\n\nggplot(cgss, aes(x = age)) +\n  geom_dotplot(binwidth = 1) + \n  labs(title = \"调查人群年龄分布\",\n       y = \"比例\",\n       x = \"年龄\")\n\n\n\n\n\n\n\n\nfill和选项color可分别用于指定每个点的填充色和边框色。\n\nggplot(cgss, aes(x = age)) +\n  geom_dotplot(binwidth = 1,\n               fill = \"gold\", \n               color=\"black\") + \n  labs(title = \"调查人群年龄分布\",\n       y = \"比例\",\n       x = \"年龄\")"
  },
  {
    "objectID": "bigdata/visualization/visualization.html#双变量图",
    "href": "bigdata/visualization/visualization.html#双变量图",
    "title": "数据描述与可视化",
    "section": "双变量图",
    "text": "双变量图\n\n两个类别变量\n绘制两个类别变量之间的关系时，通常使用堆积条形图、分组条形图或分段条形图。\n\n堆积条形图\n\nlibrary(ggplot2)\n\nggplot(cgss, aes(x =region , fill = health)) + \n  geom_bar(position = \"stack\")\n\n\n\n\n\n\n\n\n\n\n分类条形图\n分类条形图将第二个类别变量的条形并排放置。\n\nggplot(cgss, aes(x = region, fill = health)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n分段条形图\n分段条形图是一种堆叠条形图，其中每个条形代表 100%。\n\nggplot(cgss, aes(x = region, fill = health)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"比例\")\n\n\n\n\n\n\n\n\n\n\n改善颜色和标签\n\nfactor修改类别变量的地区变量顺序以及健康状态变量的顺序和标签\nscale_y_continuous修改 y 轴刻度标记标签\nlabs提供标题并更改 x 轴和 y 轴的标签以及图例\nscale_fill_brewer更改填充颜色方案\ntheme_minimal删除灰色背景并改变网格颜色\n\n\nggplot(cgss, aes(x = region, fill = health)) + \n  geom_bar(position = \"fill\") +\n  scale_y_continuous(breaks = seq(0, 1, .2), \n                     label = scales::percent) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(y = \"比例\", \n       fill=\"健康状态\",\n       x = \"地区\",\n       title = \"调查人群分地区健康状态\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n两个尺度变量\n两个尺度变量之间的关系通常使用散点图和折线图来显示。\n\n散点图\n\nggplot(cgss, \n       aes(x = age, y = income)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\ngeom_point函数的选项\n\ncolor- 点颜色\n\nsize- 点大小\n\nshape- 点的形状\n\nalpha- 点透明度。透明度范围从 0（透明）到 1（不透明）。\n\n\n函数scale_x_continuous和分别控制x轴和yscale_y_continuous轴上的缩放。\n\n\nggplot(cgss, \n       aes(x = age, y = income)) +\n  geom_point(color=\"cornflowerblue\", \n             size = 2, \n             alpha=.8) +\n  scale_y_continuous(label = scales::label_currency(prefix=\"￥\",suffix = \"元\"), \n                     limits = c(0, 200000)) +\n  scale_x_continuous(breaks = seq(0, 90, 10), \n                     limits=c(15, 90)) + \n  labs(x = \"年龄\",\n       y = \"\",\n       title = \"年龄与收入的关系\",\n       subtitle = \"基于CGSS2015抽样数据\")\n\n\n\n\n\n\n\n\n\n添加最佳拟合线，支持多种类型的拟合线，包括线性、多项式和非参数 (loess)。默认情况下，显示这些线的 95% 置信限度。\n\n\nggplot(cgss, aes(x = age, y = income)) +\n  geom_point(color= \"steelblue\") +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n很奇怪的，收入会随着年龄的增加而降低。然而，在30-40岁间似乎出现了一个峰值，说明35岁之前可能是上升的。直线无法反映这种非线性效应，曲线更适合。通常使用二次（一次弯曲）或三次（两次弯曲）线。很少需要使用高阶（&gt;3）多项式。\n\nggplot(cgss, aes(x = age, y = income)) +\n  geom_point(color= \"steelblue\") +\n  geom_smooth(method = \"lm\", \n              formula = y ~ poly(x, 2), \n              color = \"indianred3\")\n\n\n\n\n\n\n\n\n很可惜，二次曲线也没有证据反映这种变化，平滑的非参数拟合线也许可以更好地描述这种关系。默认的非参数拟合是Loess线，代表局部加权散点图平滑。\n\nggplot(cgss, aes(x = age, y = income)) +\n  geom_point(color= \"steelblue\") +\n  geom_smooth(color = \"tomato\")\n\n\n\n\n\n\n\n\n\n\n\n类别与尺度变量\n针对类别变量和尺度变量之间的关系，可以使用汇总统计数据的条形图、分组核密度图、并排箱线图、并排小提琴图、均值/标准差图、脊线图和克利夫兰图。\n\n条形图（汇总统计数据）\n可以使用条形图显示尺度变量在分类变量各个水平上的汇总统计数据（例如，均值或中位数）。\n\nlibrary(dplyr)\nplotdata &lt;- cgss %&gt;%\n  group_by(health) %&gt;%\n  summarize(mean_income = mean(income))\n\nggplot(plotdata, aes(x = health, y = mean_income)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\n\nfactor函数可以修改每个健康状态的标签\n\nscale_y_continuous函数可以改进 y 轴的标签\n\ngeom_text函数可以为每个条形图添加平均值\n\n\nlibrary(scales)\nggplot(plotdata, \n       aes(x = health, y = mean_income)) +\n  geom_bar(stat = \"identity\", \n           fill = \"cornflowerblue\") +\n  geom_text(aes(label = label_currency(accuracy = 1, prefix = \"￥\")(mean_income)), \n            vjust = -0.25) +\n  scale_y_continuous(breaks = seq(0, 40000, 5000), \n                     label = label_currency(prefix = \"￥\")) +\n  labs(title = \"各健康水平的平均收入\", \n       subtitle = \"\",\n       x = \"\",\n       y = \"\")\n\n\n\n\n\n\n\n\n条形图的局限性是不显示数据的分布，仅显示每个组的汇总统计数据。分组核密度图在一定程度上纠正了这一缺陷。\n\n\n分组核密度图\n\nggplot(cgss, aes(x = income, fill = health)) +\n  geom_density(alpha = 0.4) +\n  labs(title = \"各健康水平的收入分布\")\n\n\n\n\n\n\n\n\n\n\n箱线图\n\n箱线图显示分布的第 25 个百分位数、中位数和第75 个百分位数。须线（垂直线）可捕捉正态分布的约 99%，超出此范围的观测值则绘制为表示异常值的点。\n箱线图适用于比较数值变量的组（即分类变量的级别）。\n\n\nggplot(cgss, aes(x = health, y = income)) +\n  geom_boxplot() +\n  labs(title = \"各健康水平的收入分布\")\n\n\n\n\n\n\n\n\n\n缺口箱线图提供了一种近似方法来直观地显示组间是否存在差异。虽然这不是正式的检验方法，但如果两个箱线图的缺口不重叠，则有强有力的证据（置信度为 95%）表明两组的中位数存在差异。\n\n\nggplot(cgss, aes(x = health, y = income)) +\n  geom_boxplot(notch = TRUE, \n               fill = \"cornflowerblue\", \n               alpha = .7) +\n  labs(title = \"各健康水平的收入分布\")\n\n\n\n\n\n\n\n\n\n\n小提琴图\n小提琴图与核密度图类似，但是镜像并旋转了 90度。\n\nggplot(cgss, aes(x = health, y = income)) +\n  geom_violin() +\n  labs(title = \"各健康水平的收入分布\")\n\n\n\n\n\n\n\n\n\n组合箱线图和小提琴图\n\n\nggplot(cgss, aes(x = health, y = income)) +\n  geom_violin(fill = \"cornflowerblue\") +\n  geom_boxplot(width = .15, \n               fill = \"orange\",\n               outlier.color = \"orange\",\n               outlier.size = 2) + \n  labs(title = \"各健康水平的收入分布\")\n\n\n\n\n\n\n\n\n有时候需要设置调试geom_boxplot的width参数，确保箱线图与小提琴图契合。\n\n\n山脊线图\n脊线图（也称为 joyplot）显示多个组中尺度变量的分布。它们类似于带有垂直刻面的核密度图，但占用的空间较小。脊线图使用ggridges包创建。\n\nlibrary(ggplot2)\nlibrary(ggridges)\n\nggplot(cgss, \n       aes(x = income, y = health, fill = health)) +\n  geom_density_ridges() + \n  theme_ridges() +\n  labs(\"各健康水平的收入分布\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n平均值/SEM 图\n\n一种常用的比较数值变量组数据的方法是绘制带误差线的均值图。误差线可以表示标准差、均值的标准误差或置信区间。\n\n\nlibrary(dplyr)\nplotdata &lt;- cgss %&gt;%\n  group_by(health) %&gt;%\n  summarize(n = n(),\n         mean = mean(income),\n         sd = sd(income),\n         se = sd / sqrt(n),\n         ci = qt(0.975, df = n - 1) * sd / sqrt(n))\nggplot(plotdata, \n       aes(x = health, \n           y = mean, \n           group = 1)) +\n  geom_point(size = 3) +\n  geom_line() +\n  geom_errorbar(aes(ymin = mean - se, \n                    ymax = mean + se), \n                width = .1)\n\n\n\n\n\n\n\n\n\n比较不性别和健康水平的收入\n\n\nplotdata &lt;- cgss %&gt;%\n  group_by(health, sex) %&gt;%\n  summarize(n = n(),\n            mean = mean(income),\n            sd = sd(income),\n            se = sd/sqrt(n))\n\n# 按性别绘制均值和标准误\nggplot(plotdata, aes(x = health,\n                     y = mean, \n                     group=sex, \n                     color=sex)) +\n  geom_point(size = 3) +\n  geom_line(linewidth = 1) +\n  geom_errorbar(aes(ymin  =mean - se, \n                    ymax = mean+se), \n                width = .1)\n\n\n\n\n\n\n\n\n\n误差线有重叠，可以稍微避开水平位置来解决。\n\n\npd &lt;- position_dodge(0.2)\nggplot(plotdata, \n       aes(x = health, \n           y = mean, \n           group=sex, \n           color=sex)) +\n  geom_point(position = pd, \n             size = 3) +\n  geom_line(position = pd,\n            linewidth = 1) +\n  geom_errorbar(aes(ymin = mean - se, \n                    ymax = mean + se), \n                width = .1, \n                position= pd)\n\n\n\n\n\n\n\n\n\n美化图表达到出版质量\n\n\npd &lt;- position_dodge(0.2)\nggplot(plotdata, \n       aes(x = health, y = mean, group=sex, color=sex)) +\n  geom_point(position=pd, \n             size=3) +\n  geom_line(position=pd, \n            linewidth = 1) +\n  geom_errorbar(aes(ymin = mean - se, \n                    ymax = mean + se), \n                width = .1, \n                position=pd, \n                size=1) +\n  scale_y_continuous(label = scales::label_currency(prefix = \"￥\")) +\n  scale_color_brewer(palette=\"Set1\") +\n  theme_minimal() +\n  labs(title = \"按性别和健康状况划分的平均收入\",\n       subtitle = \"(均值 +/- 标准误)\",\n       x = \"\", \n       y = \"\",\n       color = \"性别\")\n\n\n\n\n\n\n\n\n\n\n带状图\n\n分组变量和数值变量之间的关系也可以用散点图来显示。\n\n\nggplot(cgss, aes(y = health, x = income)) +\n  geom_point() + \n  labs(title = \"各健康水平的收入分布\")\n\n\n\n\n\n\n\n\n\n点的重叠会使解释变得困难。可以适当将点错开一点距离，更容易看出关系。操作是在每个 y 坐标上添加一个小的随机数，代码是将geom_point替换为geom_jitter。\n\n\nggplot(cgss, aes(y = health, x = income)) +\n  geom_jitter() + \n  labs(title = \"各健康水平的收入分布\")\n\n\n\n\n\n\n\n\n\n使用颜色增加组间的对比\n\n\nlibrary(scales)\nggplot(cgss, \n       aes(y = health, x = income, color = health)) +\n  geom_jitter(alpha = 0.7) + \n  scale_x_continuous(label = label_currency(prefix = \"￥\")) +\n  labs(title = \"各健康水平的收入分布\", \n       subtitle = \"CGSS2015数据\",\n       x = \"\",\n       y = \"\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n可以将箱线图叠加在带状图上\n\n\nlibrary(scales)\nggplot(cgss, \n       aes(x = health, \n           y = income, color = health)) +\n  geom_boxplot(size=1,\n               outlier.shape = 1,\n               outlier.color = \"black\",\n               outlier.size  = 3) +\n  geom_jitter(alpha = 0.5, \n              width=.2) + \n  scale_y_continuous(label = label_currency(prefix = \"￥\")) +\n  labs(title = \"各健康水平的收入分布\", \n       subtitle = \"CGSS2015数据\",\n       x = \"\",\n       y = \"\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nggpol包提供geom_boxjitter函数，可以创建一个混合箱线图（箱线图组合散点图）\n\n\nlibrary(ggpol)\nlibrary(scales)\nggplot(cgss, \n       aes(x = health, \n           y = income, \n           fill=health)) +\n  geom_boxjitter(color=\"black\",\n                 jitter.color = \"darkgrey\",\n                 errorbar.draw = TRUE) +\n  scale_y_continuous(label = label_currency(prefix = \"￥\")) +\n  labs(title = \"各健康水平的收入分布\", \n       subtitle = \"CGSS2015数据\",\n       x = \"\",\n       y = \"\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n克利夫兰点图\n\n如果比较数值变量的各个观测值，特别是比较大量数值汇总统计数据时，可以使用克利夫兰图\n\n\nlibrary(dplyr)\nplotdata &lt;- cgss %&gt;%\n  group_by(province) %&gt;%\n  summarize(mean_income = mean(income, na.rm=T))\n\nggplot(plotdata, \n       aes(x= mean_income, y = province)) +\n  geom_point()\n\n\n\n\n\n\n\n# 对数据进行排序会更直观\nggplot(plotdata, aes(x=mean_income, \n                     y=reorder(province, mean_income))) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n美化得到棒棒糖图\n\n\n# Fancy Cleveland plot\nggplot(plotdata, aes(x=mean_income, \n                     y=reorder(province, mean_income))) +\n  geom_point(color=\"blue\", size = 2) +\n  geom_segment(aes(x = 40, \n               xend = mean_income, \n               y = reorder(province, mean_income), \n               yend = reorder(province, mean_income)),\n               color = \"lightgrey\") +\n  labs (x = \"平均收入（元）\",\n        y = \"\",\n        title = \"分地区的平均收入\",\n        subtitle = \"CGSS2015数据\") +\n  theme_minimal() + \n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())"
  },
  {
    "objectID": "enp/index.html#生态系统",
    "href": "enp/index.html#生态系统",
    "title": "环境政策分析",
    "section": "生态系统",
    "text": "生态系统"
  },
  {
    "objectID": "enp/index.html#文化与社会观念",
    "href": "enp/index.html#文化与社会观念",
    "title": "环境政策分析",
    "section": "文化与社会观念",
    "text": "文化与社会观念"
  },
  {
    "objectID": "enp/index.html#监管环境",
    "href": "enp/index.html#监管环境",
    "title": "环境政策分析",
    "section": "监管环境",
    "text": "监管环境"
  },
  {
    "objectID": "enp/index.html#政治与制度背景",
    "href": "enp/index.html#政治与制度背景",
    "title": "环境政策分析",
    "section": "政治与制度背景",
    "text": "政治与制度背景"
  },
  {
    "objectID": "enp/index.html#空气污染",
    "href": "enp/index.html#空气污染",
    "title": "环境政策分析",
    "section": "空气污染",
    "text": "空气污染"
  },
  {
    "objectID": "enp/index.html#水资源与环境政策",
    "href": "enp/index.html#水资源与环境政策",
    "title": "环境政策分析",
    "section": "水资源与环境政策",
    "text": "水资源与环境政策"
  },
  {
    "objectID": "enp/index.html#土地管理政策",
    "href": "enp/index.html#土地管理政策",
    "title": "环境政策分析",
    "section": "土地管理政策",
    "text": "土地管理政策"
  },
  {
    "objectID": "enp/index.html#固体废弃物管理政策",
    "href": "enp/index.html#固体废弃物管理政策",
    "title": "环境政策分析",
    "section": "固体废弃物管理政策",
    "text": "固体废弃物管理政策"
  },
  {
    "objectID": "enp/index.html#能源政策",
    "href": "enp/index.html#能源政策",
    "title": "环境政策分析",
    "section": "能源政策",
    "text": "能源政策"
  },
  {
    "objectID": "enp/index.html#国际环境问题",
    "href": "enp/index.html#国际环境问题",
    "title": "环境政策分析",
    "section": "国际环境问题",
    "text": "国际环境问题"
  },
  {
    "objectID": "enp/index.html#案例",
    "href": "enp/index.html#案例",
    "title": "环境政策分析",
    "section": "案例",
    "text": "案例\n\n大气环境治理案例\n\n水环境治理案例"
  },
  {
    "objectID": "analyr/lecture8/datamining.html",
    "href": "analyr/lecture8/datamining.html",
    "title": "数据挖掘",
    "section": "",
    "text": "大量数据被收集和存储\n\n网站数据、电子商务\n\n超市和商店的购物信息\n\n银行账户和信用卡的交易\n\n\n计算机变得越来越便宜，越来越高效\n\n竞争压力越来越大\n\n为客户提供更优质的定制化服务（客户关系管理）\n\n\n\n\n\n\n收集存储的数据急剧增长(每秒1.7兆字节)\n\n遥感卫星\n\n天文望远镜\n\n基因数据\n\n科学模拟数据\n\n\n传统技术无法处理\n\n数据挖掘能够帮助科学家\n\n进行数据分类和分段\n建构假设\n\n2024年诺贝尔物理学奖获得者约翰·霍普菲尔德和杰弗里·欣顿是两名机器学习领域的元老级人物。 他们使用物理学工具，设计了人工神经网络，为当今强大的机器学习技术奠定了基础。\n2024年诺贝尔化学奖授予戴维·贝克、德米斯·哈萨比斯和约翰·江珀。他们采用人工智能在“计算蛋白质设计”和“蛋白质结构预测”方面成就斐然。这也是继物理学奖之后，诺贝尔奖再次被授予人工智能（AI）的相关成果及科学家。\n\n\n\n\n\n\n数据中经常有一些“隐藏”的信息，很难一目了然的发现\n\n手工分析可能需要好几个星期的时间去发现有用的信息\n\n很多数据根本没有用于分析\n\n\n\n\n\n\n从数据中提取隐含的、未知的和潜在有用的信息\n\n通过自动或半自动的探索和分析方法在大型数据中发现有意义的模式\u000b\n\n什么不是数据挖掘？\n\n服务数据中查找用户的电话号码\n\n用百度搜索“京东快递”的信息\n\n什么是数据挖掘？\n\n某些姓名在某些地区可能更普遍\n\n依据具体情形对搜索引擎返回的信息进行分组（对京东快递服务的正面和负面评价）\n\n\n\n\n\n来自机器学习、人工智能、模式识别、统计学和数据库方面的思想\n\n传统的技术无能为力\n\n数据量太大\n\n高维度数据\n\n异种数据和复杂数据\n\n\n\n\n\n\n分类 [预测]\n\n聚类 [描述]\n\n关联规则发现 [描述]\n\n序列模式发现 [描述]\n\n回归 [预测]\n\n背离检测 [预测]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n给定一组记录 (训练集)\n\n每条记录包含一组属性，其中一个属性称之为类别.\n\n\n构建一个模型使类别属性成为其他属性值的函数.\n\n目标：尽可能将未见过的记录精确归类.\n\n测试集用于决定模型的精度. 一般将数据分成训练集和测试集，采用训练集构建模型，用测试集去验证。\n\n\n分类的例子：退税、婚姻、收入？逃税\n\n\n\n\n\n\n\n\n\n\n\n\n\n目的： 锁定可能购买新产品的消费者减少邮寄成本\n\n方法:\n\n采用已有相似产品的数据.\n\n已知哪些客户决定买，哪些客户决定不买. 购买的决定就是类别属性.\n\n收集这些客户的人口学特征、生活方式等信息.\n\n工作性质、生活区域、收入情况等.\n\n采用这些信息作用输入属性来训练分类器模型.\n\n\n\n\n\n\n目的：预测信用卡交易中的欺诈.\n\n方法：\n\n采用信用卡持有人的交易信息作用属性.\n\n持卡人什么时候买、买什么、购买频率等\n\n过去的交易是否是欺诈或正常交易，这便是分类属性.\n\n训练交易分类模型.\n\n用模型对某个账户的信用卡交易进行分析检测是否存在欺诈.\n\n\n\n\n\n\n\n目的：预测天体的类别（恒星或星系），特别是针对那些在望远镜上看起来不清楚.\n\n3000 images with 23,040 x 23,040 pixels per image.\n\n\n方法:\n\n图像分段.\n\n测量图像的属性，每个天体有四十个属性.\n\n基于属性构建分类模型.\n\n成功找到16个新的高红移类星体, 难以发现的最远天体!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n有一组数据点，每个点有一组采用相似测量方式的属性，实现如下分类（分簇、分堆）\n\n簇内数据点比其他簇的数据点更相似.\n\n簇间数据点的差别要尽可能大.\n\n\n相似性的测量:\n\n欧氏距离用于连续型属性.\n\n其他基于具体问题的度量方式.\n\n\n聚类示意\n\n\n\n\n\n\n\n\n\n\n\n\n\n目的：对市场客户进行划分，利于采用不同的营销策略针对不同的客户群体.\n\n方法:\n\n基于客户的地理位置和生活习惯收集不同的信息属性.\n\n发现相似客户的组群.\n\n通过对比簇内客户和簇间客户的购买模式检验聚类的质量.\n\n\n\n\n\n\n目的: 根据文档中的主题词的出现情况对文档进行分组.\n\n方法：识别每个文档中频繁出现的主题词.利用不同主题词出现的频率测量文档的相似性，并对其聚类.\n\n应用：信息检索能够利用聚类的信息将新文档或检索词与聚类后文档联系起来.\n\n文本聚类示例\n- 聚类点：洛杉矶时报的3204篇文章.\n- 相似性测量：文档中有多少相同的词.\n\n\n\n\n\n\n\n\n\n股市数据聚类\n\n观察每天的股票动态.\n\n聚类点：股票-{涨/跌}\n\n相似性测量：如果同一天相同的涨跌频繁发生那么两点越相似.\n\n用关联性法则量化相关性.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n给定一组记录，每条记录包含给定集合中的某些项目；\n\n发现能够根据其他项目出现来预测某个项目出现的依赖规则.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n假设发现了如下规则 {可乐, … } –&gt; {薯片}\n\n薯片是后发生 =&gt; 能够用于决定如何提升其销量.\n\n可乐是先发生 =&gt; 能够用于了解如果可乐出现断货，会影响哪一类产品的销量.\n\n可乐先发生和薯片后发生 =&gt; 能够用于了解什么产品应该与可乐一起销售以提高薯片的销量！\n\n\n\n\n\n目标：通过顾客的购买行为了解会被同时购买的商品.\n\n目的：处理扫码后的销售数据发现商品之间的依赖关系.\n\n一条规则 –\n\n如果一位顾客买了尿布和牛奶，他很有可能会买啤酒.\n\n所以超市在尿布货架旁布置啤酒！\n\n\n\n\n\n\n目的：电器维修公司想预测维修消费者电器需要的原配件，那么可以事先带上，以减少往返和上门的次数.\n\n方法：对不同区域消费者之前维修所用原配件和工具的数据进行处理，发现共同出现的模式.\n\n\n\n\n\n\n在假定线性或非线性依赖关系的前提下，根据其他变量预测给定连续型变量的值.\n\n在统计学和神经网络领域应用最多.\n\n例子：\n\n基于广告费用预测新产品的销售量.\n\n利用温度、湿度和气压等构建函数预测风速.\n\n对股市指标进行时间序列预测.\n\n\n\n\n\n\n从正常行为中检测显著的异常\n\n应用：\n\n信用卡欺诈检测\n\n网络入侵检测：一般大学每日网络访问量多达1亿个连接\n\n\n\n\n\n\n尺度扩展性\n\n高维度\n\n异种数据和复杂数据\n\n数据质量\n\n数据所有权和分布\n\n隐私保护\n\n实时动态数据流"
  },
  {
    "objectID": "analyr/lecture8/datamining.html#数据挖掘与机器学习",
    "href": "analyr/lecture8/datamining.html#数据挖掘与机器学习",
    "title": "数据挖掘",
    "section": "",
    "text": "大量数据被收集和存储\n\n网站数据、电子商务\n\n超市和商店的购物信息\n\n银行账户和信用卡的交易\n\n\n计算机变得越来越便宜，越来越高效\n\n竞争压力越来越大\n\n为客户提供更优质的定制化服务（客户关系管理）\n\n\n\n\n\n\n收集存储的数据急剧增长(每秒1.7兆字节)\n\n遥感卫星\n\n天文望远镜\n\n基因数据\n\n科学模拟数据\n\n\n传统技术无法处理\n\n数据挖掘能够帮助科学家\n\n进行数据分类和分段\n建构假设\n\n2024年诺贝尔物理学奖获得者约翰·霍普菲尔德和杰弗里·欣顿是两名机器学习领域的元老级人物。 他们使用物理学工具，设计了人工神经网络，为当今强大的机器学习技术奠定了基础。\n2024年诺贝尔化学奖授予戴维·贝克、德米斯·哈萨比斯和约翰·江珀。他们采用人工智能在“计算蛋白质设计”和“蛋白质结构预测”方面成就斐然。这也是继物理学奖之后，诺贝尔奖再次被授予人工智能（AI）的相关成果及科学家。\n\n\n\n\n\n\n数据中经常有一些“隐藏”的信息，很难一目了然的发现\n\n手工分析可能需要好几个星期的时间去发现有用的信息\n\n很多数据根本没有用于分析\n\n\n\n\n\n\n从数据中提取隐含的、未知的和潜在有用的信息\n\n通过自动或半自动的探索和分析方法在大型数据中发现有意义的模式\u000b\n\n什么不是数据挖掘？\n\n服务数据中查找用户的电话号码\n\n用百度搜索“京东快递”的信息\n\n什么是数据挖掘？\n\n某些姓名在某些地区可能更普遍\n\n依据具体情形对搜索引擎返回的信息进行分组（对京东快递服务的正面和负面评价）\n\n\n\n\n\n来自机器学习、人工智能、模式识别、统计学和数据库方面的思想\n\n传统的技术无能为力\n\n数据量太大\n\n高维度数据\n\n异种数据和复杂数据\n\n\n\n\n\n\n分类 [预测]\n\n聚类 [描述]\n\n关联规则发现 [描述]\n\n序列模式发现 [描述]\n\n回归 [预测]\n\n背离检测 [预测]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n给定一组记录 (训练集)\n\n每条记录包含一组属性，其中一个属性称之为类别.\n\n\n构建一个模型使类别属性成为其他属性值的函数.\n\n目标：尽可能将未见过的记录精确归类.\n\n测试集用于决定模型的精度. 一般将数据分成训练集和测试集，采用训练集构建模型，用测试集去验证。\n\n\n分类的例子：退税、婚姻、收入？逃税\n\n\n\n\n\n\n\n\n\n\n\n\n\n目的： 锁定可能购买新产品的消费者减少邮寄成本\n\n方法:\n\n采用已有相似产品的数据.\n\n已知哪些客户决定买，哪些客户决定不买. 购买的决定就是类别属性.\n\n收集这些客户的人口学特征、生活方式等信息.\n\n工作性质、生活区域、收入情况等.\n\n采用这些信息作用输入属性来训练分类器模型.\n\n\n\n\n\n\n目的：预测信用卡交易中的欺诈.\n\n方法：\n\n采用信用卡持有人的交易信息作用属性.\n\n持卡人什么时候买、买什么、购买频率等\n\n过去的交易是否是欺诈或正常交易，这便是分类属性.\n\n训练交易分类模型.\n\n用模型对某个账户的信用卡交易进行分析检测是否存在欺诈.\n\n\n\n\n\n\n\n目的：预测天体的类别（恒星或星系），特别是针对那些在望远镜上看起来不清楚.\n\n3000 images with 23,040 x 23,040 pixels per image.\n\n\n方法:\n\n图像分段.\n\n测量图像的属性，每个天体有四十个属性.\n\n基于属性构建分类模型.\n\n成功找到16个新的高红移类星体, 难以发现的最远天体!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n有一组数据点，每个点有一组采用相似测量方式的属性，实现如下分类（分簇、分堆）\n\n簇内数据点比其他簇的数据点更相似.\n\n簇间数据点的差别要尽可能大.\n\n\n相似性的测量:\n\n欧氏距离用于连续型属性.\n\n其他基于具体问题的度量方式.\n\n\n聚类示意\n\n\n\n\n\n\n\n\n\n\n\n\n\n目的：对市场客户进行划分，利于采用不同的营销策略针对不同的客户群体.\n\n方法:\n\n基于客户的地理位置和生活习惯收集不同的信息属性.\n\n发现相似客户的组群.\n\n通过对比簇内客户和簇间客户的购买模式检验聚类的质量.\n\n\n\n\n\n\n目的: 根据文档中的主题词的出现情况对文档进行分组.\n\n方法：识别每个文档中频繁出现的主题词.利用不同主题词出现的频率测量文档的相似性，并对其聚类.\n\n应用：信息检索能够利用聚类的信息将新文档或检索词与聚类后文档联系起来.\n\n文本聚类示例\n- 聚类点：洛杉矶时报的3204篇文章.\n- 相似性测量：文档中有多少相同的词.\n\n\n\n\n\n\n\n\n\n股市数据聚类\n\n观察每天的股票动态.\n\n聚类点：股票-{涨/跌}\n\n相似性测量：如果同一天相同的涨跌频繁发生那么两点越相似.\n\n用关联性法则量化相关性.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n给定一组记录，每条记录包含给定集合中的某些项目；\n\n发现能够根据其他项目出现来预测某个项目出现的依赖规则.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n假设发现了如下规则 {可乐, … } –&gt; {薯片}\n\n薯片是后发生 =&gt; 能够用于决定如何提升其销量.\n\n可乐是先发生 =&gt; 能够用于了解如果可乐出现断货，会影响哪一类产品的销量.\n\n可乐先发生和薯片后发生 =&gt; 能够用于了解什么产品应该与可乐一起销售以提高薯片的销量！\n\n\n\n\n\n目标：通过顾客的购买行为了解会被同时购买的商品.\n\n目的：处理扫码后的销售数据发现商品之间的依赖关系.\n\n一条规则 –\n\n如果一位顾客买了尿布和牛奶，他很有可能会买啤酒.\n\n所以超市在尿布货架旁布置啤酒！\n\n\n\n\n\n\n目的：电器维修公司想预测维修消费者电器需要的原配件，那么可以事先带上，以减少往返和上门的次数.\n\n方法：对不同区域消费者之前维修所用原配件和工具的数据进行处理，发现共同出现的模式.\n\n\n\n\n\n\n在假定线性或非线性依赖关系的前提下，根据其他变量预测给定连续型变量的值.\n\n在统计学和神经网络领域应用最多.\n\n例子：\n\n基于广告费用预测新产品的销售量.\n\n利用温度、湿度和气压等构建函数预测风速.\n\n对股市指标进行时间序列预测.\n\n\n\n\n\n\n从正常行为中检测显著的异常\n\n应用：\n\n信用卡欺诈检测\n\n网络入侵检测：一般大学每日网络访问量多达1亿个连接\n\n\n\n\n\n\n尺度扩展性\n\n高维度\n\n异种数据和复杂数据\n\n数据质量\n\n数据所有权和分布\n\n隐私保护\n\n实时动态数据流"
  },
  {
    "objectID": "analyr/lecture1/introduction.html",
    "href": "analyr/lecture1/introduction.html",
    "title": "定量研究方法课程介绍",
    "section": "",
    "text": "课程性质：必修课程\n\n课程目的：介绍定量研究方法、统计分析方法\n\n主要内容：\n\n定量分析的研究设计\n\n主要定量方法介绍：调查法、实验法、二手数据分析\n\n统计分析的基本原理与应用\n\n统计分析工具的运用：R语言"
  },
  {
    "objectID": "analyr/lecture1/introduction.html#课程介绍",
    "href": "analyr/lecture1/introduction.html#课程介绍",
    "title": "定量研究方法课程介绍",
    "section": "",
    "text": "课程性质：必修课程\n\n课程目的：介绍定量研究方法、统计分析方法\n\n主要内容：\n\n定量分析的研究设计\n\n主要定量方法介绍：调查法、实验法、二手数据分析\n\n统计分析的基本原理与应用\n\n统计分析工具的运用：R语言"
  },
  {
    "objectID": "analyr/lecture1/introduction.html#学习目标",
    "href": "analyr/lecture1/introduction.html#学习目标",
    "title": "定量研究方法课程介绍",
    "section": "学习目标",
    "text": "学习目标\n\n能够提出具体的研究问题，进行研究设计。\n\n能够理解和应用定量研究方法。\n\n能够理解定量研究方法的优点和缺点。\n\n能够理解各种统计方法，并能够选择合适的统计方法分析定量数据。\n\n能熟练使用统计软件对数据进行处理和分析。\n\n能够对统计分析的结果进行解释，了解其社会与政策含义。\n\n能够独立实施研究项目，展示研究成果。\n\n能够初步进行定量分析论文的写作。"
  },
  {
    "objectID": "analyr/lecture1/introduction.html#先修课程和知识准备",
    "href": "analyr/lecture1/introduction.html#先修课程和知识准备",
    "title": "定量研究方法课程介绍",
    "section": "先修课程和知识准备",
    "text": "先修课程和知识准备\n本课程要求学生已经具有一定统计学和定量研究方法基础，在本科阶段至少学习过一门统计学相关课程，例如《社会统计学》《概率论与数理统计》《统计学》《计量经济学》等，至少学习过一门研究方法相关课程，例如《定量研究方法》《社会研究方法》《社会调查》等。"
  },
  {
    "objectID": "analyr/lecture1/introduction.html#课程参考书籍",
    "href": "analyr/lecture1/introduction.html#课程参考书籍",
    "title": "定量研究方法课程介绍",
    "section": "课程参考书籍",
    "text": "课程参考书籍\n\n先修课程和知识参考书籍：\n\nEarl Babbie 社会研究方法（第11版），华夏出版社，2009\n尹海洁，李树林 社会统计学（第2版），中国人民大学出版社，2018\n\n研究方法参考书籍（研究方法）：\n\nRemler, D. K., & Van Ryzin, G. G. (2015). Research Methods in Practice: Strategies for Description and Causation (2nd ed.). Thousand Oaks, CA: Sage.\n\n统计参考书籍（统计分析）：\n\nGelman and Hill, Data Analysis Using Regression and Multilevel/Hierarchical Models, Cambridge University Press, 2006\n\nR语言参考书籍（R语言）：\n\nKabacoff （王小宁等译）R语言实战（第3版），人民邮电出版社，2023\n\n其他参考书籍\n\nGary King, Robert O. Keohane, Sidney Verda, 社会科学中的研究设计，格致出版社，2014. （KKV）\n\nMiller, G., & Yang, K. (2008). Handbook of Research Methods in Public Administration. New York: Taylor & Francis.\n\n陈晓萍, 徐淑英, & 樊景立 (Eds.). (2012). 组织与管理研究的实证方法 (第二版.). 北京: 北京大学出版社."
  },
  {
    "objectID": "analyr/lecture1/introduction.html#课程要求与成绩",
    "href": "analyr/lecture1/introduction.html#课程要求与成绩",
    "title": "定量研究方法课程介绍",
    "section": "课程要求与成绩",
    "text": "课程要求与成绩\n\n课堂参与（10%）：每次旷课扣5分，课前请假不扣分\n\n平时作业（50%）：每次作业10分，5-6次共50分，课后下周一中午12点前提交\n\n课程论文（30%）：复制一篇论文的结果，进行延伸讨论，完成一篇规范的定量分析文章\n\n口头报告（10%）：对课程论文内容进行口头报告\n\n\n时间要求：\n\n第6周之前选好文章并准备好个人课程论文数据\n\n第15-16周最后两次课进行口头报告\n\n结课两周后按时提交课程论文（数据、程序、论文）\n\n\n其他要求：引文规范、避免抄袭。\n\n课外学习内容：预习复习课程内容，阅读学习材料，学习相关软件的操作，完成练习，准备数据，进行数据分析，写作课程论文。"
  },
  {
    "objectID": "analyr/lecture1/introduction.html#课程计划",
    "href": "analyr/lecture1/introduction.html#课程计划",
    "title": "定量研究方法课程介绍",
    "section": "课程计划",
    "text": "课程计划\n\n课程介绍\n\n选题与文献综述\n\n研究方法与研究设计\n\n测量与操作化\n\n抽样\n\n调查研究\n\n实验法\n\n统计软件介绍R\n\n多元回归模型\n\nLogistic回归\n\n广义线性模型\n\n调节效应、中介效应与结构方程\n\n多层次模型（optional）\n\n大数据收集与机器学习应用介绍（optional）\n\nNote：课程计划会根据课程进度，适度调整。"
  },
  {
    "objectID": "analyr/lecture1/introduction.html#课后阅读与作业",
    "href": "analyr/lecture1/introduction.html#课后阅读与作业",
    "title": "定量研究方法课程介绍",
    "section": "课后阅读与作业",
    "text": "课后阅读与作业\n\n课后阅读：\n\nRemler & Van Ryzin, Chapter 1: Research in the Real World.\n\nRemler & Van Ryzin, Chapter 2: Theory, Models, and Research Questions.\n\n\n选读：\n\nKKV 《社会科学中的研究设计》第1、2章内容"
  },
  {
    "objectID": "analyr/lecture1/introduction.html#课后作业",
    "href": "analyr/lecture1/introduction.html#课后作业",
    "title": "定量研究方法课程介绍",
    "section": "课后作业：",
    "text": "课后作业：\n\nRemler & Van Ryzin, Chapter 1: 1.1, 1.3, 1.4.\n\nRemler & Van Ryzin, Chapter 2: 2.5, 2.6 (要求解释性选题). 阅读中遇到的问题"
  },
  {
    "objectID": "dataanlyr/lecture7/sem.html",
    "href": "dataanlyr/lecture7/sem.html",
    "title": "调节效应、中介效应和结构方程",
    "section": "",
    "text": "调节效应（Moderation Effect）是统计学和社会科学中的一个概念，指的是某个第三变量（称为调节变量或调节因子）能够改变两个其他变量之间的关系的强度或方向所产生的作用。通过调节效应的分析，可以更好地理解复杂的变量关系，为研究者提供更加精细化的理论指导和建议。\n当一个调节变量存在时，它会通过影响自变量与因变量之间的关系来发挥作用。例如：\n\n如果 ( X ) 是自变量，( Y ) 是因变量，( Z ) 是调节变量，那么调节效应的核心是： [ Y = _0 + _1 X + _2 Z + _3 (X Z) + ] 其中，( _3 ) 是交互项系数，表示 ( Z ) 调节 ( X ) 和 ( Y ) 关系的强弱或方向。\n\n\n\n\n\n政府透明度与公众信任\n\n\n自变量（X）：政府透明度（Transparency）。\n因变量（Y）：公众对政府的信任（Public Trust）。\n调节变量（Z）：公民政治参与水平（Political Participation）。\n\n解释：高水平的政治参与可能会增强透明度对公众信任的正面影响，因为公民更关注并感受到透明度带来的好处。\n效果：\n\n高参与：透明度增加显著提高公众信任。\n低参与：透明度的影响不明显。\n\n\n\n\n财政分权与地方经济增长\n\n\n自变量（X）：财政分权程度（Fiscal Decentralization）。\n因变量（Y）：地方经济增长（Local Economic Growth）。\n调节变量（Z）：地方政府问责制（Government Accountability）。\n\n解释：高问责制的地方，财政分权带来的灵活性能更有效地促进经济增长；而低问责制可能导致腐败和低效。\n效果：\n\n高问责制：财政分权显著促进经济增长。\n低问责制：财政分权可能削弱经济增长。\n\n\n\n\n环境政策严格性与污染治理成效\n\n\n自变量（X）：环境政策严格性（Environmental Policy Stringency）。\n因变量（Y）：污染治理效果（Pollution Mitigation Effectiveness）。\n调节变量（Z）：企业技术创新能力（Technological Innovation in Firms）。\n\n解释：具有高技术创新能力的企业能够更好地适应和满足严格的环境政策要求。\n效果：\n\n高创新能力：政策严格性对治理成效有更大影响。\n低创新能力：政策严格性可能导致高成本而无法显著改善污染。\n\n\n\n\n\n\n调节效应通常用交互图（Interaction Plot）展示。\n\n将( X ) 和 ( Y ) 的关系在不同 ( Z ) 水平下的表现。\n\n不同 ( Z ) 水平会呈现不同的斜率或曲线。\n\n\n\n\n一项关于气候变化与灾害Chapman and Lickel 2015的研究中，研究者向211名实验参与者讲述非洲发生干旱造成人道主义危机，告诉其中一半的参与者气候变化是造成干旱的原因，另一半参与者未被告知任何关于干旱的原因。接着通过一系列问题让实验参与者评价拒绝援助的正当性，以及了解他们对气候变化的怀疑程度。最后了解参与者捐款的意愿。 实验的目的是了解框架（frame），即是否告知干旱是由于气候变化产生的，对捐助意愿(donate)的影响数据。\n假定对气候变化怀疑（skeptic）程度较高的人框架对捐助意愿的效应也比较小，会存在调节效应。\n\ndisaster &lt;- read.csv(\"disaster.csv\", header = T)\n# 调节效应\nmoderafit &lt;- lm(donate ~ frame + skeptic + frame:skeptic, data = disaster)\nsummary(moderafit)\n\n\nCall:\nlm(formula = donate ~ frame + skeptic + frame:skeptic, data = disaster)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8341 -0.7077  0.1659  0.9101  2.6682 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    5.02947    0.22632  22.223   &lt;2e-16 ***\nframe          0.67930    0.33091   2.053   0.0413 *  \nskeptic       -0.13953    0.05790  -2.410   0.0168 *  \nframe:skeptic -0.17071    0.08393  -2.034   0.0432 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.234 on 207 degrees of freedom\nMultiple R-squared:  0.1343,    Adjusted R-squared:  0.1218 \nF-statistic: 10.71 on 3 and 207 DF,  p-value: 1.424e-06\n\n\n\n\n\n\n中介效应（Mediation Effect）是统计学和社会科学中的一个概念，指一个中介变量（Mediating Variable）在自变量（Independent Variable, (X)）和因变量（Dependent Variable, (Y)）之间起中介作用，即部分或全部传递自变量对因变量的影响。中介效应主要用于探讨变量之间的作用机制，揭示因果路径。中介效应的分析能够揭示隐藏的机制和作用路径，为理论研究提供深入的解释，并为实践应用提供精细化的决策依据。\n\n\n如果 (X) 通过一个中介变量 (M) 影响 (Y)，这表明 (M) 是 (X) 和 (Y) 之间的中介变量。公式化表达为： [ X M Y ] 其中：\n\n(X)：自变量，独立变量。\n(Y)：因变量，依赖变量。\n(M)：中介变量。\n\n中介效应说明 (X) 不仅直接影响 (Y)（直接效应），还通过 (M) 间接影响 (Y)（间接效应）。\n\n\n\n经典的中介效应模型可以分为三步：\n\n总效应模型：(Y = cX + _1)，其中 (c) 是 (X) 对 (Y) 的总效应。\n中介变量模型：(M = aX + _2)，其中 (a) 是 (X) 对 (M) 的效应。\n结果变量模型：(Y = c’X + bM + _3)，\n\n(c’)：控制 (M) 后，(X) 对 (Y) 的直接效应。\n(b)：(M) 对 (Y) 的效应。\n\n\n间接效应由 (a b) 表示，总效应可以分解为： [ c = c’ + (a b) ]\n\n\n\n\n总效应：自变量对因变量的总影响，包括直接效应和间接效应。\n直接效应：自变量对因变量的直接影响，不通过中介变量传递。\n间接效应：自变量通过中介变量对因变量的影响。\n\n\n\n\n\n完全中介效应（Full Mediation）\n\n(X) 对 (Y) 的影响完全通过 (M) 传递。\n控制 (M) 后，(X) 对 (Y) 的直接效应 (c’) 不显著。\n\n部分中介效应（Partial Mediation）\n\n(X) 对 (Y) 的影响部分通过 (M) 传递。\n控制 (M) 后，(X) 对 (Y) 的直接效应 (c’) 仍然显著。\n\n\n\n\n\n\n教育政策研究\n\n\n自变量：教育投资（Education Investment）。\n中介变量：师资水平（Teacher Quality）。\n因变量：学生成绩（Student Performance）。\n\n解释：教育投资可以通过提升师资水平来间接影响学生成绩。\n\n\n\n公共卫生领域\n\n\n自变量：健康教育活动（Health Education）。\n中介变量：健康知识水平（Health Knowledge）。\n因变量：健康行为（Health Behavior）。\n\n解释：健康教育活动通过提高健康知识水平，进而改变人们的健康行为。\n\n\n\n公共服务数字化与市民满意度\n\n\n自变量（X）：公共服务数字化水平（Digitalization of Public Services）。\n中介变量（M）：服务便利性（Convenience of Service）。\n因变量（Y）：市民满意度（Citizen Satisfaction）。\n\n解释：数字化的服务提高了服务的便利性，间接提升市民对公共服务的满意度。\n\n\n\n\n\n\n逐步回归分析：按照三步模型依次验证每个路径的显著性。\nBootstrap方法：通过重复抽样计算间接效应及其置信区间，常用于提高检验效力。\nSobel检验：检验间接效应是否显著，计算公式为： [ z = ] 其中 () 表示估计系数的标准误。\n\n\n\n\n\n中介效应：关注变量之间的因果路径，解释“为什么”自变量会影响因变量。\n调节效应：关注变量之间的关系强弱，解释“在什么情况下”自变量会影响因变量。\n\n\n\n\n上面气候变化与灾害的研究中，框架可能是通过影响正当性，然后正当性再影响捐助意愿的，即中介效应。\n\nlibrary(lavaan)\n\nThis is lavaan 0.6-19\nlavaan is FREE software! Please report any bugs.\n\n# 中介效应\nmediamodel &lt;- ' # 直接效应\ndonate ~ c*frame\n# 中介效应\njustify ~ a*frame\ndonate ~ b*justify\n# 间接效应 (a*b)\nab := a*b\n# 总效应\ntotal := c + (a*b)\n'\nmediafit &lt;- sem(mediamodel, data = disaster)\nsummary(mediafit)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n  Number of observations                           211\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  donate ~                                            \n    frame      (c)    0.212    0.135    1.576    0.115\n  justify ~                                           \n    frame      (a)    0.134    0.127    1.054    0.292\n  donate ~                                            \n    justify    (b)   -0.953    0.072  -13.159    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .donate            0.948    0.092   10.271    0.000\n   .justify           0.856    0.083   10.271    0.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    ab               -0.128    0.122   -1.051    0.293\n    total             0.084    0.181    0.463    0.643\n\n\n绘制图形\n\nlibrary(\"semPlot\")\nsemPaths(mediafit,whatLabels = 'est',residuals = F, nCharNodes=0, sizeMan = 12,edge.label.cex = 1.5)\n\n\n\n\n\n\n\n\n\n\n\n\n条件过程模型（Conditional Process Model）是一个综合框架，用于同时分析中介效应（Mediation Effect）和调节效应（Moderation Effect）。它探讨自变量通过中介变量影响因变量的机制，同时考虑调节变量如何影响这一机制的强度或方向。条件过程模型是一种强大的分析工具，能够揭示复杂的变量关系，为研究者提供深入的理论解释和实践指导。\n\n\n条件过程模型结合了中介效应和调节效应，回答以下关键问题：\n\n机制问题：自变量如何通过中介变量间接影响因变量？（中介效应）\n情境问题：这种中介效应在什么情况下更强或更弱？（调节效应）\n\n公式化表示为： [ Y = b_1 M + b_2 X + b_3 W + b_4 (M W) + ] 其中：\n\n(X)：自变量。\n(M)：中介变量。\n(Y)：因变量。\n(W)：调节变量。\n(b_4)：调节变量 (W) 对中介效应的调节作用。\n\n\n\n\n\n中介效应的调节：调节变量 (W) 改变了自变量 (X) 通过中介变量 (M) 对因变量 (Y) 的间接影响。\n模型交互：条件过程模型包含交互项（例如 (M W)），用来描述调节效应如何影响中介效应。\n多路径分析：条件过程模型可以同时研究直接效应、间接效应和调节效应。\n\n\n\n\n\n调节的中介效应（Moderated Mediation Effect）\n\n\n定义：调节变量影响中介效应中某一路径的强度或方向。\n解释：调节变量 ((W)) 改变了自变量 ((X)) 通过中介变量 ((M)) 对因变量 ((Y)) 的间接效应。\n(X)：自变量\n(M)：中介变量\n(W)：调节变量，对 (X M) 或 (M Y) 的路径进行调节\n(Y)：因变量\n\n示例：环保政策严格性 ((X)) 通过企业创新 ((M)) 改善环境质量 ((Y))，但监管强度 ((W)) 调节这一过程。\n\n中介的调节效应（Mediated Moderation Effect）\n\n\n定义：中介变量的某一路径影响受调节变量的影响。\n解释：中介变量 ((M)) 解释了调节变量 ((W)) 如何影响自变量 ((X)) 对因变量 ((Y)) 的关系。\n(X W)：自变量和调节变量的交互作用\n(M)：中介变量，解释交互作用如何影响 (Y)\n(Y)：因变量\n\n示例：政策执行力度 ((X)) 和地方经济发展水平 ((W)) 的交互作用通过基层治理能力 ((M)) 间接影响社会稳定 ((Y))。\n\n结合复杂模型（Combined Model）\n\n解释：同时存在调节的中介效应和中介的调节效应，展示了变量之间更复杂的关系。 - (X → M → Y)：中介效应 - (W)：调节变量同时作用于 (X → M) 和 (M → Y) 的路径。\n示例：公共政策透明度 ((X)) 通过公众信任 ((M)) 增强公众满意度 ((Y))，但这种过程因社会参与度 ((W)) 的不同而改变。\n\n\n\n\n基于回归分析的交互项建模\n\n\n\n在回归模型中添加中介变量、调节变量以及交互项（如 (M W)）。\n\n分析每个路径系数的显著性以验证条件过程关系。\n\n\nBootstrap法\n\n\n\n通过Bootstrap重复抽样方法计算间接效应的置信区间。\n\n验证不同调节水平下的中介效应是否显著。\n\n\nPROCESS工具\n\n\n\nPROCESS工具是Andrew F. Hayes开发的一个专用宏，用于SPSS和R语言，可以直接分析复杂的条件过程模型。\n\n\n结构方程的路径分析\n\n\n\n结合各类的结构方程分析软件进行路径分析。\n\n\n\n\n\n综合性强：能够同时分析机制问题（中介效应）和情境问题（调节效应）。\n解释力强：揭示复杂的因果路径和不同情境下的效果差异。\n广泛适用：适用于社会科学、心理学、公共管理等多个领域。\n\n\n\n\n\n综合调节与中介效应，框架对捐助意愿的直接和间接效应是受到怀疑程度的调节的，即被调节的中介效应。\n\n# 具有调节中介效应的条件过程模型\ncpmodel &lt;- ' # 直接效应\ndonate ~ c1*frame + c2*skeptic + c3*skeptic:frame \n# 中介效应\njustify ~ a1*frame + a2*skeptic + a3*skeptic:frame\ndonate ~ b*justify\n\n# 间接效应取决于skeptic的值，需要用平均值（或其他代表值）带入计算间接效应\n# 间接效应 (a*b) skeptic取平均值3.38\na1b := (a1+a3*3.38)*b\n# 总效应\ntotal := c1 + (a1+a3*3.38)*b\n'\ncpfit &lt;- sem(cpmodel, data = disaster)\nsummary(cpfit)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                           211\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  donate ~                                            \n    frame     (c1)    0.160    0.264    0.606    0.544\n    skeptic   (c2)   -0.043    0.046   -0.918    0.359\n    skptc:frm (c3)    0.015    0.068    0.219    0.827\n  justify ~                                           \n    frame     (a1)   -0.562    0.216   -2.606    0.009\n    skeptic   (a2)    0.105    0.038    2.782    0.005\n    skptc:frm (a3)    0.201    0.055    3.675    0.000\n  donate ~                                            \n    justify    (b)   -0.923    0.083  -11.113    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .donate            0.943    0.092   10.271    0.000\n   .justify           0.648    0.063   10.271    0.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    a1b              -0.108    0.103   -1.054    0.292\n    total             0.052    0.285    0.182    0.856\n\n\n图形\n\nsemPaths(cpfit,whatLabels = 'est', layout = \"spring\",,residuals = F, nCharNodes=0, sizeMan = 8,edge.label.cex = 1)"
  },
  {
    "objectID": "dataanlyr/lecture7/sem.html#调节效应和中介效应",
    "href": "dataanlyr/lecture7/sem.html#调节效应和中介效应",
    "title": "调节效应、中介效应和结构方程",
    "section": "",
    "text": "调节效应（Moderation Effect）是统计学和社会科学中的一个概念，指的是某个第三变量（称为调节变量或调节因子）能够改变两个其他变量之间的关系的强度或方向所产生的作用。通过调节效应的分析，可以更好地理解复杂的变量关系，为研究者提供更加精细化的理论指导和建议。\n当一个调节变量存在时，它会通过影响自变量与因变量之间的关系来发挥作用。例如：\n\n如果 ( X ) 是自变量，( Y ) 是因变量，( Z ) 是调节变量，那么调节效应的核心是： [ Y = _0 + _1 X + _2 Z + _3 (X Z) + ] 其中，( _3 ) 是交互项系数，表示 ( Z ) 调节 ( X ) 和 ( Y ) 关系的强弱或方向。\n\n\n\n\n\n政府透明度与公众信任\n\n\n自变量（X）：政府透明度（Transparency）。\n因变量（Y）：公众对政府的信任（Public Trust）。\n调节变量（Z）：公民政治参与水平（Political Participation）。\n\n解释：高水平的政治参与可能会增强透明度对公众信任的正面影响，因为公民更关注并感受到透明度带来的好处。\n效果：\n\n高参与：透明度增加显著提高公众信任。\n低参与：透明度的影响不明显。\n\n\n\n\n财政分权与地方经济增长\n\n\n自变量（X）：财政分权程度（Fiscal Decentralization）。\n因变量（Y）：地方经济增长（Local Economic Growth）。\n调节变量（Z）：地方政府问责制（Government Accountability）。\n\n解释：高问责制的地方，财政分权带来的灵活性能更有效地促进经济增长；而低问责制可能导致腐败和低效。\n效果：\n\n高问责制：财政分权显著促进经济增长。\n低问责制：财政分权可能削弱经济增长。\n\n\n\n\n环境政策严格性与污染治理成效\n\n\n自变量（X）：环境政策严格性（Environmental Policy Stringency）。\n因变量（Y）：污染治理效果（Pollution Mitigation Effectiveness）。\n调节变量（Z）：企业技术创新能力（Technological Innovation in Firms）。\n\n解释：具有高技术创新能力的企业能够更好地适应和满足严格的环境政策要求。\n效果：\n\n高创新能力：政策严格性对治理成效有更大影响。\n低创新能力：政策严格性可能导致高成本而无法显著改善污染。\n\n\n\n\n\n\n调节效应通常用交互图（Interaction Plot）展示。\n\n将( X ) 和 ( Y ) 的关系在不同 ( Z ) 水平下的表现。\n\n不同 ( Z ) 水平会呈现不同的斜率或曲线。\n\n\n\n\n一项关于气候变化与灾害Chapman and Lickel 2015的研究中，研究者向211名实验参与者讲述非洲发生干旱造成人道主义危机，告诉其中一半的参与者气候变化是造成干旱的原因，另一半参与者未被告知任何关于干旱的原因。接着通过一系列问题让实验参与者评价拒绝援助的正当性，以及了解他们对气候变化的怀疑程度。最后了解参与者捐款的意愿。 实验的目的是了解框架（frame），即是否告知干旱是由于气候变化产生的，对捐助意愿(donate)的影响数据。\n假定对气候变化怀疑（skeptic）程度较高的人框架对捐助意愿的效应也比较小，会存在调节效应。\n\ndisaster &lt;- read.csv(\"disaster.csv\", header = T)\n# 调节效应\nmoderafit &lt;- lm(donate ~ frame + skeptic + frame:skeptic, data = disaster)\nsummary(moderafit)\n\n\nCall:\nlm(formula = donate ~ frame + skeptic + frame:skeptic, data = disaster)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8341 -0.7077  0.1659  0.9101  2.6682 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    5.02947    0.22632  22.223   &lt;2e-16 ***\nframe          0.67930    0.33091   2.053   0.0413 *  \nskeptic       -0.13953    0.05790  -2.410   0.0168 *  \nframe:skeptic -0.17071    0.08393  -2.034   0.0432 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.234 on 207 degrees of freedom\nMultiple R-squared:  0.1343,    Adjusted R-squared:  0.1218 \nF-statistic: 10.71 on 3 and 207 DF,  p-value: 1.424e-06\n\n\n\n\n\n\n中介效应（Mediation Effect）是统计学和社会科学中的一个概念，指一个中介变量（Mediating Variable）在自变量（Independent Variable, (X)）和因变量（Dependent Variable, (Y)）之间起中介作用，即部分或全部传递自变量对因变量的影响。中介效应主要用于探讨变量之间的作用机制，揭示因果路径。中介效应的分析能够揭示隐藏的机制和作用路径，为理论研究提供深入的解释，并为实践应用提供精细化的决策依据。\n\n\n如果 (X) 通过一个中介变量 (M) 影响 (Y)，这表明 (M) 是 (X) 和 (Y) 之间的中介变量。公式化表达为： [ X M Y ] 其中：\n\n(X)：自变量，独立变量。\n(Y)：因变量，依赖变量。\n(M)：中介变量。\n\n中介效应说明 (X) 不仅直接影响 (Y)（直接效应），还通过 (M) 间接影响 (Y)（间接效应）。\n\n\n\n经典的中介效应模型可以分为三步：\n\n总效应模型：(Y = cX + _1)，其中 (c) 是 (X) 对 (Y) 的总效应。\n中介变量模型：(M = aX + _2)，其中 (a) 是 (X) 对 (M) 的效应。\n结果变量模型：(Y = c’X + bM + _3)，\n\n(c’)：控制 (M) 后，(X) 对 (Y) 的直接效应。\n(b)：(M) 对 (Y) 的效应。\n\n\n间接效应由 (a b) 表示，总效应可以分解为： [ c = c’ + (a b) ]\n\n\n\n\n总效应：自变量对因变量的总影响，包括直接效应和间接效应。\n直接效应：自变量对因变量的直接影响，不通过中介变量传递。\n间接效应：自变量通过中介变量对因变量的影响。\n\n\n\n\n\n完全中介效应（Full Mediation）\n\n(X) 对 (Y) 的影响完全通过 (M) 传递。\n控制 (M) 后，(X) 对 (Y) 的直接效应 (c’) 不显著。\n\n部分中介效应（Partial Mediation）\n\n(X) 对 (Y) 的影响部分通过 (M) 传递。\n控制 (M) 后，(X) 对 (Y) 的直接效应 (c’) 仍然显著。\n\n\n\n\n\n\n教育政策研究\n\n\n自变量：教育投资（Education Investment）。\n中介变量：师资水平（Teacher Quality）。\n因变量：学生成绩（Student Performance）。\n\n解释：教育投资可以通过提升师资水平来间接影响学生成绩。\n\n\n\n公共卫生领域\n\n\n自变量：健康教育活动（Health Education）。\n中介变量：健康知识水平（Health Knowledge）。\n因变量：健康行为（Health Behavior）。\n\n解释：健康教育活动通过提高健康知识水平，进而改变人们的健康行为。\n\n\n\n公共服务数字化与市民满意度\n\n\n自变量（X）：公共服务数字化水平（Digitalization of Public Services）。\n中介变量（M）：服务便利性（Convenience of Service）。\n因变量（Y）：市民满意度（Citizen Satisfaction）。\n\n解释：数字化的服务提高了服务的便利性，间接提升市民对公共服务的满意度。\n\n\n\n\n\n\n逐步回归分析：按照三步模型依次验证每个路径的显著性。\nBootstrap方法：通过重复抽样计算间接效应及其置信区间，常用于提高检验效力。\nSobel检验：检验间接效应是否显著，计算公式为： [ z = ] 其中 () 表示估计系数的标准误。\n\n\n\n\n\n中介效应：关注变量之间的因果路径，解释“为什么”自变量会影响因变量。\n调节效应：关注变量之间的关系强弱，解释“在什么情况下”自变量会影响因变量。\n\n\n\n\n上面气候变化与灾害的研究中，框架可能是通过影响正当性，然后正当性再影响捐助意愿的，即中介效应。\n\nlibrary(lavaan)\n\nThis is lavaan 0.6-19\nlavaan is FREE software! Please report any bugs.\n\n# 中介效应\nmediamodel &lt;- ' # 直接效应\ndonate ~ c*frame\n# 中介效应\njustify ~ a*frame\ndonate ~ b*justify\n# 间接效应 (a*b)\nab := a*b\n# 总效应\ntotal := c + (a*b)\n'\nmediafit &lt;- sem(mediamodel, data = disaster)\nsummary(mediafit)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n  Number of observations                           211\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  donate ~                                            \n    frame      (c)    0.212    0.135    1.576    0.115\n  justify ~                                           \n    frame      (a)    0.134    0.127    1.054    0.292\n  donate ~                                            \n    justify    (b)   -0.953    0.072  -13.159    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .donate            0.948    0.092   10.271    0.000\n   .justify           0.856    0.083   10.271    0.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    ab               -0.128    0.122   -1.051    0.293\n    total             0.084    0.181    0.463    0.643\n\n\n绘制图形\n\nlibrary(\"semPlot\")\nsemPaths(mediafit,whatLabels = 'est',residuals = F, nCharNodes=0, sizeMan = 12,edge.label.cex = 1.5)\n\n\n\n\n\n\n\n\n\n\n\n\n条件过程模型（Conditional Process Model）是一个综合框架，用于同时分析中介效应（Mediation Effect）和调节效应（Moderation Effect）。它探讨自变量通过中介变量影响因变量的机制，同时考虑调节变量如何影响这一机制的强度或方向。条件过程模型是一种强大的分析工具，能够揭示复杂的变量关系，为研究者提供深入的理论解释和实践指导。\n\n\n条件过程模型结合了中介效应和调节效应，回答以下关键问题：\n\n机制问题：自变量如何通过中介变量间接影响因变量？（中介效应）\n情境问题：这种中介效应在什么情况下更强或更弱？（调节效应）\n\n公式化表示为： [ Y = b_1 M + b_2 X + b_3 W + b_4 (M W) + ] 其中：\n\n(X)：自变量。\n(M)：中介变量。\n(Y)：因变量。\n(W)：调节变量。\n(b_4)：调节变量 (W) 对中介效应的调节作用。\n\n\n\n\n\n中介效应的调节：调节变量 (W) 改变了自变量 (X) 通过中介变量 (M) 对因变量 (Y) 的间接影响。\n模型交互：条件过程模型包含交互项（例如 (M W)），用来描述调节效应如何影响中介效应。\n多路径分析：条件过程模型可以同时研究直接效应、间接效应和调节效应。\n\n\n\n\n\n调节的中介效应（Moderated Mediation Effect）\n\n\n定义：调节变量影响中介效应中某一路径的强度或方向。\n解释：调节变量 ((W)) 改变了自变量 ((X)) 通过中介变量 ((M)) 对因变量 ((Y)) 的间接效应。\n(X)：自变量\n(M)：中介变量\n(W)：调节变量，对 (X M) 或 (M Y) 的路径进行调节\n(Y)：因变量\n\n示例：环保政策严格性 ((X)) 通过企业创新 ((M)) 改善环境质量 ((Y))，但监管强度 ((W)) 调节这一过程。\n\n中介的调节效应（Mediated Moderation Effect）\n\n\n定义：中介变量的某一路径影响受调节变量的影响。\n解释：中介变量 ((M)) 解释了调节变量 ((W)) 如何影响自变量 ((X)) 对因变量 ((Y)) 的关系。\n(X W)：自变量和调节变量的交互作用\n(M)：中介变量，解释交互作用如何影响 (Y)\n(Y)：因变量\n\n示例：政策执行力度 ((X)) 和地方经济发展水平 ((W)) 的交互作用通过基层治理能力 ((M)) 间接影响社会稳定 ((Y))。\n\n结合复杂模型（Combined Model）\n\n解释：同时存在调节的中介效应和中介的调节效应，展示了变量之间更复杂的关系。 - (X → M → Y)：中介效应 - (W)：调节变量同时作用于 (X → M) 和 (M → Y) 的路径。\n示例：公共政策透明度 ((X)) 通过公众信任 ((M)) 增强公众满意度 ((Y))，但这种过程因社会参与度 ((W)) 的不同而改变。\n\n\n\n\n基于回归分析的交互项建模\n\n\n\n在回归模型中添加中介变量、调节变量以及交互项（如 (M W)）。\n\n分析每个路径系数的显著性以验证条件过程关系。\n\n\nBootstrap法\n\n\n\n通过Bootstrap重复抽样方法计算间接效应的置信区间。\n\n验证不同调节水平下的中介效应是否显著。\n\n\nPROCESS工具\n\n\n\nPROCESS工具是Andrew F. Hayes开发的一个专用宏，用于SPSS和R语言，可以直接分析复杂的条件过程模型。\n\n\n结构方程的路径分析\n\n\n\n结合各类的结构方程分析软件进行路径分析。\n\n\n\n\n\n综合性强：能够同时分析机制问题（中介效应）和情境问题（调节效应）。\n解释力强：揭示复杂的因果路径和不同情境下的效果差异。\n广泛适用：适用于社会科学、心理学、公共管理等多个领域。\n\n\n\n\n\n综合调节与中介效应，框架对捐助意愿的直接和间接效应是受到怀疑程度的调节的，即被调节的中介效应。\n\n# 具有调节中介效应的条件过程模型\ncpmodel &lt;- ' # 直接效应\ndonate ~ c1*frame + c2*skeptic + c3*skeptic:frame \n# 中介效应\njustify ~ a1*frame + a2*skeptic + a3*skeptic:frame\ndonate ~ b*justify\n\n# 间接效应取决于skeptic的值，需要用平均值（或其他代表值）带入计算间接效应\n# 间接效应 (a*b) skeptic取平均值3.38\na1b := (a1+a3*3.38)*b\n# 总效应\ntotal := c1 + (a1+a3*3.38)*b\n'\ncpfit &lt;- sem(cpmodel, data = disaster)\nsummary(cpfit)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                           211\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  donate ~                                            \n    frame     (c1)    0.160    0.264    0.606    0.544\n    skeptic   (c2)   -0.043    0.046   -0.918    0.359\n    skptc:frm (c3)    0.015    0.068    0.219    0.827\n  justify ~                                           \n    frame     (a1)   -0.562    0.216   -2.606    0.009\n    skeptic   (a2)    0.105    0.038    2.782    0.005\n    skptc:frm (a3)    0.201    0.055    3.675    0.000\n  donate ~                                            \n    justify    (b)   -0.923    0.083  -11.113    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .donate            0.943    0.092   10.271    0.000\n   .justify           0.648    0.063   10.271    0.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    a1b              -0.108    0.103   -1.054    0.292\n    total             0.052    0.285    0.182    0.856\n\n\n图形\n\nsemPaths(cpfit,whatLabels = 'est', layout = \"spring\",,residuals = F, nCharNodes=0, sizeMan = 8,edge.label.cex = 1)"
  },
  {
    "objectID": "dataanlyr/lecture7/sem.html#结构方程的特点",
    "href": "dataanlyr/lecture7/sem.html#结构方程的特点",
    "title": "调节效应、中介效应和结构方程",
    "section": "结构方程的特点",
    "text": "结构方程的特点\n\n结构方程分析需要建立在理论基础上，从变量的测量、变量关系的假定和模型的设定都需要有清晰的理论支持或逻辑推理作为依据。\n结构方程模型同时处理潜变量的测量和变量间关系的分析，变量测量中的误差也被包含在变量关系的分析过程中。\n结构方程是基于变量间的协方差进行分析，协方差能够反映变量间的关联，也能反映理论模型生成的协方差与实际观测所得的协方差之间的差异。\n结构方程需要大样本，样本量大于200。\n结构方程的评估基于多重指标对整体模型进行比较，不依赖于单一的统计显著性。"
  },
  {
    "objectID": "dataanlyr/lecture7/sem.html#结构方程模型分析的过程",
    "href": "dataanlyr/lecture7/sem.html#结构方程模型分析的过程",
    "title": "调节效应、中介效应和结构方程",
    "section": "结构方程模型分析的过程",
    "text": "结构方程模型分析的过程"
  },
  {
    "objectID": "dataanlyr/lecture7/sem.html#结构方程的组成",
    "href": "dataanlyr/lecture7/sem.html#结构方程的组成",
    "title": "调节效应、中介效应和结构方程",
    "section": "结构方程的组成",
    "text": "结构方程的组成\n\n结构方程的变量：\n\n结构方程主要变量为尺度变量，类别变量只作为分组讨论的调节变量。\n潜变量(F)必须有两个以上（一般为3个以上，2个的情况需要模型存在多个潜变量，且之间存在关联性）的测量变量(V1, V2)，测量变量间的协方差反映潜在变量的共同影响，测量变量无法被潜变量解释的部分为测量误差(E1, E2)。\n内生潜变量所影响（对应）的为内生测量变量，外生潜变量所影响（对应）的为外生测量变量，内生潜变量的残差称为干扰项。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n结构方程的参数：\n\n测量模型参数包括潜变量与测量变量的关联强度\\(\\lambda\\)，也成为因子载荷，外生测量变量的测量误差\\(\\delta\\)，内生测量变量的测量误差\\(\\varepsilon\\)，外生变量的协方差\\(\\phi\\)。\n结构模型参数包括外生潜变量与内生潜变量之间的关系\\(\\gamma\\)，内生潜变量之间的关系\\(\\beta\\)，内生潜变量的残差或干扰项\\(\\zeta\\)。\n\n\n\n结构方程模型设定\n\n简效原则：将变量间的关系以最符合理论又最简单扼要的方式加以设定。如果一个简单模型能够解释较多实际数据的变化，那么以这个模型来说明数据的关系，比较不会得到错误的结论，结构方程可以防止弃真错误，难以防止纳伪错误。\n结构方程会遇到等值模型问题，即不同设定的模型拟合优度相等，可以通过前导理论策略（通过理论排除对等的模型）或参数竞争比较策略（比较对等模型参数估计）来解决。"
  },
  {
    "objectID": "dataanlyr/lecture7/sem.html#结构方程模型识别",
    "href": "dataanlyr/lecture7/sem.html#结构方程模型识别",
    "title": "调节效应、中介效应和结构方程",
    "section": "结构方程模型识别",
    "text": "结构方程模型识别\n\n结构方程的不同设定会产生模型识别问题，只有在过度识别条件下，才能对模型参数进行计算估计。通过比较待估参数数量\\(t\\)和测量数据量\\(DP\\)之间的大小，可以判断（必要不充分）模型能否参数估计。\n\n当\\(t &lt; DP\\)，为过度识别，如同有三个方程，求两个未知数的解。\n当\\(t = DP\\)，为充分识别，如同有两个个方程，求两个未知数的解。\n当\\(t &gt; DP\\)，为识别不足，如同只有一个方程，求两个未知数的解。\n其中，待估参数数量\\(t\\)根据具体的模型设定决定，测量数据量\\(DP\\)由外生测量变量的个数\\(p\\)和内生测量变量的个数\\(q\\)计算。 \\[DP=\\frac{(p+q)(p+q+1)}{2}\\]\n\n当结构方程模型设定没有结构关系的假设，模型可以顺利识别（Null Beta Rule）；当结构方程模型设定只估计结构参数，干扰项只估计方差不估计相关，模型自动识别（递归法则）。尽量保持简单的模型结构。"
  },
  {
    "objectID": "dataanlyr/lecture7/sem.html#结构方程模型拟合评估",
    "href": "dataanlyr/lecture7/sem.html#结构方程模型拟合评估",
    "title": "调节效应、中介效应和结构方程",
    "section": "结构方程模型拟合评估",
    "text": "结构方程模型拟合评估\n\n结构方程模型分析策略与一般统计推断有明显差异，是以支持原假设作为模型拟合度存在的证据。\n结构方程的原假设是偏好的模型（根据理论对结构系数做出了某些假定，某些系数不为零）与实际观察的数据是相符的，其检验则是通过与饱和模型（对各观察变量之间关系都做了相关假定，所有系数均不为零）相比较，如果模型之间卡方检验值比较大，则说明两个模型存在显著差异，拒绝原假设（拒绝偏好的模型），接受饱和模型。\n模型的卡方统计量、自由度和p值，卡方值越小越好，p值大于0.05。但是样本量越大，p值会减少倾向显著，作为拟合优度指标并不准确，需要考虑其他拟合指数。如果模型正确，卡方统计量会等于其自由度，但增加估计参数的数量会减小卡方值。\n近似均方根残差Root Mean Square Error of Approximation (RMSEA; Steiger, 1990) 和90%的置信区间，越小越好，小于0.08比较理想。\n比较拟合指数Comparative Fit Index (CFI; Bentler, 1990)，越大越好，应大于0.9。\n标准化均方根残差Standardized Root Mean Square Residual (SRMR)，越小越好，大于0.1说明拟合不好。"
  },
  {
    "objectID": "dataanlyr/lecture7/sem.html#验证性因子分析",
    "href": "dataanlyr/lecture7/sem.html#验证性因子分析",
    "title": "调节效应、中介效应和结构方程",
    "section": "验证性因子分析",
    "text": "验证性因子分析\n\n验证性因子分析的原理\n\n不同于探索性因子分析（EFA），验证性因子分析（CFA）必须有特定的理论或概念架构作为基础。CFA可以作为结构方程模型的前置步骤，也可以独立进行，只检验测量模型。\nCFA要求构念要有明确的操作化定义界定内容与范畴；测量构念的指标要能被明确指出，并且同一构念指标要具有相当的一致性，不同构念指标要具有区分度；还要经过统计验证观察数据能否支持或推翻构念是否存在的假设。\nCFA模型中，从潜变量到测量变量的箭头，代表研究者所假设的潜变量到测量变量的因果关系，统计估计量称为因子载荷，类似于回归系数，测量变量的变异可以拆解为共同变异（common variance）和独特变异（unique variance）。\n与回归模型将测量误差作为随机误差处理不同，CFA将测量变量中分离独特变异，这种变异包括了随机误差和系统误差（例如方法效益带来的误差，会导致题项相关系数偏大）。CFA可以通过共变关系分析和多维测量假设有效估计独特变异中的系统性误差。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCFA的内部拟合检验\n\nCFA除了要求整体拟合效果外，还要求针对个别因子的质量进行检验，了解个别参数是否理想（项目信效度），各潜变量的组合情形是否稳定可靠（构念的信效度）。遇到不理想的参数，可以剔除不良题项或添加参数提高测量模型的内在拟合。\n项目质量检验：题项测量误差越小则信度越高，而因子载荷越高则测量误差越小，所以可以用因子载荷来判断题项质量。一般当因子载荷\\(\\lambda\\)大于0.71时，\\(\\lambda^2\\)为50%，意味着潜变量能够解释测量变量50%的变异（即回归的R方），说明项目具有理想质量。但是对于社会科学而言，编制的量表因子载荷都不会太高，因子载荷\\(\\lambda\\)大于0.55（\\(\\lambda^2\\)为30%）即是理想的结果。\n组合信度（\\(\\rho_c\\)）：对于一组题项而言，潜变量的变异代表真实分数的变异，因此题项的组合信度可以用测量变量变异被潜变量解释的百分比来表示（类似于内部一致性系数，\\(Cronbach's \\quad \\alpha\\)）。一般量表信度需达到0.7，社会科学领域不易达到此水平，0.5以上可认为获得基本稳定性。\n\n\\[\\rho_c=\\frac{(\\Sigma\\lambda_i)^2}{((\\Sigma\\lambda_i)^2 + \\Sigma\\Theta_{ii}+2\\Sigma\\Theta_{ij})}\\]\n\n平均变异萃取量（\\(\\rho_\\nu\\)）：测量题项的因子载荷越高，表示题项能够反映潜变量的能力越高，潜变量因子能够解释各个测量变量变异的程度越大，因此可以用平均变异萃取量（即EFA中的特征值）反映潜变量被测量变量有效估计的聚敛程度。\\(\\rho_\\nu\\)大于0.5，表示潜变量聚敛能力理想。\n\n\\[\\rho_{\\nu}=\\frac{\\Sigma\\lambda^2_i}{(\\Sigma\\lambda^2_i + \\Sigma\\Theta_{ii})}=\\frac{\\Sigma\\lambda^2_i}{n}\\]\n\n因素区辩力：不同潜变量之间必须能够有效分离。\n\n相关系数区间估计：如果两个潜变量的相关系数的95%置信区间包含1，表示构念缺乏区辩力。\n\n竞争模式比较法：将设定CFA模型与完全相关模型（将潜变量的相关设定为1）相比较，如果两个模型没有区别，表示构念缺乏区辩力。\n\n平均变异萃取量比较法：比较两个潜变量的平均变异萃取量的平均值是否大于其相关系数的平方。\n\n\n\n\n验证性因素分析的步骤\n\n建立测量模型的假设\n进行模型识别，输入模型指令\n执行CFA分析\n结果分析\n模型修正\n完成分析，给出报告\n\n\n\n组织创新气氛测量模型\n案例来自《组织创新气氛量表》（邱皓政，1999），样本是384位企业员工，量表为Likert式6点度量的自陈量表，基于理论和文献先界定影响组织气氛知觉的因素包括组织价值、工作方式、团队合作、领导风格、学习成长、环境气氛等6个因素，每个因素采用3个题项测量，共有18个题项，题项描述性统计如下。\n\nlibrary(haven)\nlibrary(tidyverse)\nlibrary(modelsummary)\nlibrary(lavaan)\nlibrary(semPlot)\n\ndat &lt;- read_sav(\"ch05.sav\")\nnames(dat) &lt;- c(\"A1\",\"A2\",\"A3\",\"B1\",\"B2\",\"B3\",\"C1\",\"C2\",\"C3\",\"D1\",\"D2\",\"D3\",\"E1\",\"E2\",\"E3\",\"F1\",\"F2\",\"F3\")\ndat &lt;- zap_labels(dat)\ndim(dat) \n\n[1] 313  18\n\n\n\ndatasummary_skim(dat, fun_numeric = list('取值'=NUnique, '缺失值'=PercentMissing, '均值'=Mean, '中位数'=Median,'标准差'=SD, '最小值'=Min, '最大值'=Max), output = \"data.frame\", type = \"numeric\", fmt_sprintf(\"%.2f\")) |&gt; knitr::kable()\n\n\n\n\n\n取值\n缺失值\n均值\n中位数\n标准差\n最小值\n最大值\n\n\n\n\nA1\n6\n0\n4.42\n5.00\n0.98\n1.00\n6.00\n\n\nA2\n6\n0\n4.31\n4.00\n1.02\n1.00\n6.00\n\n\nA3\n6\n0\n4.07\n4.00\n0.97\n1.00\n6.00\n\n\nB1\n6\n0\n4.02\n4.00\n1.16\n1.00\n6.00\n\n\nB2\n6\n0\n4.25\n4.00\n1.16\n1.00\n6.00\n\n\nB3\n6\n0\n4.24\n4.00\n1.09\n1.00\n6.00\n\n\nC1\n6\n0\n4.37\n4.00\n0.98\n1.00\n6.00\n\n\nC2\n6\n0\n4.34\n4.00\n1.03\n1.00\n6.00\n\n\nC3\n6\n0\n4.31\n4.00\n1.05\n1.00\n6.00\n\n\nD1\n6\n0\n4.83\n5.00\n0.94\n1.00\n6.00\n\n\nD2\n5\n0\n4.95\n5.00\n0.84\n2.00\n6.00\n\n\nD3\n5\n0\n4.83\n5.00\n0.91\n2.00\n6.00\n\n\nE1\n6\n0\n4.63\n5.00\n0.97\n1.00\n6.00\n\n\nE2\n6\n0\n4.73\n5.00\n1.01\n1.00\n6.00\n\n\nE3\n6\n0\n4.70\n5.00\n0.98\n1.00\n6.00\n\n\nF1\n6\n0\n4.23\n4.00\n1.17\n1.00\n6.00\n\n\nF2\n6\n0\n4.63\n5.00\n1.09\n1.00\n6.00\n\n\nF3\n6\n0\n4.49\n5.00\n0.94\n1.00\n6.00\n\n\n\n\n\n\n测量模型的设定\n\n模型有18个测量变量和6个潜变量\n模型中18个测量误差\n为确定6个潜变量的度量，每个因素方差设定为1\n每个测量变量只受单一潜变量影响，因此有18个因子载荷参数\n因子共变允许自由估计，产生15个相关系数参数\n测量误差之间视为独立，没有共变关系\n\n\n\n\n\n\n\n\n\n\n\n\n测量模型的识别\n\n测量数据\\(DP=18\\times(18+1)/2=171\\)\n模型待估参数包括18个因子载荷、18个测量误差、15个潜变量协方差（潜变量方差设定为1，不需要估计），因此待估参数\\(t=18+18+15=51\\)\n因为待估参数小于测量数据，所以测量模型可以识别\n\n\n\n\n模型分析与报告\nlavaan包的模型设定默认不同潜变量之间具有相关性（即允许因子共变），如果需要不同的设定，可以在cfa函数中用\\(orthogonal=T\\)将协方差约束为0；cfa函数默认是将第1个因子载荷设定为1，如需固定潜变量方差为1，可以通过\\(std.lv=TRUE\\)来设定。\n\ncfa_model  &lt;-'\n#定义测量模型  \n  FA =~ L11*A1 + L21*A2 + L31*A3\n  FB =~ L12*B1 + L22*B2 + L32*B3\n  FC =~ L13*C1 + L23*C2 + L33*C3\n  FD =~ L14*D1 + L24*D2 + L34*D3\n  FE =~ L15*E1 + L25*E2 + L35*E3\n  FF =~ L16*F1 + L26*F2 + L36*F3'\n\ncfa_fit &lt;- cfa(model = cfa_model, \n               data = dat,\n               std.lv = TRUE) # 根据模型设定，固定潜变量方差为1\nsummary(cfa_fit, \n        fit.measures = T, # 输出拟合指标\n        standard = T)  # 输出标准化解\n\nlavaan 0.6-19 ended normally after 31 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        51\n\n  Number of observations                           313\n\nModel Test User Model:\n                                                      \n  Test statistic                               241.755\n  Degrees of freedom                               120\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              2842.819\n  Degrees of freedom                               153\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.955\n  Tucker-Lewis Index (TLI)                       0.942\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6751.785\n  Loglikelihood unrestricted model (H1)      -6630.907\n                                                      \n  Akaike (AIC)                               13605.569\n  Bayesian (BIC)                             13796.626\n  Sample-size adjusted Bayesian (SABIC)      13634.870\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.057\n  90 Percent confidence interval - lower         0.047\n  90 Percent confidence interval - upper         0.067\n  P-value H_0: RMSEA &lt;= 0.050                    0.132\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.052\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  FA =~                                                                 \n    A1       (L11)    0.815    0.051   15.967    0.000    0.815    0.830\n    A2       (L21)    0.706    0.055   12.733    0.000    0.706    0.692\n    A3       (L31)    0.614    0.054   11.430    0.000    0.614    0.634\n  FB =~                                                                 \n    B1       (L12)    0.789    0.062   12.759    0.000    0.789    0.682\n    B2       (L22)    0.961    0.058   16.591    0.000    0.961    0.833\n    B3       (L32)    0.856    0.056   15.406    0.000    0.856    0.788\n  FC =~                                                                 \n    C1       (L13)    0.699    0.053   13.178    0.000    0.699    0.717\n    C2       (L23)    0.736    0.056   13.119    0.000    0.736    0.715\n    C3       (L33)    0.696    0.058   11.950    0.000    0.696    0.663\n  FD =~                                                                 \n    D1       (L14)    0.810    0.045   18.177    0.000    0.810    0.867\n    D2       (L24)    0.741    0.040   18.750    0.000    0.741    0.886\n    D3       (L34)    0.655    0.047   14.086    0.000    0.655    0.720\n  FE =~                                                                 \n    E1       (L15)    0.806    0.046   17.392    0.000    0.806    0.830\n    E2       (L25)    0.912    0.046   19.851    0.000    0.912    0.906\n    E3       (L35)    0.791    0.047   16.810    0.000    0.791    0.811\n  FF =~                                                                 \n    F1       (L16)    0.641    0.066    9.663    0.000    0.641    0.550\n    F2       (L26)    0.828    0.058   14.296    0.000    0.828    0.758\n    F3       (L36)    0.786    0.049   16.142    0.000    0.786    0.837\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  FA ~~                                                                 \n    FB                0.542    0.054    9.989    0.000    0.542    0.542\n    FC                0.494    0.061    8.102    0.000    0.494    0.494\n    FD                0.417    0.058    7.176    0.000    0.417    0.417\n    FE                0.526    0.052   10.080    0.000    0.526    0.526\n    FF                0.695    0.046   15.148    0.000    0.695    0.695\n  FB ~~                                                                 \n    FC                0.697    0.047   14.906    0.000    0.697    0.697\n    FD                0.447    0.055    8.127    0.000    0.447    0.447\n    FE                0.575    0.048   12.091    0.000    0.575    0.575\n    FF                0.391    0.061    6.403    0.000    0.391    0.391\n  FC ~~                                                                 \n    FD                0.522    0.055    9.493    0.000    0.522    0.522\n    FE                0.603    0.050   12.127    0.000    0.603    0.603\n    FF                0.600    0.054   11.032    0.000    0.600    0.600\n  FD ~~                                                                 \n    FE                0.557    0.046   12.046    0.000    0.557    0.557\n    FF                0.316    0.062    5.137    0.000    0.316    0.316\n  FE ~~                                                                 \n    FF                0.443    0.056    7.944    0.000    0.443    0.443\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .A1                0.300    0.046    6.509    0.000    0.300    0.311\n   .A2                0.544    0.055    9.980    0.000    0.544    0.522\n   .A3                0.561    0.052   10.700    0.000    0.561    0.598\n   .B1                0.716    0.068   10.525    0.000    0.716    0.535\n   .B2                0.408    0.057    7.211    0.000    0.408    0.306\n   .B3                0.447    0.052    8.602    0.000    0.447    0.379\n   .C1                0.461    0.050    9.305    0.000    0.461    0.485\n   .C2                0.519    0.055    9.354    0.000    0.519    0.489\n   .C3                0.618    0.061   10.173    0.000    0.618    0.560\n   .D1                0.217    0.031    7.055    0.000    0.217    0.248\n   .D2                0.151    0.024    6.204    0.000    0.151    0.215\n   .D3                0.399    0.037   10.799    0.000    0.399    0.482\n   .E1                0.292    0.032    9.243    0.000    0.292    0.311\n   .E2                0.181    0.030    6.113    0.000    0.181    0.179\n   .E3                0.325    0.033    9.733    0.000    0.325    0.342\n   .F1                0.948    0.083   11.400    0.000    0.948    0.698\n   .F2                0.507    0.058    8.700    0.000    0.507    0.425\n   .F3                0.265    0.042    6.276    0.000    0.265    0.300\n    FA                1.000                               1.000    1.000\n    FB                1.000                               1.000    1.000\n    FC                1.000                               1.000    1.000\n    FD                1.000                               1.000    1.000\n    FE                1.000                               1.000    1.000\n    FF                1.000                               1.000    1.000\n\n\n\n模型结果的报告\nsummary报告第一部分为模型基本信息，估计方法为最大似然估计，优化方法为非线性最小优化，待估参数51个，样本量313。\nsummary报告第二部分为拟合指标，包括卡方值和检验结果、CFI、RMSEA、SRMR等。\n\n模型拟合度分析：对照前面的标准，卡方值显著性水平\\(0&lt;0.05\\)，表示假设模型与观察值之间有显著的差异；\\(\\chi^2/df&gt;2\\)也说明拟合度不理想；NFI、NNFI、CFI均大于标准0.9，SRMR小于标准0.08，这四个指标都表明拟合比较理想；RMSEA为0.057略微大于标准0.05，表明模型拟合不理想；综合来看，理论模型没有达到最佳的拟合度，仍有修正的空间。\n\n\nfitmeasures(cfa_fit, c(\"chisq\", \"df\", \"pvalue\", \"nfi\",\"nnfi\",\"cfi\", \"rmsea\", \"srmr\"))\n\n  chisq      df  pvalue     nfi    nnfi     cfi   rmsea    srmr \n241.755 120.000   0.000   0.915   0.942   0.955   0.057   0.052 \n\n\nsummary报告第三部分为参数估计结果，包括因子载荷、潜变量协方差、测量误差和潜变量的方差。\n\n潜变量（Latent variables）部分为因子载荷参数估计值相关结果，std.lv对应的是潜变量被标准化时的载荷，std.all对应的是所有变量都被标准化时的载荷（类似于标准化的回归系数）。\n\n协方差（Covariances）部分为潜变量协方差，由于潜变量方差设定为1，此处协方差也等于潜变量间的相关系数。\n\n方差（Variances）部分为测量误差的方差和潜变量的方差，由于潜变量方差设定为1，所以此处均为1。\n\n\n\n残差分析\n模型拟合后可以依据拟合协方差矩阵列出估计的测量变量的方差和协方差，这些模型导出数与实际观察值之间的差距即为残差。通过分析标准化残差的大小和分布，可以了解测量变量间关系的拟合程度，确定存在问题的题项。\n\nfitted(cfa_fit) # 提取拟合协方差矩阵\n\n$cov\n      A1    A2    A3    B1    B2    B3    C1    C2    C3    D1    D2    D3\nA1 0.965                                                                  \nA2 0.576 1.043                                                            \nA3 0.500 0.433 0.937                                                      \nB1 0.349 0.302 0.262 1.338                                                \nB2 0.425 0.368 0.320 0.759 1.332                                          \nB3 0.378 0.328 0.285 0.675 0.823 1.179                                    \nC1 0.282 0.244 0.212 0.385 0.469 0.417 0.950                              \nC2 0.297 0.257 0.223 0.405 0.494 0.439 0.515 1.061                        \nC3 0.280 0.243 0.211 0.383 0.467 0.415 0.487 0.512 1.102                  \nD1 0.275 0.239 0.207 0.286 0.348 0.310 0.296 0.312 0.295 0.873            \nD2 0.252 0.218 0.190 0.262 0.319 0.284 0.271 0.285 0.269 0.601 0.700      \nD3 0.223 0.193 0.168 0.231 0.282 0.251 0.239 0.252 0.238 0.531 0.486 0.829\nE1 0.345 0.299 0.260 0.365 0.445 0.396 0.340 0.358 0.338 0.363 0.332 0.294\nE2 0.391 0.339 0.294 0.413 0.504 0.448 0.385 0.405 0.383 0.411 0.376 0.333\nE3 0.339 0.294 0.255 0.359 0.437 0.389 0.334 0.352 0.332 0.357 0.326 0.289\nF1 0.363 0.315 0.274 0.198 0.241 0.214 0.269 0.283 0.268 0.164 0.150 0.133\nF2 0.469 0.407 0.353 0.255 0.311 0.277 0.347 0.366 0.346 0.212 0.194 0.171\nF3 0.446 0.386 0.335 0.243 0.296 0.263 0.330 0.347 0.328 0.201 0.184 0.163\n      E1    E2    E3    F1    F2    F3\nA1                                    \nA2                                    \nA3                                    \nB1                                    \nB2                                    \nB3                                    \nC1                                    \nC2                                    \nC3                                    \nD1                                    \nD2                                    \nD3                                    \nE1 0.942                              \nE2 0.735 1.013                        \nE3 0.638 0.721 0.951                  \nF1 0.229 0.259 0.225 1.359            \nF2 0.296 0.335 0.291 0.531 1.193      \nF3 0.281 0.318 0.276 0.504 0.651 0.883\n\ncfares &lt;- resid(cfa_fit)  # 提取残差\ncfares$cov\n\n       A1     A2     A3     B1     B2     B3     C1     C2     C3     D1     D2\nA1  0.000                                                                      \nA2 -0.003  0.000                                                               \nA3  0.022 -0.037  0.000                                                        \nB1 -0.080  0.057  0.037  0.000                                                 \nB2 -0.009  0.104 -0.057  0.001  0.000                                          \nB3 -0.021  0.053 -0.011  0.002 -0.001  0.000                                   \nC1 -0.085  0.022 -0.054  0.022  0.015  0.024  0.000                            \nC2 -0.019  0.039 -0.101  0.005 -0.020 -0.041  0.042  0.000                     \nC3  0.070  0.167  0.079 -0.004  0.002  0.006 -0.050 -0.004  0.000              \nD1  0.030  0.113 -0.048 -0.012  0.026  0.043 -0.052 -0.006  0.022  0.000       \nD2 -0.033  0.021 -0.071 -0.031 -0.041 -0.029 -0.076 -0.003  0.018  0.008  0.000\nD3 -0.010  0.075 -0.050 -0.005  0.057  0.067  0.094  0.114  0.117 -0.028  0.004\nE1 -0.017  0.015  0.087  0.021  0.073  0.063 -0.001 -0.077  0.047  0.055  0.013\nE2 -0.047  0.100  0.032 -0.029 -0.034 -0.024  0.024 -0.034  0.016 -0.021 -0.054\nE3 -0.103  0.105 -0.010 -0.031 -0.013  0.019  0.005 -0.004  0.045  0.005 -0.019\nF1 -0.018 -0.060  0.058 -0.035  0.097  0.021  0.070  0.118  0.111  0.107  0.130\nF2 -0.007 -0.043 -0.064 -0.122 -0.060  0.004 -0.080 -0.048  0.121 -0.015 -0.005\nF3  0.031 -0.007  0.018 -0.027  0.040  0.015 -0.006 -0.057  0.024 -0.033 -0.044\n       D3     E1     E2     E3     F1     F2     F3\nA1                                                 \nA2                                                 \nA3                                                 \nB1                                                 \nB2                                                 \nB3                                                 \nC1                                                 \nC2                                                 \nC3                                                 \nD1                                                 \nD2                                                 \nD3  0.000                                          \nE1  0.083  0.000                                   \nE2  0.074 -0.001  0.000                            \nE3  0.045 -0.016  0.010  0.000                     \nF1  0.187  0.076  0.109  0.097  0.000              \nF2  0.038 -0.056 -0.012 -0.019  0.019  0.000       \nF3  0.028 -0.017  0.001 -0.016 -0.032  0.009  0.000\n\nrange(cfares$cov); median(cfares$cov)  # 残差的全距和中位数\n\n[1] -0.1216818  0.1866437\n\n\n[1] 7.123659e-07\n\nresid(cfa_fit, type=\"standardized\") # 提取标准化残差，方便与标准正态分布比较\n\n$type\n[1] \"standardized\"\n\n$cov\n       A1     A2     A3     B1     B2     B3     C1     C2     C3     D1     D2\nA1  0.000                                                                      \nA2 -0.363  0.000                                                               \nA3  1.765 -1.670  0.000                                                        \nB1 -2.017  1.218  0.774  0.000                                                 \nB2 -0.326  2.626 -1.426  0.055  0.000                                          \nB3 -0.700  1.340 -0.288  0.081 -0.146  0.000                                   \nC1 -2.886  0.559 -1.395  0.552  0.522  0.784  0.000                            \nC2 -0.611  0.950 -2.455  0.117 -0.662 -1.305  2.421  0.000                     \nC3  1.876  3.832  1.833 -0.099  0.044  0.162 -2.676 -0.185  0.000              \nD1  1.262  3.253 -1.394 -0.301  0.901  1.417 -1.900 -0.216  0.653  0.000       \nD2 -1.688  0.684 -2.317 -0.888 -1.700 -1.090 -3.170 -0.104  0.599  3.454  0.000\nD3 -0.301  1.933 -1.283 -0.112  1.480  1.767  2.707  3.162  3.024 -4.148  0.748\nE1 -0.643  0.417  2.439  0.528  2.335  1.975 -0.045 -2.494  1.359  2.226  0.609\nE2 -2.361  3.078  0.956 -0.781 -1.407 -0.873  0.943 -1.297  0.479 -1.071 -3.481\nE3 -3.617  2.895 -0.260 -0.760 -0.408  0.568  0.172 -0.119  1.255  0.213 -0.859\nF1 -0.456 -1.244  1.205 -0.544  1.656  0.373  1.449  2.317  2.177  2.172  2.947\nF2 -0.295 -1.216 -1.743 -2.351 -1.483  0.104 -2.284 -1.330  2.968 -0.463 -0.189\nF3  1.823 -0.259  0.661 -0.666  1.394  0.490 -0.222 -2.153  0.745 -1.412 -2.229\n       D3     E1     E2     E3     F1     F2     F3\nA1                                                 \nA2                                                 \nA3                                                 \nB1                                                 \nB2                                                 \nB3                                                 \nC1                                                 \nC2                                                 \nC3                                                 \nD1                                                 \nD2                                                 \nD3  0.000                                          \nE1  2.629  0.000                                   \nE2  2.415 -0.342  0.000                            \nE3  1.378 -1.831  2.050  0.000                     \nF1  3.664  1.520  2.231  1.915  0.000              \nF2  0.933 -1.571 -0.403 -0.528  0.649  0.000       \nF3  0.835 -0.641  0.056 -0.579 -2.117  1.473  0.000\n\ncfastd &lt;- cfares$cov[lower.tri(unclass(cfares$cov),diag = F)]\n\nggplot(mapping = aes(sample=cfastd)) +\n  geom_qq() +\n  geom_qq_line() # 绘制QQ残差散点图分析是否服从正态分布\n\n\n\n\n\n\n\n\n\n模型修饰指数：残差分析除了初步观察测量变量间的问题，还可以将模型没有设定的关系参数加入假设，并与原设定模型相比，计算模型如果调整后这些残差变化对于模型改善的贡献，即为模型修饰指数（Modification Index）。\n当MI指数大于5时，表示该残差有修正的必要。EPC（expected parameter change）表示预期的参数改变量，一般综合考虑MI和EPC确定修正的方向。\n\n\nmodificationindices(cfa_fit, sort. = T, maximum.number = 6)\n\n    lhs op rhs     mi    epc sepc.lv sepc.all sepc.nox\n96   FC =~  D3 19.838  0.241   0.241    0.265    0.265\n63   FA =~  C3 14.826  0.289   0.289    0.276    0.276\n118  FE =~  A1 14.048 -0.258  -0.258   -0.262   -0.262\n265  D1 ~~  D2 13.890  0.192   0.192    1.061    1.061\n266  D1 ~~  D3 13.426 -0.132  -0.132   -0.448   -0.448\n175  A2 ~~  E1 12.247 -0.097  -0.097   -0.243   -0.243\n\n\n\n\n内在拟合度检验\n根据内部拟合检验部分提出的各项内部拟合指标，对测量模型进行检验。\n\n项目质量检验：因子载荷均大于0.55，大部分大于0.71，可以通过。\n\n\ncfa_lambda &lt;- inspect(cfa_fit,what=\"std\")$lambda\ncfa_lambda &lt;- cfa_lambda[cfa_lambda!=0]\nprint(cfa_lambda, digits = 2)  # 检验因子载荷\n\n [1] 0.83 0.69 0.63 0.68 0.83 0.79 0.72 0.71 0.66 0.87 0.89 0.72 0.83 0.91 0.81\n[16] 0.55 0.76 0.84\n\n\n\n组合信度CR均大于0.7，显示内在拟合较好；平均变异萃取量AVE除因素C、F外均大于0.5，说明拟合一般。\n\n\nlibrary(semTools)    \ncompRelSEM(cfa_fit) # 计算组合信度CR\n\n   FA    FB    FC    FD    FE    FF \n0.769 0.812 0.743 0.869 0.889 0.748 \n\nAVE(cfa_fit)        # 计算平均变异萃取量AVE\n\n   FA    FB    FC    FD    FE    FF \n0.523 0.592 0.487 0.681 0.725 0.499 \n\n\n\n因素区辩力：采取相关系数区间估计检验，潜变量的相关系数最大值为0.7，标准误约为0.05，95%置信区间为0.6-0.8，没有包含1，说明潜变量的区分度较好（参见summary报告第三部分的潜变量协方差，即相关系数）。\n\n\ncfa_res &lt;- summary(cfa_fit, standard=T)\ncfa_res$pe[43:57,] |&gt;\nselect(lhs, op, rhs, est, se, z, std.all, pvalue) |&gt;\nmutate(across(where(is.numeric), ~round(., 3)))\n\n   lhs op rhs   est    se      z std.all pvalue\n43  FA ~~  FB 0.542 0.054  9.989   0.542      0\n44  FA ~~  FC 0.494 0.061  8.102   0.494      0\n45  FA ~~  FD 0.417 0.058  7.176   0.417      0\n46  FA ~~  FE 0.526 0.052 10.080   0.526      0\n47  FA ~~  FF 0.695 0.046 15.148   0.695      0\n48  FB ~~  FC 0.697 0.047 14.906   0.697      0\n49  FB ~~  FD 0.447 0.055  8.127   0.447      0\n50  FB ~~  FE 0.575 0.048 12.091   0.575      0\n51  FB ~~  FF 0.391 0.061  6.403   0.391      0\n52  FC ~~  FD 0.522 0.055  9.493   0.522      0\n53  FC ~~  FE 0.603 0.050 12.127   0.603      0\n54  FC ~~  FF 0.600 0.054 11.032   0.600      0\n55  FD ~~  FE 0.557 0.046 12.046   0.557      0\n56  FD ~~  FF 0.316 0.062  5.137   0.316      0\n57  FE ~~  FF 0.443 0.056  7.944   0.443      0\n\n\n\n\n\n测量模型的调整\n\n测量模型调整的原则\n\n对于测量模型，可以对测量变量与潜变量之间的关系进行调整，删减测量变量或建立测量变量与其他潜变量之间的联系，也可以对测量变量残差的共变关系进行调整增减。\n对测量模型进行修正，应结合理论，避免过度拟合。\n\n\n\n\n\n\n\n\n\n\n\n\n假设模型的调整和设定\n根据前面CFA分析检验的结果，MI指数显示测量变量C3与潜变量A之间的关系如果纳入模型能够提升拟合度14.82，增加参数量因子载荷0.28，综合考量是贡献最大的参数改善。因此需要调整假设和设定，测量变量C3同时受到潜变量C和A的影响，其他参数设定不变。\n\n\n模型调整后的结果报告\n\n估计参数数量增加1个到52个，C3在A上的因子载荷为0.268，并且结果是显著的，并且其他变量显著性未受影响。\n\n\nnew_model &lt;-' \n#define the measurement model  \n  FA =~ L11*A1 + L21*A2 + L31*A3 + L37 * C3\n  FB =~ L12*B1 + L22*B2 + L32*B3\n  FC =~ L13*C1 + L23*C2 + L33*C3\n  FD =~ L14*D1 + L24*D2 + L34*D3\n  FE =~ L15*E1 + L25*E2 + L35*E3\n  FF =~ L16*F1 + L26*F2 + L36*F3'\n\nnew_fit &lt;- cfa(new_model,\n               data = dat,\n               std.lv = TRUE)\nsummary(new_fit)\n\nlavaan 0.6-19 ended normally after 30 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        52\n\n  Number of observations                           313\n\nModel Test User Model:\n                                                      \n  Test statistic                               226.794\n  Degrees of freedom                               119\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  FA =~                                               \n    A1       (L11)    0.805    0.051   15.803    0.000\n    A2       (L21)    0.711    0.055   12.859    0.000\n    A3       (L31)    0.616    0.054   11.510    0.000\n    C3       (L37)    0.268    0.065    4.128    0.000\n  FB =~                                               \n    B1       (L12)    0.789    0.062   12.766    0.000\n    B2       (L22)    0.961    0.058   16.591    0.000\n    B3       (L32)    0.855    0.056   15.407    0.000\n  FC =~                                               \n    C1       (L13)    0.729    0.054   13.605    0.000\n    C2       (L23)    0.755    0.057   13.284    0.000\n    C3       (L33)    0.533    0.066    8.080    0.000\n  FD =~                                               \n    D1       (L14)    0.811    0.045   18.185    0.000\n    D2       (L24)    0.741    0.040   18.743    0.000\n    D3       (L34)    0.655    0.047   14.078    0.000\n  FE =~                                               \n    E1       (L15)    0.806    0.046   17.389    0.000\n    E2       (L25)    0.912    0.046   19.857    0.000\n    E3       (L35)    0.791    0.047   16.809    0.000\n  FF =~                                               \n    F1       (L16)    0.641    0.066    9.656    0.000\n    F2       (L26)    0.828    0.058   14.305    0.000\n    F3       (L36)    0.786    0.049   16.137    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  FA ~~                                               \n    FB                0.545    0.054   10.046    0.000\n    FC                0.409    0.067    6.078    0.000\n    FD                0.424    0.058    7.319    0.000\n    FE                0.533    0.052   10.283    0.000\n    FF                0.700    0.046   15.368    0.000\n  FB ~~                                               \n    FC                0.668    0.049   13.556    0.000\n    FD                0.447    0.055    8.128    0.000\n    FE                0.574    0.048   12.089    0.000\n    FF                0.391    0.061    6.401    0.000\n  FC ~~                                               \n    FD                0.491    0.057    8.614    0.000\n    FE                0.569    0.052   10.857    0.000\n    FF                0.545    0.059    9.309    0.000\n  FD ~~                                               \n    FE                0.557    0.046   12.045    0.000\n    FF                0.316    0.062    5.137    0.000\n  FE ~~                                               \n    FF                0.443    0.056    7.943    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .A1                0.317    0.045    6.981    0.000\n   .A2                0.538    0.054    9.962    0.000\n   .A3                0.557    0.052   10.694    0.000\n   .C3                0.629    0.057   10.951    0.000\n   .B1                0.715    0.068   10.524    0.000\n   .B2                0.408    0.057    7.221    0.000\n   .B3                0.447    0.052    8.608    0.000\n   .C1                0.418    0.051    8.222    0.000\n   .C2                0.491    0.057    8.612    0.000\n   .D1                0.216    0.031    7.036    0.000\n   .D2                0.151    0.024    6.209    0.000\n   .D3                0.400    0.037   10.802    0.000\n   .E1                0.293    0.032    9.249    0.000\n   .E2                0.181    0.030    6.108    0.000\n   .E3                0.325    0.033    9.736    0.000\n   .F1                0.949    0.083   11.402    0.000\n   .F2                0.506    0.058    8.689    0.000\n   .F3                0.265    0.042    6.283    0.000\n    FA                1.000                           \n    FB                1.000                           \n    FC                1.000                           \n    FD                1.000                           \n    FE                1.000                           \n    FF                1.000                           \n\n\n\n模型拟合度的变化：卡方值依然显著未改善；卡方均值改善小于2；NFI、NNFI、CFI均有所增加；RMSEA、SRMR均有所减小，说明拟合度有改善。\n\n\nfitmeasures(cfa_fit, c(\"chisq\", \"df\", \"pvalue\", \"nfi\",\"nnfi\",\"cfi\", \"rmsea\", \"srmr\")) # 原设定模型\n\n  chisq      df  pvalue     nfi    nnfi     cfi   rmsea    srmr \n241.755 120.000   0.000   0.915   0.942   0.955   0.057   0.052 \n\nfitmeasures(new_fit, c(\"chisq\", \"df\", \"pvalue\", \"nfi\",\"nnfi\",\"cfi\", \"rmsea\", \"srmr\")) # 调整后模型\n\n  chisq      df  pvalue     nfi    nnfi     cfi   rmsea    srmr \n226.794 119.000   0.000   0.920   0.948   0.960   0.054   0.048 \n\n\n\n参数估计的比较：调整的测量变量C3与因素A的因子载荷为0.27，与MI指数增加因子载荷0.28非常接近，模型调整后，因子C的载荷有所变化，在C1、C2、C3上的载荷由0.70、0.74、0.70变化为0.73、0.76、0.53；因子A上的载荷变化不大。模型调整后，潜变量的相关系数有所变化，变化最大的是因子A和因子C之间的相关系数。\n\n\n\n\n\n\n\n\n\n\n\n\n\n模型的路径图\n\nsemPaths(cfa_fit, what = \"std\") \n\n\n\n\n\n\n\nsemPaths(new_fit, what = \"std\")"
  },
  {
    "objectID": "dataanlyr/lecture7/sem.html#路径分析",
    "href": "dataanlyr/lecture7/sem.html#路径分析",
    "title": "调节效应、中介效应和结构方程",
    "section": "路径分析",
    "text": "路径分析\n路径分析所包括的变量都是外显观察变量，不涉及潜变量，所用的操作可采用结构方程分析技术，相当于没有测量模型的结构方程。当模型结构过于复杂时，会出现自由度\\(df\\le0\\)的情况，无法计算拟合指数。\n\n模型设定与识别\n研究假设组织气氛知觉包括6个变量：组织价值（VALUE）、工作方式（JOBSTYLE）、团队合作（TEAMWORK）、领导风格（LEADERSH）、学习成长（LEARNING）、环境气氛（ENVIRON）。这6个变量均会影响员工的组织承诺感（commit），进而影响员工的工作绩效（out），此时，组织承诺作为中介变量，同时年资（TENURE）是控制变量，既影响组织承诺，也影响工作绩效。 整个模型有9个观察变量，因此测量数据\\(DP=(9\\times10)/2=45\\)。模型设定6个组织气氛知觉变量之间都相关，因此有15个相关系数参数，7个外生变量有7个方差参数，2个内生变量有2个解释残差（即回归方程的误差项），9个路径回归系数，所以待估参数\\(t=15+7+2+9=33\\)个。\\(t&lt;DP\\)因此模型时可识别的。\n\n\n\n\n\n\n\n\n\n\n\n参数估计\n\n变量描述\n\ndat &lt;- read_sav(\"ch07.sav\")\ndatasummary_skim(dat, fun_numeric = list('取值'=NUnique, '缺失值'=PercentMissing, '均值'=Mean, '中位数'=Median,'标准差'=SD, '最小值'=Min, '最大值'=Max), output = \"data.frame\", type = \"numeric\", fmt_sprintf(\"%.2f\")) |&gt; knitr::kable()\n\n\n\n\n\n取值\n缺失值\n均值\n中位数\n标准差\n最小值\n最大值\n\n\n\n\nout\n9\n0\n4.22\n4.33\n0.63\n2.00\n5.00\n\n\ncommit\n22\n0\n9.49\n9.67\n1.59\n4.00\n12.00\n\n\nVALUE\n15\n0\n4.27\n4.33\n0.81\n1.00\n6.00\n\n\nJOBSTYLE\n15\n0\n4.18\n4.33\n0.95\n1.00\n6.00\n\n\nTEAMWORK\n14\n0\n4.33\n4.33\n0.83\n1.33\n6.00\n\n\nLEADERSH\n12\n0\n4.87\n5.00\n0.77\n2.00\n6.00\n\n\nLEARNING\n14\n0\n4.71\n5.00\n0.88\n1.67\n6.00\n\n\nENVIRONM\n14\n0\n4.43\n4.33\n0.88\n1.00\n6.00\n\n\nTENURE\n106\n0\n9.12\n4.00\n9.23\n0.08\n32.00\n\n\n\n\n\n\n\n参数估计设定\n\n设定结构模型部分（代码part1），组织承诺commit被7个变量解释；员工绩效被2个变量解释。代码形式与回归方程类似。\n设定相关系数部分（代码part2），lavaan包会自动外生变量添加两两相关的相关系数参数，路径图里显示6个组织气氛变量与年资不相关，因此要手动设定。\n还可以定义感兴趣的间接效应和总效应（代码part3和part4）。\n\n\npath1 &lt;-'\n#part1：设定结构模型\n  commit~ a1*VALUE+a2*JOBSTYLE+a3*TEAMWORK+a4*LEADERSH\n          +a5*LEARNING+a6*ENVIRONM+a7*TENURE\n  out~c*TENURE+b*commit\n#part2：设定外生变量的相关系数\n  VALUE    ~~ JOBSTYLE \n  VALUE    ~~ TEAMWORK \n  VALUE    ~~ LEADERSH \n  VALUE    ~~ LEARNING\n  VALUE    ~~ ENVIRONM \n  JOBSTYLE ~~ TEAMWORK \n  JOBSTYLE ~~ LEADERSH \n  JOBSTYLE ~~ LEARNING \n  JOBSTYLE ~~ ENVIRONM \n  TEAMWORK ~~ LEADERSH \n  TEAMWORK ~~ LEARNING \n  TEAMWORK ~~ ENVIRONM\n  LEADERSH ~~ LEARNING \n  LEADERSH ~~ ENVIRONM \n  LEARNING ~~ ENVIRONM \n  # 设定相关系数为0\n  TENURE   ~~ 0*VALUE \n  TENURE   ~~ 0*JOBSTYLE \n  TENURE   ~~ 0*TEAMWORK \n  TENURE   ~~ 0*LEADERSH \n  TENURE   ~~ 0*LEARNING\n  TENURE   ~~ 0*ENVIRONM\n#part3：定义间接效应 (a*b)\n  a1b := a1*b\n  a2b := a2*b\n  a3b := a3*b\n  a4b := a4*b\n  a5b := a5*b\n  a6b := a6*b\n  a7b := a7*b\n#part4：定义年资TENURE的总效应\n  total := c + a7b'\n\n\n\n结果报告\nsummary报告第一部分为模型基本信息，估计方法为最大似然估计，优化方法为非线性最小优化，待估参数33个，样本量281.\nsummary报告第二部分为拟合度检验，包括卡方值和检验结果、CFI、RMSEA、SRMR等。\n\n模型拟合度分析：对照前面的标准，CFI值0.98大于标准0.9，SRMR值0.06小于标准0.08，表明拟合比较理想；RMSEA为0.067略微大于标准0.05，表明模型拟合不理想；综合来看，理论模型拟合度较好。\n\nsummary报告第三部分为参数估计结果，包括路径回归系数、外生变量协方差、外生变量方差和内生变量的解释残差、直接效应和间接效应参数估计。\n\n回归模型（Regressions）部分为路径回归系数估计值相关结果，std.all对应的是标准化的回归系数。团队合作（TEAMWORK）和领导风格（LEADERSH）对组织承诺（commit）的回归系数不显著。其余回归系数均显著。\n\n协方差（Covariances）部分为外生变量协方差，由于模型设定年资（TENURE）与其他外生变量不相关，因此固定为0，std.all是标准化后的协方差，即外生变量的相关系数。协方差估计结果均显著，说明外生变量间相关性存在。\n\n方差（Variances）部分为内生变量的解释残差（.commit和.out）和外生变量的方差。\n\n定义参数估计（Defined Parameters）部分为总效应和间接效应系数相关信息。同样，团队合作（TEAMWORK）和领导风格（LEADERSH）的间接效应系数a3b和a4b不显著，显然这与回归模型的结论一致。\n\n\npath1_fit &lt;- sem(path1, data = dat)\npath1_res &lt;- summary(path1_fit, fit.measures=T,standard = T)\npath1_res\n\nlavaan 0.6-19 ended normally after 43 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        33\n\n  Number of observations                           281\n\nModel Test User Model:\n                                                      \n  Test statistic                                27.106\n  Degrees of freedom                                12\n  P-value (Chi-square)                           0.007\n\nModel Test Baseline Model:\n\n  Test statistic                               866.406\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.982\n  Tucker-Lewis Index (TLI)                       0.945\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3522.025\n  Loglikelihood unrestricted model (H1)      -3508.473\n                                                      \n  Akaike (AIC)                                7110.051\n  Bayesian (BIC)                              7230.116\n  Sample-size adjusted Bayesian (SABIC)       7125.475\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.067\n  90 Percent confidence interval - lower         0.033\n  90 Percent confidence interval - upper         0.101\n  P-value H_0: RMSEA &lt;= 0.050                    0.181\n  P-value H_0: RMSEA &gt;= 0.080                    0.287\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.060\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  commit ~                                                              \n    VALUE     (a1)    0.428    0.116    3.687    0.000    0.428    0.218\n    JOBSTYLE  (a2)    0.242    0.100    2.409    0.016    0.242    0.146\n    TEAMWORK  (a3)    0.106    0.118    0.895    0.371    0.106    0.056\n    LEADERSH  (a4)    0.108    0.119    0.912    0.362    0.108    0.053\n    LEARNING  (a5)    0.363    0.115    3.159    0.002    0.363    0.203\n    ENVIRONM  (a6)    0.276    0.106    2.611    0.009    0.276    0.153\n    TENURE    (a7)    0.022    0.008    2.816    0.005    0.022    0.129\n  out ~                                                                 \n    TENURE     (c)    0.011    0.004    2.964    0.003    0.011    0.157\n    commit     (b)    0.172    0.021    8.135    0.000    0.172    0.430\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  VALUE ~~                                                              \n    JOBSTYLE          0.367    0.051    7.244    0.000    0.367    0.479\n    TEAMWORK          0.271    0.043    6.288    0.000    0.271    0.405\n    LEADERSH          0.206    0.039    5.274    0.000    0.206    0.331\n    LEARNING          0.344    0.047    7.328    0.000    0.344    0.486\n    ENVIRONM          0.369    0.047    7.782    0.000    0.369    0.524\n  JOBSTYLE ~~                                                           \n    TEAMWORK          0.420    0.053    7.853    0.000    0.420    0.530\n    LEADERSH          0.328    0.048    6.829    0.000    0.328    0.446\n    LEARNING          0.461    0.057    8.078    0.000    0.461    0.550\n    ENVIRONM          0.269    0.052    5.149    0.000    0.269    0.323\n  TEAMWORK ~~                                                           \n    LEADERSH          0.318    0.043    7.443    0.000    0.318    0.496\n    LEARNING          0.390    0.049    7.884    0.000    0.390    0.533\n    ENVIRONM          0.368    0.049    7.571    0.000    0.368    0.506\n  LEADERSH ~~                                                           \n    LEARNING          0.386    0.047    8.259    0.000    0.386    0.566\n    ENVIRONM          0.249    0.043    5.800    0.000    0.249    0.369\n  LEARNING ~~                                                           \n    ENVIRONM          0.332    0.050    6.626    0.000    0.332    0.430\n  VALUE ~~                                                              \n    TENURE            0.000                               0.000    0.000\n  JOBSTYLE ~~                                                           \n    TENURE            0.000                               0.000    0.000\n  TEAMWORK ~~                                                           \n    TENURE            0.000                               0.000    0.000\n  LEADERSH ~~                                                           \n    TENURE            0.000                               0.000    0.000\n  LEARNING ~~                                                           \n    TENURE            0.000                               0.000    0.000\n  ENVIRONM ~~                                                           \n    TENURE            0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .commit            1.456    0.123   11.853    0.000    1.456    0.586\n   .out               0.305    0.026   11.853    0.000    0.305    0.773\n    VALUE             0.647    0.055   11.853    0.000    0.647    1.000\n    JOBSTYLE          0.907    0.077   11.853    0.000    0.907    1.000\n    TEAMWORK          0.691    0.058   11.853    0.000    0.691    1.000\n    LEADERSH          0.598    0.050   11.853    0.000    0.598    1.000\n    LEARNING          0.776    0.065   11.853    0.000    0.776    1.000\n    ENVIRONM          0.765    0.065   11.853    0.000    0.765    1.000\n    TENURE           84.834    7.157   11.853    0.000   84.834    1.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    a1b               0.073    0.022    3.358    0.001    0.073    0.094\n    a2b               0.041    0.018    2.310    0.021    0.041    0.063\n    a3b               0.018    0.020    0.889    0.374    0.018    0.024\n    a4b               0.019    0.021    0.906    0.365    0.019    0.023\n    a5b               0.062    0.021    2.945    0.003    0.062    0.087\n    a6b               0.047    0.019    2.486    0.013    0.047    0.066\n    a7b               0.004    0.001    2.661    0.008    0.004    0.055\n    total             0.014    0.004    3.787    0.000    0.014    0.212\n\n\n\n\n\n直接与间接效应\n路径分析的目的之一是探讨内生变量被外生变量解释的总体效应、直接效应和间接效应。尽管采用分阶段回归的方式也能够估计出各个效应值，但是结构方程技术更方便对这些效应进行统计检验。\n\n直接效应：summary报告参数估计的回归模型部分的路径系数便是直接效应。例如组织价值（VALUE）对组织承诺（commit）的直接效应为a1，即0.428，而年资（TENURE）对工作绩效（out）的直接效应为c，即0.011.\n间接效应：外生变量通过中介变量对另一个内生变量产生的影响，便是间接效应。例如间接效应系数a1b代表组织价值对组织承诺的效应（a1=0.428）通过组织承诺对工作绩效的直接效应（b=0.172）间接影响到工作绩效，形成间接效应（a1b），并且数值上间接效应等于两段直接效应的乘积，即\\(a1b=a1\\times b=0.428\\times 0.172=0.073\\)，统计检验显示其p值为0.001，该间接效应具有统计上的意义。\n总效应：一个变量对结果变量直接效应和间接效应的总和便是总效应。例如，年资（TENURE）对工作绩效的直接效应（c=0.011），年资通过组织承诺对工作绩效的间接效应（a7b=0.004），那么年资对工作绩效的总效应\\(total=c+a7b=0.011+0.004\\simeq 0.015\\)，统计检验其p值为0，该总效应具有统计上的意义。\n\n\npath1_res$pe[c(1:9,40:47),] |&gt;\n  select(lhs, op, rhs, est, se, z, std.all, pvalue) |&gt;\n  mutate(across(where(is.numeric), ~round(., 3)))\n\n      lhs op      rhs   est    se     z std.all pvalue\n1  commit  ~    VALUE 0.428 0.116 3.687   0.218  0.000\n2  commit  ~ JOBSTYLE 0.242 0.100 2.409   0.146  0.016\n3  commit  ~ TEAMWORK 0.106 0.118 0.895   0.056  0.371\n4  commit  ~ LEADERSH 0.108 0.119 0.912   0.053  0.362\n5  commit  ~ LEARNING 0.363 0.115 3.159   0.203  0.002\n6  commit  ~ ENVIRONM 0.276 0.106 2.611   0.153  0.009\n7  commit  ~   TENURE 0.022 0.008 2.816   0.129  0.005\n8     out  ~   TENURE 0.011 0.004 2.964   0.157  0.003\n9     out  ~   commit 0.172 0.021 8.135   0.430  0.000\n40    a1b :=     a1*b 0.073 0.022 3.358   0.094  0.001\n41    a2b :=     a2*b 0.041 0.018 2.310   0.063  0.021\n42    a3b :=     a3*b 0.018 0.020 0.889   0.024  0.374\n43    a4b :=     a4*b 0.019 0.021 0.906   0.023  0.365\n44    a5b :=     a5*b 0.062 0.021 2.945   0.087  0.003\n45    a6b :=     a6*b 0.047 0.019 2.486   0.066  0.013\n46    a7b :=     a7*b 0.004 0.001 2.661   0.055  0.008\n47  total :=    c+a7b 0.014 0.004 3.787   0.212  0.000\n\n\n\n\n模型调整\n模型调整主要依据MI指数，考察大于5的取值，结合理论进行判断。\n\n工作绩效与组织承诺的残差间有显著的修正空间，MI值为15.2，但是预期变化量EPC为-0.24（原设定为0），表明有某种未观察到的变量，同时影响工作绩效和组织承诺，但是对它们的影响效应是相反的，尚缺乏有效的理论支撑。\n\n工作绩效对组织承诺的结构参数对应的MI值为15.2，表示该参数要纳入模型，但是如果纳入会导致回馈循环效应，并且预期改变量为-0.8，表示员工工作绩效越高，组织承诺越低，难以有理论或逻辑上的解释，缺乏合理性。\n\n学习成长、领导风格、组织价值、团队合作对工作绩效的结构参数对应的MI值均大于5，预测值也为正，表明这些变量得分越高，工作绩效越好，理论逻辑上是合理的。可以考虑增加这些变量对结果变量的直接效应。\n\n\nmodificationindices(path1_fit, sort. = T, maximum.number = 8)\n\n       lhs op      rhs     mi    epc sepc.lv sepc.all sepc.nox\n48  commit ~~      out 15.228 -0.244  -0.244   -0.366   -0.366\n63  commit  ~      out 15.228 -0.800  -0.800   -0.319   -0.319\n68     out  ~ LEARNING 13.063  0.158   0.158    0.222    0.222\n67     out  ~ LEADERSH  9.170  0.140   0.140    0.173    0.173\n64     out  ~    VALUE  7.031  0.127   0.127    0.162    0.162\n66     out  ~ TEAMWORK  6.771  0.115   0.115    0.152    0.152\n125 TENURE  ~ ENVIRONM  5.352  1.453   1.453    0.138    0.138\n120 TENURE  ~    VALUE  4.581  1.462   1.462    0.128    0.128\n\n\n先增加一条学习成长到工作绩效的直接效应，设定新的模型，试试看。\n\npath2 &lt;- '#part1：set the structure model\n  commit~ a1*VALUE+a2*JOBSTYLE+a3*TEAMWORK+a4*LEADERSH\n          +a5*LEARNING+a6*ENVIRONM+a7*TENURE\n  out~c*TENURE+b*commit + d*LEARNING # 添加直接效应d\n#part2：set the correlation\n  VALUE    ~~ JOBSTYLE \n  VALUE    ~~ TEAMWORK \n  VALUE    ~~ LEADERSH \n  VALUE    ~~ LEARNING\n  VALUE    ~~ ENVIRONM \n  JOBSTYLE ~~ TEAMWORK \n  JOBSTYLE ~~ LEADERSH \n  JOBSTYLE ~~ LEARNING \n  JOBSTYLE ~~ ENVIRONM \n  TEAMWORK ~~ LEADERSH \n  TEAMWORK ~~ LEARNING \n  TEAMWORK ~~ ENVIRONM\n  LEADERSH ~~ LEARNING \n  LEADERSH ~~ ENVIRONM \n  LEARNING ~~ ENVIRONM \n  # fix\n  TENURE   ~~ 0*VALUE \n  TENURE   ~~ 0*JOBSTYLE \n  TENURE   ~~ 0*TEAMWORK \n  TENURE   ~~ 0*LEADERSH \n  TENURE   ~~ 0*LEARNING\n  TENURE   ~~ 0*ENVIRONM\n#part3：define indirect effect (a*b)\n  a1b := a1*b\n  a2b := a2*b\n  a3b := a3*b\n  a4b := a4*b\n  a5b := a5*b\n  a6b := a6*b\n  a7b := a7*b\n#part4：define total effect of COMMIT\n  total := c + a7b'\n\npath2_fit &lt;- sem(path2, data = dat)\n\nsummary(path2_fit, fit.measures=T, standard=T)\n\nlavaan 0.6-19 ended normally after 44 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        34\n\n  Number of observations                           281\n\nModel Test User Model:\n                                                      \n  Test statistic                                13.714\n  Degrees of freedom                                11\n  P-value (Chi-square)                           0.249\n\nModel Test Baseline Model:\n\n  Test statistic                               866.406\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.997\n  Tucker-Lewis Index (TLI)                       0.989\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3515.330\n  Loglikelihood unrestricted model (H1)      -3508.473\n                                                      \n  Akaike (AIC)                                7098.659\n  Bayesian (BIC)                              7222.363\n  Sample-size adjusted Bayesian (SABIC)       7114.551\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.030\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.073\n  P-value H_0: RMSEA &lt;= 0.050                    0.736\n  P-value H_0: RMSEA &gt;= 0.080                    0.024\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.041\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  commit ~                                                              \n    VALUE     (a1)    0.428    0.116    3.687    0.000    0.428    0.218\n    JOBSTYLE  (a2)    0.242    0.100    2.409    0.016    0.242    0.146\n    TEAMWORK  (a3)    0.106    0.118    0.895    0.371    0.106    0.056\n    LEADERSH  (a4)    0.108    0.119    0.912    0.362    0.108    0.053\n    LEARNING  (a5)    0.363    0.115    3.159    0.002    0.363    0.203\n    ENVIRONM  (a6)    0.276    0.106    2.611    0.009    0.276    0.153\n    TENURE    (a7)    0.022    0.008    2.816    0.005    0.022    0.129\n  out ~                                                                 \n    TENURE     (c)    0.011    0.004    3.192    0.001    0.011    0.166\n    commit     (b)    0.125    0.024    5.201    0.000    0.125    0.315\n    LEARNING   (d)    0.158    0.043    3.706    0.000    0.158    0.222\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  VALUE ~~                                                              \n    JOBSTYLE          0.367    0.051    7.244    0.000    0.367    0.479\n    TEAMWORK          0.271    0.043    6.288    0.000    0.271    0.405\n    LEADERSH          0.206    0.039    5.274    0.000    0.206    0.331\n    LEARNING          0.344    0.047    7.328    0.000    0.344    0.486\n    ENVIRONM          0.369    0.047    7.782    0.000    0.369    0.524\n  JOBSTYLE ~~                                                           \n    TEAMWORK          0.420    0.053    7.853    0.000    0.420    0.530\n    LEADERSH          0.328    0.048    6.829    0.000    0.328    0.446\n    LEARNING          0.461    0.057    8.078    0.000    0.461    0.550\n    ENVIRONM          0.269    0.052    5.149    0.000    0.269    0.323\n  TEAMWORK ~~                                                           \n    LEADERSH          0.318    0.043    7.443    0.000    0.318    0.496\n    LEARNING          0.390    0.049    7.884    0.000    0.390    0.533\n    ENVIRONM          0.368    0.049    7.571    0.000    0.368    0.506\n  LEADERSH ~~                                                           \n    LEARNING          0.386    0.047    8.259    0.000    0.386    0.566\n    ENVIRONM          0.249    0.043    5.800    0.000    0.249    0.369\n  LEARNING ~~                                                           \n    ENVIRONM          0.332    0.050    6.626    0.000    0.332    0.430\n  VALUE ~~                                                              \n    TENURE            0.000                               0.000    0.000\n  JOBSTYLE ~~                                                           \n    TENURE            0.000                               0.000    0.000\n  TEAMWORK ~~                                                           \n    TENURE            0.000                               0.000    0.000\n  LEADERSH ~~                                                           \n    TENURE            0.000                               0.000    0.000\n  LEARNING ~~                                                           \n    TENURE            0.000                               0.000    0.000\n  ENVIRONM ~~                                                           \n    TENURE            0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .commit            1.456    0.123   11.853    0.000    1.456    0.586\n   .out               0.291    0.025   11.853    0.000    0.291    0.739\n    VALUE             0.647    0.055   11.853    0.000    0.647    1.000\n    JOBSTYLE          0.907    0.077   11.853    0.000    0.907    1.000\n    TEAMWORK          0.691    0.058   11.853    0.000    0.691    1.000\n    LEADERSH          0.598    0.050   11.853    0.000    0.598    1.000\n    LEARNING          0.776    0.065   11.853    0.000    0.776    1.000\n    ENVIRONM          0.765    0.065   11.853    0.000    0.765    1.000\n    TENURE           84.834    7.157   11.853    0.000   84.834    1.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    a1b               0.054    0.018    3.008    0.003    0.054    0.069\n    a2b               0.030    0.014    2.186    0.029    0.030    0.046\n    a3b               0.013    0.015    0.882    0.378    0.013    0.018\n    a4b               0.014    0.015    0.898    0.369    0.014    0.017\n    a5b               0.046    0.017    2.700    0.007    0.046    0.064\n    a6b               0.035    0.015    2.333    0.020    0.035    0.048\n    a7b               0.003    0.001    2.477    0.013    0.003    0.040\n    total             0.014    0.004    3.869    0.000    0.014    0.206\n\n\n学习成长到工作绩效的直接效应（d）为0.158，p值为0，说明该效应存在。比较前后模型的拟合度指标发现，卡方值明显下降，p值已大于0.05，RMSEA和SRMR都小于目标值。新模型完美拟合，无需再进行调整。\n\nfitmeasures(path1_fit, c(\"chisq\", \"df\", \"pvalue\", \"nfi\",\"nnfi\",\"cfi\", \"rmsea\", \"srmr\")) # 原设定模型\n\n chisq     df pvalue    nfi   nnfi    cfi  rmsea   srmr \n27.106 12.000  0.007  0.969  0.945  0.982  0.067  0.060 \n\nfitmeasures(path2_fit, c(\"chisq\", \"df\", \"pvalue\", \"nfi\",\"nnfi\",\"cfi\", \"rmsea\", \"srmr\")) # 调整后模型\n\n chisq     df pvalue    nfi   nnfi    cfi  rmsea   srmr \n13.714 11.000  0.249  0.984  0.989  0.997  0.030  0.041 \n\n\n前后两个模型是嵌套关系，可以进行嵌套模型的卡方差异检验，检验模型修正带来的拟合优度增幅是否具有显著意义。\n\nanova(path1_fit, path2_fit)\n\n\nChi-Squared Difference Test\n\n          Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \npath2_fit 11 7098.7 7222.4 13.714                                          \npath1_fit 12 7110.1 7230.1 27.106     13.391 0.20999       1  0.0002528 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n可以绘制两个模型的路径图进行比较。"
  },
  {
    "objectID": "dataanlyr/lecture7/sem.html#统合模型分析",
    "href": "dataanlyr/lecture7/sem.html#统合模型分析",
    "title": "调节效应、中介效应和结构方程",
    "section": "统合模型分析",
    "text": "统合模型分析\n\n统合模型的基础\n\n统合模型的构成：统合模型包含测量模型和结构模型。如果结构模型中的变量都是潜变量，则称为完全统合模型或完全潜在模型，如果结构模型中的某一个或某几个变量是单一指标的测量变量，则称为部分潜在模型。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n统合模型分析步骤：通常采用两阶段策略，第一个阶段是针对测量模型确定因素结构的拟合性，第二阶段则是在不改变测量模型的前提下，增加结构模型的设定，并评估结构模型的拟合性。该方法的优点在于测量模型的检测可以提供潜变量的聚敛与区分效度的信息，结构模型则可以提供预测效度的证据。\n变量组合的策略：结构方程模型分析经常会以变量组合策略简化测量模型，使结构模型得以在比较简化的情形下进行估计。\n\n例如某个潜变量有6个测量变量，可以每两题加起来求平均值，将6个变量聚合成3个变量以降低模型的复杂程度。聚合层次的变量分数称为组合分数。特别是当样本量太少，而模型估计参数很多，变通做法是通过将测量变量组合成单一变量，将潜变量转换为观察变量后进行路径分析。\n\n变量组合的优点：简化模型、提高模型拟合度、获得较理想的估计解、得到较好的测量信度、较高的变量解释力、观察变量的尺度更具有等距性与正态性、更理想的检验效力（每题的样本量与题数的比值更高）、避免特殊题项的干扰。\n\n变量组合不应为简化而简化，需要提出简化的理由与统计量上支持，必须经过验证性因子分析的检验，保证构念的单一维度，并且组合的题项要具有相同的尺度（例如都是二分变量或者都是相同点数的likert量表）。\n\n\n\n\n统合模型分析的操作\n250位教师的创意教学行为调查数据，包括教师的创造人格特质（Person）、创意教学自我效能感（Efficacy）、组织社会化（Socialized）、教学创新行为（Crea）等4个潜变量的10个测量变量。\n\n创造人格特质包括多角推理、兴趣广泛、乐在工作3个测量变量\n\n自我效能感包括自我肯定、自我防卫、社会支持寻求、外在压力抗衡4个测量变量\n\n组织社会化包括组织融入和工作熟练度2个测量变量\n\n教学创新行为已将9个量表题项加总成组合分数\n\n\n假设模型\n研究假设：\n\n教师创意教学⾃我效能越⾼，越能够表现出创意教学⾏为。\n\n教师个⼈创造性格越强、组织社会化程度越⾼，则创意教学⾃我效能越⾼。\n\n教师个⼈创造性格与组织社会化程度两项特质，会透过⾃我效能的中介作⽤，间接影响教师的创意教学⾏为。\n\n数据是以协方差矩阵的形式给出的。可以通过热力图展示测量变量间的相关系数。\n\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(modelsummary)\nlibrary(semPlot)\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\nlibrary(semptools) # 给路径图标记显著性\n\nN &lt;- 250 # 样本量\n# 协方差矩阵\nCOV &lt;- structure(c(17.711, 1.843, 0.801, 1.243, 1.692, 7.518, 7.585, \n                   6.499, 3.02, 2.663, 1.843, 0.404, 0.146, 0.227, 0.226, 1.074, \n                   1.062, 0.906, 0.369, 0.318, 0.801, 0.146, 0.374, 0.11, 0.115, \n                   0.301, 0.538, 0.388, 0.237, 0.307, 1.243, 0.227, 0.11, 0.589, \n                   0.208, 0.531, 0.609, 0.434, 0.322, 0.335, 1.692, 0.226, 0.115, \n                   0.208, 0.393, 0.741, 0.806, 0.664, 0.196, 0.326, 7.518, 1.074, \n                   0.301, 0.531, 0.741, 7.565, 5.202, 4.53, 1.947, 2.092, 7.585, \n                   1.062, 0.538, 0.609, 0.806, 5.202, 7.046, 4.626, 1.524, 1.824, \n                   6.499, 0.906, 0.388, 0.434, 0.664, 4.53, 4.626, 6.335, 1.649, \n                   1.662, 3.02, 0.369, 0.237, 0.322, 0.196, 1.947, 1.524, 1.649, \n                   4.482, 2.335, 2.663, 0.318, 0.307, 0.335, 0.326, 2.092, 1.824, \n                   1.662, 2.335, 2.982), dim = c(10L, 10L), dimnames = list(c(\"CREAT\", \n                                                                              \"SEFF1\", \"SEFF2\", \"SEFF3\", \"SEFF4\", \"PER1\", \"PER2\", \"PER3\", \"SOC1\", \n                                                                              \"SOC2\"), c(\"CREAT\", \"SEFF1\", \"SEFF2\", \"SEFF3\", \"SEFF4\", \"PER1\", \n                                                                                         \"PER2\", \"PER3\", \"SOC1\", \"SOC2\")))\n\n# X和Y相关系数，等于二者的协方差除以二者标准差的积：\nCOR &lt;- COV / (sqrt(diag(COV) %*% t(diag(COV))))\n\n# 调用corrplot命令，绘制热力图\n# Generate a lighter palette\ncol &lt;- colorRampPalette(c(\"#BB4444\", \"#EE9988\", \"#FFFFFF\", \"#77AADD\", \"#4477AA\"))\n\ncorrplot(COR, method = \"shade\", shade.col = NA, tl.col = \"black\", tl.srt = 45,\n         col = col(200), addCoef.col = \"black\", cl.pos = \"n\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n模型设定与识别\n\n由于该模型有10个测量变量，测量数据的数目为\\(DP=10\\times11/2=55\\)个。\n\n模型有5个外生测量变量，5个内生测量变量，所以有5个外生测量残差，4个内生测量残差（潜变量CREA只有1个指标，完美预测，该测量变量残差固定为0，因后面要调整测量模型，概念模型中依然采取测量变量和潜变量分开的方式，本质是1个变量），待估参数9个。\n\n模型有2个外生潜变量，2个内生潜变量，所以外生潜变量的协方差1个（概念图未标出）和方差2个（与之前的CFA部分处理不同，此处不通过限定外生变量方差为1来标准化，改为限定第一个因子载荷为1，如下），内生潜变量2个解释残差，待估参数5个。\n\n每个测量变量仅受一个潜变量影响，故产生10个因子载荷，为确立两个潜变量的尺度，潜变量的第一因子载荷固定为1，所以要扣除4个，待估参数6个。\n\n内生潜变量被外生潜变量解释，有5个待估结构参数（路径系数）。\n\n因此，待估参数\\(t=9+5+6+5=25\\)个。\n\n\n\n测量模型分析阶段\n模型拟合情况大致良好，卡方p值为0，卡方自由度比为2.6，接近2，NFI为0.94，CFI为0.96，RMSEA为0.08，SRMR为0.04，可以继续检验MI指数按照验证性因子分析介绍的方式调整测量模型，这里直接进行第二阶段的结构模型估计。\n\ncfa_mod &lt;- '\n# measurement model\n    CREA =~ 1*CREAT # 由于CREA只有一个测量变量，因此设定因子载荷为1，默认测量变量残差方差为0\n    SEFF =~ SEFF1 + SEFF2 + SEFF3 + SEFF4\n    PER  =~ PER1  + PER2  + PER3\n    SOC  =~ SOC1  + SOC2'\n\ncfa_fit &lt;- cfa(model = cfa_mod, sample.cov = COV, sample.nobs = 250)\nsummary(cfa_fit, standard=T)\n\nlavaan 0.6-19 ended normally after 63 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        25\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                                80.964\n  Degrees of freedom                                30\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  CREA =~                                                               \n    CREAT             1.000                               4.200    1.000\n  SEFF =~                                                               \n    SEFF1             1.000                               0.537    0.846\n    SEFF2             0.474    0.074    6.421    0.000    0.255    0.417\n    SEFF3             0.735    0.091    8.116    0.000    0.395    0.515\n    SEFF4             0.834    0.070   11.981    0.000    0.448    0.716\n  PER =~                                                                \n    PER1              1.000                               2.274    0.828\n    PER2              1.009    0.064   15.877    0.000    2.294    0.866\n    PER3              0.875    0.062   14.129    0.000    1.989    0.792\n  SOC =~                                                                \n    SOC1              1.000                               1.478    0.700\n    SOC2              1.064    0.149    7.148    0.000    1.573    0.913\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  CREA ~~                                                               \n    SEFF              1.861    0.205    9.093    0.000    0.826    0.826\n    PER               7.467    0.852    8.761    0.000    0.782    0.782\n    SOC               2.575    0.546    4.717    0.000    0.415    0.415\n  SEFF ~~                                                               \n    PER               0.985    0.121    8.145    0.000    0.807    0.807\n    SOC               0.342    0.076    4.485    0.000    0.432    0.432\n  PER ~~                                                                \n    SOC               1.787    0.350    5.114    0.000    0.532    0.532\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .CREAT             0.000                               0.000    0.000\n   .SEFF1             0.114    0.018    6.519    0.000    0.114    0.284\n   .SEFF2             0.308    0.028   10.834    0.000    0.308    0.826\n   .SEFF3             0.431    0.041   10.582    0.000    0.431    0.734\n   .SEFF4             0.191    0.020    9.385    0.000    0.191    0.488\n   .PER1              2.365    0.283    8.355    0.000    2.365    0.314\n   .PER2              1.758    0.241    7.302    0.000    1.758    0.250\n   .PER3              2.354    0.261    9.033    0.000    2.354    0.373\n   .SOC1              2.279    0.339    6.714    0.000    2.279    0.511\n   .SOC2              0.495    0.311    1.592    0.111    0.495    0.167\n    CREA             17.640    1.578   11.180    0.000    1.000    1.000\n    SEFF              0.288    0.037    7.721    0.000    1.000    1.000\n    PER               5.170    0.667    7.751    0.000    1.000    1.000\n    SOC               2.185    0.438    4.992    0.000    1.000    1.000\n\nfitmeasures(cfa_fit)[c('chisq', 'df', 'pvalue', 'nfi', 'nnfi', 'cfi', 'rmsea', 'srmr')] |&gt; round(3)\n\n chisq     df pvalue    nfi   nnfi    cfi  rmsea   srmr \n80.964 30.000  0.000  0.936  0.937  0.958  0.082  0.042 \n\n\n\n\n结构模型分析阶段\n将测量模型与结构模型组合在一起就构成了统合模型。测量模型不变，按照研究假设设定结构模型，进行分析并报告结果。\n\nsem_mod&lt;-'\n# 测量模型\n    CREA =~ 1*CREAT\n    SEFF =~ SEFF1 + SEFF2 + SEFF3 + SEFF4\n    PER  =~ PER1  + PER2  + PER3\n    SOC  =~ SOC1  + SOC2\n# 结构模型\n    SEFF ~ a1*PER + c1*SOC\n    CREA ~ a2*PER + b1*SEFF + c2*SOC\n# 设定外生潜变量之间的协方差\n    SOC  ~~ PER \n'\nsem_fit &lt;- sem(model = sem_mod, sample.cov = COV, sample.nobs = 250)\nsummary(sem_fit, standard=T)\n\nlavaan 0.6-19 ended normally after 81 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        25\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                                80.964\n  Degrees of freedom                                30\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  CREA =~                                                               \n    CREAT             1.000                               4.200    1.000\n  SEFF =~                                                               \n    SEFF1             1.000                               0.537    0.846\n    SEFF2             0.474    0.074    6.421    0.000    0.255    0.417\n    SEFF3             0.735    0.091    8.116    0.000    0.395    0.515\n    SEFF4             0.834    0.070   11.981    0.000    0.448    0.716\n  PER =~                                                                \n    PER1              1.000                               2.274    0.828\n    PER2              1.009    0.064   15.877    0.000    2.294    0.866\n    PER3              0.875    0.062   14.129    0.000    1.989    0.792\n  SOC =~                                                                \n    SOC1              1.000                               1.478    0.700\n    SOC2              1.064    0.149    7.148    0.000    1.573    0.913\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SEFF ~                                                                \n    PER       (a1)    0.190    0.019    9.944    0.000    0.805    0.805\n    SOC       (c1)    0.001    0.026    0.046    0.964    0.003    0.003\n  CREA ~                                                                \n    PER       (a2)    0.616    0.189    3.266    0.001    0.334    0.334\n    SEFF      (b1)    4.364    0.807    5.407    0.000    0.558    0.558\n    SOC       (c2)   -0.010    0.147   -0.065    0.948   -0.003   -0.003\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  PER ~~                                                                \n    SOC               1.787    0.350    5.114    0.000    0.532    0.532\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .CREAT             0.000                               0.000    0.000\n   .SEFF1             0.114    0.018    6.519    0.000    0.114    0.284\n   .SEFF2             0.308    0.028   10.834    0.000    0.308    0.826\n   .SEFF3             0.431    0.041   10.582    0.000    0.431    0.734\n   .SEFF4             0.191    0.020    9.385    0.000    0.191    0.488\n   .PER1              2.365    0.283    8.355    0.000    2.365    0.314\n   .PER2              1.758    0.241    7.302    0.000    1.758    0.250\n   .PER3              2.354    0.261    9.033    0.000    2.354    0.373\n   .SOC1              2.279    0.339    6.714    0.000    2.279    0.511\n   .SOC2              0.495    0.311    1.592    0.111    0.495    0.167\n   .CREA              4.941    0.600    8.238    0.000    0.280    0.280\n   .SEFF              0.100    0.020    4.954    0.000    0.349    0.349\n    PER               5.170    0.667    7.751    0.000    1.000    1.000\n    SOC               2.185    0.438    4.992    0.000    1.000    1.000\n\n\n各因子载荷均达到显著性水平，说明测量模型理想；结构模型部分，性格对自我效能感、自我效能感对创新行性格对创新行为的3个回归系数显著，社会化对自我效能感和创新行为的2个回归系数均不显著。两个外生潜变量性格和社会化的协方差和2个方差都是显著的。\n模型拟合情况如下，可以看到，模型的拟合程度还是不错的，但仍有修正空间。\n\nfitmeasures(sem_fit)[c('chisq', 'df', 'pvalue', 'nfi', 'nnfi', 'cfi', 'rmsea', 'srmr')] |&gt; round(3) # 查看拟合结果\n\n chisq     df pvalue    nfi   nnfi    cfi  rmsea   srmr \n80.964 30.000  0.000  0.936  0.937  0.958  0.082  0.042 \n\nsemPaths(sem_fit, \n         what = \"std\", \n         fade = F, \n         rotation = 2,\n         layout = \"tree2\",\n         edge.color = \"darkgrey\",\n         esize = 3,\n         edge.label.cex = 1.1,\n         ask = FALSE)\n\n\n\n\n\n\n\n\n\n\n模型调整\n\nmodificationindices(sem_fit, sort. = T, maximum.number = 5)\n\n     lhs op   rhs     mi    epc sepc.lv sepc.all sepc.nox\n47   PER =~ SEFF1 13.127  0.154   0.351    0.553    0.553\n64 CREAT ~~ SEFF4 13.032  0.329   0.329       NA       NA\n34  CREA =~ SEFF4 12.759  0.073   0.308    0.492    0.492\n72 SEFF1 ~~ SEFF4 11.869 -0.068  -0.068   -0.459   -0.459\n80 SEFF2 ~~  PER1  9.985 -0.196  -0.196   -0.230   -0.230\n\n\n模型修饰指数(MI)最高的五项推荐了5条修改路径。第一条是外生潜变量性格（PER）影响自我效能的内生测量变量自我肯定（SEFF1）的因子载荷（MI=13.13）；第二条是内生测量变量创新行为（CREAT）和压力抗衡（SEFF4）的测量残差的相关性；第三条是内生潜变量创新行为（CREA）影响自我效能的测量变量压力抗衡（SEFF4）的因子载荷（MI=13.03）；第四条是测量变量自我肯定（SEFF1）和压力抗衡（SEFF4）的测量误差的相关性；第五条是测量变量（SEFF2）与（PER1）之间的测量残差的相关性。首先模型修正以结构参数和测量因子载荷参数优先，因为这些参数涉及模型的潜变量的测量和研究假设，残差的考虑与观察不到的因素和隐含条件相关，因此不优先考虑对其调整。因此重点关注第一条和第三条建议路径。同时潜变量与测量变量之间的关系，也优先考虑内生潜变量对内生测量变量的影响或外生潜变量对外生测量变量的影响，在测量模型层面考虑内外生变量的交叉影响会使得模型过于复杂，也很难有理论的支撑。因此，选择第三条路径进行修正，这条路径也有比较合理的解释，即创新行为（CREA）影响压力抗衡（SEFF4）的因子载荷，意味当教师表现出创新行为后，可能增强其抗压的信念，因此可以将此参数纳入模型设定中。\n\nmod1_mod &lt;- '\n# 测量模型\n    CREA =~ CREAT + SEFF4 # 修饰模型，添加CREA影响SEFF4的路径\n    SEFF =~ SEFF1 + SEFF2 + SEFF3 + SEFF4\n    PER  =~ PER1  + PER2  + PER3\n    SOC  =~ SOC1  + SOC2\n    CREAT ~~ 0 * CREAT  # 固定内生测量变量CREAT的残差为0\n# 结构模型\n    SEFF ~ a1*PER + c1*SOC\n    CREA ~ a2*PER + b1*SEFF + c2*SOC\n# 设定外源潜在变量间的协方差\n    SOC  ~~ PER \n'\n\nmod1_fit &lt;- sem(model = mod1_mod, sample.cov = COV, sample.nobs = 250)\nsummary(mod1_fit, standard=T)\n\nlavaan 0.6-19 ended normally after 94 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        26\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                                67.863\n  Degrees of freedom                                29\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  CREA =~                                                               \n    CREAT             1.000                               4.200    1.000\n    SEFF4             0.058    0.013    4.614    0.000    0.243    0.389\n  SEFF =~                                                               \n    SEFF1             1.000                               0.584    0.921\n    SEFF2             0.432    0.069    6.290    0.000    0.253    0.414\n    SEFF3             0.665    0.086    7.773    0.000    0.389    0.507\n    SEFF4             0.361    0.101    3.589    0.000    0.211    0.337\n  PER =~                                                                \n    PER1              1.000                               2.280    0.831\n    PER2              1.003    0.063   15.965    0.000    2.288    0.864\n    PER3              0.872    0.061   14.206    0.000    1.988    0.791\n  SOC =~                                                                \n    SOC1              1.000                               1.484    0.702\n    SOC2              1.056    0.147    7.173    0.000    1.567    0.909\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SEFF ~                                                                \n    PER       (a1)    0.205    0.020   10.479    0.000    0.801    0.801\n    SOC       (c1)   -0.017    0.027   -0.618    0.537   -0.043   -0.043\n  CREA ~                                                                \n    PER       (a2)    0.916    0.190    4.827    0.000    0.497    0.497\n    SEFF      (b1)    2.561    0.713    3.590    0.000    0.356    0.356\n    SOC       (c2)    0.039    0.154    0.255    0.798    0.014    0.014\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  PER ~~                                                                \n    SOC               1.807    0.351    5.145    0.000    0.534    0.534\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .CREAT             0.000                               0.000    0.000\n   .SEFF4             0.211    0.020   10.758    0.000    0.211    0.539\n   .SEFF1             0.061    0.025    2.483    0.013    0.061    0.151\n   .SEFF2             0.309    0.028   10.864    0.000    0.309    0.829\n   .SEFF3             0.436    0.041   10.613    0.000    0.436    0.743\n   .PER1              2.334    0.279    8.355    0.000    2.334    0.310\n   .PER2              1.783    0.240    7.443    0.000    1.783    0.254\n   .PER3              2.357    0.260    9.079    0.000    2.357    0.374\n   .SOC1              2.261    0.340    6.657    0.000    2.261    0.507\n   .SOC2              0.515    0.308    1.672    0.094    0.515    0.173\n   .CREA              5.970    0.640    9.330    0.000    0.338    0.338\n   .SEFF              0.134    0.029    4.631    0.000    0.393    0.393\n    PER               5.200    0.667    7.795    0.000    1.000    1.000\n    SOC               2.203    0.439    5.014    0.000    1.000    1.000\n\nfitmeasures(mod1_fit)[c('chisq', 'df', 'pvalue', 'nfi', 'nnfi', 'cfi', 'rmsea', 'srmr')] |&gt; round(3) # 查看拟合结果\n\n chisq     df pvalue    nfi   nnfi    cfi  rmsea   srmr \n67.863 29.000  0.000  0.946  0.950  0.968  0.073  0.041 \n\n\n新添加的标准化因子载荷为0.389，达到显著水平。整体模型拟合度也有改善，卡方值为67.86，RMSEA为0.073。再检查MI指数，新的路径建议第四项SEFF3与SEFF4的残差相关还需要添加。其余建议路径都是内生测量变量与外生测量变量残差之间的相关性，建议路径太复杂不考虑。\n\nmodificationindices(mod1_fit, sort. = T, maximum.number = 5)\n\n     lhs op   rhs     mi    epc sepc.lv sepc.all sepc.nox\n84 SEFF1 ~~  SOC2 10.929 -0.128  -0.128   -0.723   -0.723\n86 SEFF2 ~~  PER1 10.650 -0.201  -0.201   -0.237   -0.237\n76 SEFF4 ~~  SOC1  9.449 -0.146  -0.146   -0.212   -0.212\n72 SEFF4 ~~ SEFF3  8.778  0.060   0.060    0.199    0.199\n90 SEFF2 ~~  SOC2  8.058  0.127   0.127    0.319    0.319\n\n\n添加残差相关路径后，设定模型进行分析。\n\nmod2_mod &lt;- '\n# 测量模型\n    CREA =~ CREAT + SEFF4 # 修饰模型，添加CREA影响SEFF4的路径\n    SEFF =~ SEFF1 + SEFF2 + SEFF3 + SEFF4\n    PER  =~ PER1  + PER2  + PER3\n    SOC  =~ SOC1  + SOC2\n    CREAT ~~ 0 * CREAT \n# 结构模型\n    SEFF ~ a1*PER + c1*SOC\n    CREA ~ a2*PER + b1*SEFF + c2*SOC\n# 设定外源潜在变量间的协方差\n    SOC  ~~ PER \n    SEFF3 ~~ SEFF4 # 添加内生测量变量的相关性\n# 定义间接效应 (a*b)\n    PERindiCREA := a1*b1\n    SOCindiCREA := c1*b1\n# 定义总效应\n    PERtotCREA := PERindiCREA + a2\n    SOCtotCREA := SOCindiCREA + c2\n'\n\nmod2_fit &lt;- sem(model = mod2_mod, sample.cov = COV, sample.nobs = 250)\n\nfitmeasures(mod2_fit)[c('chisq', 'df', 'pvalue', 'nfi', 'nnfi', 'cfi', 'rmsea', 'srmr')] |&gt; round(3) # 查看拟合结果\n\n chisq     df pvalue    nfi   nnfi    cfi  rmsea   srmr \n59.089 28.000  0.001  0.953  0.959  0.974  0.067  0.039 \n\nmodificationindices(mod2_fit, sort. = T, maximum.number = 5)\n\n     lhs op   rhs     mi    epc sepc.lv sepc.all sepc.nox\n90 SEFF2 ~~  PER1 10.595 -0.201  -0.201   -0.236   -0.236\n80 SEFF4 ~~  SOC1  9.903 -0.147  -0.147   -0.211   -0.211\n88 SEFF1 ~~  SOC2  9.604 -0.123  -0.123   -0.807   -0.807\n60   SOC =~ SEFF1  9.189 -0.129  -0.191   -0.301   -0.301\n94 SEFF2 ~~  SOC2  8.452  0.130   0.130    0.327    0.327\n\n\n模型拟合优度有所提升，卡方值减少到59.09，RMSEA下降到0.067。MI指数检验，只剩内生与外生测量变量的相关性路径，不考虑复杂关系，没有需要调整的路径。同时，由于最终模型和原模型有嵌套关系，因此可以进行卡方检验，显示模型具有显著改进，因此模型调整结束。\n\nanova(mod2_fit, sem_fit)\n\n\nChi-Squared Difference Test\n\n         Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nmod2_fit 28 7919.1 8014.2 59.089                                          \nsem_fit  30 7937.0 8025.0 80.964     21.875 0.19937       2  1.778e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n报告分析结果\n\n报告标准化系数：包括标准化的因子载荷、结构模型的标准化系数等。\n\n\nsummary(mod2_fit, standard=T)\n\nlavaan 0.6-19 ended normally after 235 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        27\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                                59.089\n  Degrees of freedom                                28\n  P-value (Chi-square)                           0.001\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  CREA =~                                                               \n    CREAT             1.000                               4.200    1.000\n    SEFF4             0.062    0.011    5.502    0.000    0.262    0.420\n  SEFF =~                                                               \n    SEFF1             1.000                               0.597    0.942\n    SEFF2             0.414    0.068    6.122    0.000    0.247    0.405\n    SEFF3             0.631    0.085    7.403    0.000    0.377    0.492\n    SEFF4             0.311    0.090    3.470    0.001    0.186    0.297\n  PER =~                                                                \n    PER1              1.000                               2.283    0.832\n    PER2              1.001    0.063   15.987    0.000    2.286    0.863\n    PER3              0.871    0.061   14.233    0.000    1.988    0.791\n  SOC =~                                                                \n    SOC1              1.000                               1.482    0.701\n    SOC2              1.059    0.148    7.166    0.000    1.569    0.911\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SEFF ~                                                                \n    PER       (a1)    0.209    0.020   10.678    0.000    0.799    0.799\n    SOC       (c1)   -0.023    0.027   -0.847    0.397   -0.057   -0.057\n  CREA ~                                                                \n    PER       (a2)    0.960    0.186    5.148    0.000    0.522    0.522\n    SEFF      (b1)    2.301    0.672    3.423    0.001    0.327    0.327\n    SOC       (c2)    0.047    0.155    0.306    0.759    0.017    0.017\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  PER ~~                                                                \n    SOC               1.805    0.351    5.139    0.000    0.534    0.534\n .SEFF4 ~~                                                              \n   .SEFF3             0.060    0.021    2.864    0.004    0.060    0.193\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .CREAT             0.000                               0.000    0.000\n   .SEFF4             0.215    0.020   10.911    0.000    0.215    0.552\n   .SEFF1             0.046    0.027    1.676    0.094    0.046    0.113\n   .SEFF2             0.311    0.029   10.905    0.000    0.311    0.836\n   .SEFF3             0.445    0.042   10.669    0.000    0.445    0.758\n   .PER1              2.323    0.278    8.348    0.000    2.323    0.308\n   .PER2              1.794    0.240    7.490    0.000    1.794    0.256\n   .PER3              2.357    0.259    9.089    0.000    2.357    0.374\n   .SOC1              2.268    0.340    6.679    0.000    2.268    0.508\n   .SOC2              0.508    0.309    1.644    0.100    0.508    0.171\n   .CREA              6.082    0.642    9.477    0.000    0.345    0.345\n   .SEFF              0.145    0.032    4.555    0.000    0.407    0.407\n    PER               5.212    0.667    7.811    0.000    1.000    1.000\n    SOC               2.196    0.439    5.007    0.000    1.000    1.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    PERindiCREA       0.481    0.142    3.396    0.001    0.262    0.262\n    SOCindiCREA      -0.053    0.064   -0.835    0.404   -0.019   -0.019\n    PERtotCREA        1.441    0.123   11.681    0.000    0.783    0.783\n    SOCtotCREA       -0.006    0.168   -0.034    0.973   -0.002   -0.002\n\n\n\n报告直接效应和间接效应\n\n展示创造性格（PER）和组织社会化（SOC），对于创意教学行为（CREA）的间接效应和总效应量及其统计检验。\n\nsummary(mod2_fit, standard = T)$pe |&gt;\n  filter(op == \":=\") |&gt;\n  select(label, est, se, z, pvalue, std.all) |&gt;\n  mutate(across(where(is.numeric), ~round(., 3))) |&gt;\n  arrange(label) |&gt;\n  knitr::kable(caption = \"潜在变量路径分析的部分复杂效应量\")\n\n\n潜在变量路径分析的部分复杂效应量\n\n\nlabel\nest\nse\nz\npvalue\nstd.all\n\n\n\n\nPERindiCREA\n0.481\n0.142\n3.396\n0.001\n0.262\n\n\nPERtotCREA\n1.441\n0.123\n11.681\n0.000\n0.783\n\n\nSOCindiCREA\n-0.053\n0.064\n-0.835\n0.404\n-0.019\n\n\nSOCtotCREA\n-0.006\n0.168\n-0.034\n0.973\n-0.002\n\n\n\n\n\n\n报告路径图\n\n\nsemPaths(mod2_fit, \n         what = \"std\", \n         fade = F, # 关闭路径颜色渐变\n         rotation = 2,\n         layout = \"tree2\",\n         edge.color = \"darkgrey\",\n         esize = 3,\n         edge.label.cex = 1.1,\n         ask = FALSE)\n\n\n\n\n\n\n\n\n参考文献：《结构方程与建模的原理与应用》，邱皓政、林碧芳著，中国轻工业出版社，2009."
  },
  {
    "objectID": "dataanlyr/lecture8/rapplication.html",
    "href": "dataanlyr/lecture8/rapplication.html",
    "title": "网页爬取与机器学习",
    "section": "",
    "text": "R语言可以承担一些简单的网页爬取工作，并且可以很方便的进行分析，但是具有反爬虫机制的动态网页还需更专业的爬虫工具，例如Python等。\n\n\n以厦门市生态环境局行政执法栏目为例说明静态网页爬取过程。\n爬取网页数据，首先需要确定网页数据的位置。通常可以采用CSS selector he和Xpath。CSS selector通过数据所在对象的样式和模式指定位置，参见CSS Selector的语法；而XPath使用路径表达式来选取网页文档中的节点，可以参考XPath的语法。可以通过Chrome浏览器的开发者检查元素功能，找到数据所在的页面的对象，帮助获取样式或模式信息，确定对象的位置。示例网站处罚书来自类别（class）名为gl_list1的div的对象，并且依次向下有ul、li和a三个对象层级，a链接对象中是处罚书的名称等内容。通过rvest包中的read_html()、html_nodes()、html_text()、html_attr()函数可以实现获取html文本、网页对象节点、网页对象内容、网页对象属性内容等功能。\n\nlibrary(rvest)\n\n\nmyurl = \"https://sthjj.xm.gov.cn/zwgk/gsgk/xzcf/\"\n\nweb&lt;-read_html(myurl, encoding=\"UTF-8\") #获取html网页文本\n\npunishcompany &lt;-  web %&gt;% html_nodes(\"div.gl_list1 ul li a\") |&gt; html_text()  # 获取网页数据处罚书\n\npunishdate &lt;-  web %&gt;% html_nodes(\"div.gl_list1 ul li span\") |&gt; html_text()  # 获取处罚时间\n\npunishlink &lt;- web %&gt;% html_nodes(\"div.gl_list1 ul li a\") |&gt; html_attr(name = \"href\")  # 获取处罚书链接\n\npunish &lt;- data.frame(company=punishcompany, date=punishdate, link=punishlink) # 将数据保存到数据框\n\n可以通过循环，快速爬取多个类似结构的静态页面\n\nfor(i in 1:10){\n  url &lt;- paste(myurl, \"index_\", i, \".htm\", sep = \"\")\n  web&lt;-read_html(url,encoding=\"UTF-8\")\n  punishcompany &lt;-  web %&gt;% html_nodes(\"div.gl_list1 ul li a\") |&gt; html_text() \n  punishdate &lt;-  web %&gt;% html_nodes(\"div.gl_list1 ul li span\") |&gt; html_text() \n  punishlink &lt;- web %&gt;% html_nodes(\"div.gl_list1 ul li a\") |&gt; html_attr(name = \"href\") \n\n  punish1 &lt;- data.frame(company=punishcompany, date=punishdate, link=punishlink)\n  punish &lt;- punish %&gt;% rbind(punish1)\n}\n\nhead(punish)  # 展示获取的处罚书数据\n\n                                     company       date\n1 厦门市腾盛兴电子技术有限公司行政处罚决定书 2024-11-26\n2 厦门市玖玖机动车检测有限公司行政处罚决定书 2024-11-13\n3 解除扣押决定书厦门市腾盛兴电子技术有限公司 2024-10-16\n4                       张帅涛行政处罚决定书 2024-10-11\n5         厦门日上钢圈有限公司行政处罚决定书 2024-10-11\n6   厦门钟利机动车检测有限公司行政处罚决定书 2024-09-29\n                            link\n1 ./202411/t20241126_2903149.htm\n2 ./202411/t20241113_2900716.htm\n3 ./202411/t20241129_2903969.htm\n4 ./202410/t20241011_2894766.htm\n5 ./202410/t20241011_2894762.htm\n6 ./202409/t20240929_2892737.htm\n\ntail(punish)  # 确认是否已完整获取数据\n\n                                                                       company\n193 厦门文仪电脑材料有限公司责令改正违法行为决定书 闽厦（执法）环改〔2022〕1号\n194                                责令改正违法行为决定书 厦门日上金属有限公司\n195               厦门恒兴兴业机械有限公司行政处罚决定书 闽厦环罚〔2021〕363号\n196                  厦门海湾化工有限公司行政处罚决定书  闽厦环罚〔2021〕345号\n197                   厦门海湾化工有限公司行政处罚决定书 闽厦环罚〔2021〕344号\n198  厦门三荣陶瓷开发有限公司不予行政处罚决定书  厦环（执法）不罚决字[2021]1号\n          date                           link\n193 2022-01-10 ./202201/t20220110_2616014.htm\n194 2022-01-05 ./202305/t20230526_2761318.htm\n195 2021-12-31 ./202201/t20220110_2616044.htm\n196 2021-12-15 ./202112/t20211215_2608718.htm\n197 2021-12-15 ./202112/t20211215_2608715.htm\n198 2021-12-10 ./202112/t20211215_2608668.htm\n\n\n\n\n\n动态页面与静态页面不同，一般通过与用户建立对话（session）获取用户的数据请求，采用Javascript等程序语言建立定制页面返回数据。因此采用静态页面的方法是无法定位数据的。下面以厦门市生态环境局的咨询投诉栏目为例，介绍动态页面的爬取。\n\nmyurl = \"https://sthjj.xm.gov.cn/gzcy/wyzx/\"\n\nweb&lt;-read_html(myurl, encoding=\"UTF-8\")\n\nweb %&gt;% html_nodes(\"div.gl_list2 ul li a\") \n\n{xml_nodeset (1)}\n[1] &lt;a ms-attr-href=\"'./index_18763.htm?id='+el.letterId+'&amp;chnlId='+el.ch ...\n\nweb %&gt;% html_nodes(\"div.gl_list2 ul li a\") |&gt; html_text()\n\n[1] \"\"\n\n\n动态页面可以先通过会话请求页面，在爬取数据后，可以模拟点击下一页，此时会话会指向下一页，便可以爬取下一页的数据，依此直到爬取结束。\n\nlibrary(chromote)\n\nsess &lt;- read_html_live(myurl) # 请求动态页面\n\n#sess$view()\n\nconsultdata &lt;- NULL\ni=1\nwhile(i&lt;10){\n  consult &lt;-  sess %&gt;% html_elements(\"div.gl_list2 ul li a\") |&gt; html_text()   # 获取相关数据\n  date &lt;-  sess %&gt;% html_elements(\"div.gl_list2 ul li font\") |&gt; html_text() \n  link &lt;- sess %&gt;% html_elements(\"div.gl_list2 ul li a\") |&gt; html_attr(name = \"href\") \n  if(length(consult)==0| !(length(consult)==length(date)&length(consult)==length(link))) next \n  consulttemp &lt;- data.frame(consult=consult, date=date, link=link)\n  consultdata &lt;- consultdata %&gt;% rbind(consulttemp)     # 保存到数据框\n  \n  if(nrow(consultdata) == 200) break       # 如果完成爬取，退出\n  sess$click(\"a.next.b-free-read-leaf\")     # 模拟页面点击下一页\n  i=i+1\n}\n\nhead(consultdata)  \ntail(consultdata)  # 检查数据"
  },
  {
    "objectID": "dataanlyr/lecture8/rapplication.html#网页爬取",
    "href": "dataanlyr/lecture8/rapplication.html#网页爬取",
    "title": "网页爬取与机器学习",
    "section": "",
    "text": "R语言可以承担一些简单的网页爬取工作，并且可以很方便的进行分析，但是具有反爬虫机制的动态网页还需更专业的爬虫工具，例如Python等。\n\n\n以厦门市生态环境局行政执法栏目为例说明静态网页爬取过程。\n爬取网页数据，首先需要确定网页数据的位置。通常可以采用CSS selector he和Xpath。CSS selector通过数据所在对象的样式和模式指定位置，参见CSS Selector的语法；而XPath使用路径表达式来选取网页文档中的节点，可以参考XPath的语法。可以通过Chrome浏览器的开发者检查元素功能，找到数据所在的页面的对象，帮助获取样式或模式信息，确定对象的位置。示例网站处罚书来自类别（class）名为gl_list1的div的对象，并且依次向下有ul、li和a三个对象层级，a链接对象中是处罚书的名称等内容。通过rvest包中的read_html()、html_nodes()、html_text()、html_attr()函数可以实现获取html文本、网页对象节点、网页对象内容、网页对象属性内容等功能。\n\nlibrary(rvest)\n\n\nmyurl = \"https://sthjj.xm.gov.cn/zwgk/gsgk/xzcf/\"\n\nweb&lt;-read_html(myurl, encoding=\"UTF-8\") #获取html网页文本\n\npunishcompany &lt;-  web %&gt;% html_nodes(\"div.gl_list1 ul li a\") |&gt; html_text()  # 获取网页数据处罚书\n\npunishdate &lt;-  web %&gt;% html_nodes(\"div.gl_list1 ul li span\") |&gt; html_text()  # 获取处罚时间\n\npunishlink &lt;- web %&gt;% html_nodes(\"div.gl_list1 ul li a\") |&gt; html_attr(name = \"href\")  # 获取处罚书链接\n\npunish &lt;- data.frame(company=punishcompany, date=punishdate, link=punishlink) # 将数据保存到数据框\n\n可以通过循环，快速爬取多个类似结构的静态页面\n\nfor(i in 1:10){\n  url &lt;- paste(myurl, \"index_\", i, \".htm\", sep = \"\")\n  web&lt;-read_html(url,encoding=\"UTF-8\")\n  punishcompany &lt;-  web %&gt;% html_nodes(\"div.gl_list1 ul li a\") |&gt; html_text() \n  punishdate &lt;-  web %&gt;% html_nodes(\"div.gl_list1 ul li span\") |&gt; html_text() \n  punishlink &lt;- web %&gt;% html_nodes(\"div.gl_list1 ul li a\") |&gt; html_attr(name = \"href\") \n\n  punish1 &lt;- data.frame(company=punishcompany, date=punishdate, link=punishlink)\n  punish &lt;- punish %&gt;% rbind(punish1)\n}\n\nhead(punish)  # 展示获取的处罚书数据\n\n                                     company       date\n1 厦门市腾盛兴电子技术有限公司行政处罚决定书 2024-11-26\n2 厦门市玖玖机动车检测有限公司行政处罚决定书 2024-11-13\n3 解除扣押决定书厦门市腾盛兴电子技术有限公司 2024-10-16\n4                       张帅涛行政处罚决定书 2024-10-11\n5         厦门日上钢圈有限公司行政处罚决定书 2024-10-11\n6   厦门钟利机动车检测有限公司行政处罚决定书 2024-09-29\n                            link\n1 ./202411/t20241126_2903149.htm\n2 ./202411/t20241113_2900716.htm\n3 ./202411/t20241129_2903969.htm\n4 ./202410/t20241011_2894766.htm\n5 ./202410/t20241011_2894762.htm\n6 ./202409/t20240929_2892737.htm\n\ntail(punish)  # 确认是否已完整获取数据\n\n                                                                       company\n193 厦门文仪电脑材料有限公司责令改正违法行为决定书 闽厦（执法）环改〔2022〕1号\n194                                责令改正违法行为决定书 厦门日上金属有限公司\n195               厦门恒兴兴业机械有限公司行政处罚决定书 闽厦环罚〔2021〕363号\n196                  厦门海湾化工有限公司行政处罚决定书  闽厦环罚〔2021〕345号\n197                   厦门海湾化工有限公司行政处罚决定书 闽厦环罚〔2021〕344号\n198  厦门三荣陶瓷开发有限公司不予行政处罚决定书  厦环（执法）不罚决字[2021]1号\n          date                           link\n193 2022-01-10 ./202201/t20220110_2616014.htm\n194 2022-01-05 ./202305/t20230526_2761318.htm\n195 2021-12-31 ./202201/t20220110_2616044.htm\n196 2021-12-15 ./202112/t20211215_2608718.htm\n197 2021-12-15 ./202112/t20211215_2608715.htm\n198 2021-12-10 ./202112/t20211215_2608668.htm\n\n\n\n\n\n动态页面与静态页面不同，一般通过与用户建立对话（session）获取用户的数据请求，采用Javascript等程序语言建立定制页面返回数据。因此采用静态页面的方法是无法定位数据的。下面以厦门市生态环境局的咨询投诉栏目为例，介绍动态页面的爬取。\n\nmyurl = \"https://sthjj.xm.gov.cn/gzcy/wyzx/\"\n\nweb&lt;-read_html(myurl, encoding=\"UTF-8\")\n\nweb %&gt;% html_nodes(\"div.gl_list2 ul li a\") \n\n{xml_nodeset (1)}\n[1] &lt;a ms-attr-href=\"'./index_18763.htm?id='+el.letterId+'&amp;chnlId='+el.ch ...\n\nweb %&gt;% html_nodes(\"div.gl_list2 ul li a\") |&gt; html_text()\n\n[1] \"\"\n\n\n动态页面可以先通过会话请求页面，在爬取数据后，可以模拟点击下一页，此时会话会指向下一页，便可以爬取下一页的数据，依此直到爬取结束。\n\nlibrary(chromote)\n\nsess &lt;- read_html_live(myurl) # 请求动态页面\n\n#sess$view()\n\nconsultdata &lt;- NULL\ni=1\nwhile(i&lt;10){\n  consult &lt;-  sess %&gt;% html_elements(\"div.gl_list2 ul li a\") |&gt; html_text()   # 获取相关数据\n  date &lt;-  sess %&gt;% html_elements(\"div.gl_list2 ul li font\") |&gt; html_text() \n  link &lt;- sess %&gt;% html_elements(\"div.gl_list2 ul li a\") |&gt; html_attr(name = \"href\") \n  if(length(consult)==0| !(length(consult)==length(date)&length(consult)==length(link))) next \n  consulttemp &lt;- data.frame(consult=consult, date=date, link=link)\n  consultdata &lt;- consultdata %&gt;% rbind(consulttemp)     # 保存到数据框\n  \n  if(nrow(consultdata) == 200) break       # 如果完成爬取，退出\n  sess$click(\"a.next.b-free-read-leaf\")     # 模拟页面点击下一页\n  i=i+1\n}\n\nhead(consultdata)  \ntail(consultdata)  # 检查数据"
  },
  {
    "objectID": "dataanlyr/lecture8/rapplication.html#文本挖掘",
    "href": "dataanlyr/lecture8/rapplication.html#文本挖掘",
    "title": "网页爬取与机器学习",
    "section": "文本挖掘",
    "text": "文本挖掘\nR语言能够对已获取的文本数据进行挖掘探索，对文本进行分词后，探索主题词的频率，建立文档-词条矩阵，进行文本聚类和有目的的自动分类，进行主题建模和情感分析等。\n\n设置环境\nR语言需要Java支持才能使用文本挖掘相关软件包，首先要根据操作系统下载jdk程序进行安装，然后在rstudio中安装rjava包和Rwordseg包（install.packages(“Rwordseg”, repos=“http://R-Forge.R-project.org”)），中间可能还需要手动安装一些依赖的包。\n\nlibrary(rJava)\nlibrary(Rwordseg)\n\n\n\n文本预处理与分词\n分词是文本分析的基础，可以在分词前给出文本中常见的小地名防止错误分割词汇，还可先剔除掉数字以减少干扰。在文本预处理之后，便可以采用segmentCN函数对数据进行分词，分词器采用“jiebaR”的效果比较好。分词后可以通过设定stopwords将一些没有含义的连词、语气词等剔除。\n\ninsertWords(c(\"思明\",\"湖里\",\"同安\",\"翔安\",\"海沧\",\"集美\",\"杏林\",\"厦门\",\"新垵\"))\nconsulttemp &lt;- gsub(\"[0-9０１２３４５６７８９ &lt; &gt; ~]\",\"\",consultdata$consult)\n\nconsulttemp &lt;- segmentCN(consulttemp, analyzer = \"jiebaR\")\n\nremoveStopWords &lt;- function(x,stopwords) {\nx=x[!( x %in% stopwords)]\nreturn(x)\n}\nstopwords &lt;- stopwordsCN(c(\"了\", \"是\", \"无\", \"请问\", \"及\", \"小\", \"与\", \"如何\", \"不\", \"的\", \"可以\"))\nconsulttemp &lt;-lapply(consulttemp,removeStopWords,stopwords)\n\n\n\n词条排序与词云\n\nwords &lt;- lapply(consulttemp, strsplit, \" \")\nwordsNum &lt;- table(unlist(words))\nwordsNum &lt;- sort(wordsNum, decreasing = T) #排序\nwordsData &lt;- as.data.frame(wordsNum)\n\nlibrary(wordcloud2) #加载画词云的包\n\nconsulttop150 &lt;- head(wordsData,150) #取前150个词\nwordcloud2(consulttop150)"
  },
  {
    "objectID": "dataanlyr/lecture8/rapplication.html#数据挖掘与机器学习简介",
    "href": "dataanlyr/lecture8/rapplication.html#数据挖掘与机器学习简介",
    "title": "网页爬取与机器学习",
    "section": "数据挖掘与机器学习简介",
    "text": "数据挖掘与机器学习简介\n\n为什么要挖掘数据？\n\n商业角度\n\n大量数据被收集和存储\n\n网站数据、电子商务\n\n超市和商店的购物信息\n\n银行账户和信用卡的交易\n\n\n计算机变得越来越便宜，越来越高效\n\n竞争压力越来越大\n\n为客户提供更优质的定制化服务（客户关系管理）\n\n\n\n\n科学角度\n\n收集存储的数据急剧增长(每秒1.7兆字节)\n\n遥感卫星\n\n天文望远镜\n\n基因数据\n\n科学模拟数据\n\n\n传统技术无法处理\n\n数据挖掘能够帮助科学家\n\n进行数据分类和分段\n建构假设\n\n2024年诺贝尔物理学奖获得者约翰·霍普菲尔德和杰弗里·欣顿是两名机器学习领域的元老级人物。 他们使用物理学工具，设计了人工神经网络，为当今强大的机器学习技术奠定了基础。\n2024年诺贝尔化学奖授予戴维·贝克、德米斯·哈萨比斯和约翰·江珀。他们采用人工智能在“计算蛋白质设计”和“蛋白质结构预测”方面成就斐然。这也是继物理学奖之后，诺贝尔奖再次被授予人工智能（AI）的相关成果及科学家。\n\n\n\n\n发掘大数据的动机\n\n数据中经常有一些“隐藏”的信息，很难一目了然的发现\n\n手工分析可能需要好几个星期的时间去发现有用的信息\n\n很多数据根本没有用于分析\n\n\n\n\n什么是数据挖掘？\n\n从数据中提取隐含的、未知的和潜在有用的信息\n\n通过自动或半自动的探索和分析方法在大型数据中发现有意义的模式\u000b\n\n什么不是数据挖掘？\n\n服务数据中查找用户的电话号码\n\n用百度搜索“京东快递”的信息\n\n什么是数据挖掘？\n\n某些姓名在某些地区可能更普遍\n\n依据具体情形对搜索引擎返回的信息进行分组（对京东快递服务的正面和负面评价）\n\n\n\n数据挖掘的起源\n\n来自机器学习、人工智能、模式识别、统计学和数据库方面的思想\n\n传统的技术无能为力\n\n数据量太大\n\n高维度数据\n\n异种数据和复杂数据\n\n\n\n\n数据挖掘的任务\n\n分类 [预测]\n\n聚类 [描述]\n\n关联规则发现 [描述]\n\n序列模式发现 [描述]\n\n回归 [预测]\n\n背离检测 [预测]\n\n\n\n\n\n\n\n\n\n\n\n\n分类\n\n定义\n\n给定一组记录 (训练集)\n\n每条记录包含一组属性，其中一个属性称之为类别.\n\n\n构建一个模型使类别属性成为其他属性值的函数.\n\n目标：尽可能将未见过的记录精确归类.\n\n测试集用于决定模型的精度. 一般将数据分成训练集和测试集，采用训练集构建模型，用测试集去验证。\n\n\n分类的例子：退税、婚姻、收入？逃税\n\n\n\n\n\n\n\n\n\n\n\n分类的应用：直销\n\n目的： 锁定可能购买新产品的消费者减少邮寄成本\n\n方法:\n\n采用已有相似产品的数据.\n\n已知哪些客户决定买，哪些客户决定不买. 购买的决定就是类别属性.\n\n收集这些客户的人口学特征、生活方式等信息.\n\n工作性质、生活区域、收入情况等.\n\n采用这些信息作用输入属性来训练分类器模型.\n\n\n\n\n分类的应用：信用卡欺诈检测\n\n目的：预测信用卡交易中的欺诈.\n\n方法：\n\n采用信用卡持有人的交易信息作用属性.\n\n持卡人什么时候买、买什么、购买频率等\n\n过去的交易是否是欺诈或正常交易，这便是分类属性.\n\n训练交易分类模型.\n\n用模型对某个账户的信用卡交易进行分析检测是否存在欺诈.\n\n\n\n\n\n分类的应用：太空探索分类\n\n目的：预测天体的类别（恒星或星系），特别是针对那些在望远镜上看起来不清楚.\n\n3000 images with 23,040 x 23,040 pixels per image.\n\n\n方法:\n\n图像分段.\n\n测量图像的属性，每个天体有四十个属性.\n\n基于属性构建分类模型.\n\n成功找到16个新的高红移类星体, 难以发现的最远天体!\n\n\n\n\n\n\n\n\n\n\n\n\n\n聚类\n\n定义\n\n有一组数据点，每个点有一组采用相似测量方式的属性，实现如下分类（分簇、分堆）\n\n簇内数据点比其他簇的数据点更相似.\n\n簇间数据点的差别要尽可能大.\n\n\n相似性的测量:\n\n欧氏距离用于连续型属性.\n\n其他基于具体问题的度量方式.\n\n\n聚类示意\n\n\n\n\n\n\n\n\n\n\n\n聚类应用：市场划分\n\n目的：对市场客户进行划分，利于采用不同的营销策略针对不同的客户群体.\n\n方法:\n\n基于客户的地理位置和生活习惯收集不同的信息属性.\n\n发现相似客户的组群.\n\n通过对比簇内客户和簇间客户的购买模式检验聚类的质量.\n\n\n\n\n聚类应用：文档聚类\n\n目的: 根据文档中的主题词的出现情况对文档进行分组.\n\n方法：识别每个文档中频繁出现的主题词.利用不同主题词出现的频率测量文档的相似性，并对其聚类.\n\n应用：信息检索能够利用聚类的信息将新文档或检索词与聚类后文档联系起来.\n\n文本聚类示例\n- 聚类点：洛杉矶时报的3204篇文章.\n- 相似性测量：文档中有多少相同的词.\n\n\n\n\n\n\n\n\n\n股市数据聚类\n\n观察每天的股票动态.\n\n聚类点：股票-{涨/跌}\n\n相似性测量：如果同一天相同的涨跌频繁发生那么两点越相似.\n\n用关联性法则量化相关性.\n\n\n\n\n\n\n\n\n\n\n\n\n\n关联规则发现\n\n定义\n\n给定一组记录，每条记录包含给定集合中的某些项目；\n\n发现能够根据其他项目出现来预测某个项目出现的依赖规则.\n\n\n\n\n\n\n\n\n\n\n\n\n\n关联规则应用：市场营销\n\n假设发现了如下规则 {可乐, … } –&gt; {薯片}\n\n薯片是后发生 =&gt; 能够用于决定如何提升其销量.\n\n可乐是先发生 =&gt; 能够用于了解如果可乐出现断货，会影响哪一类产品的销量.\n\n可乐先发生和薯片后发生 =&gt; 能够用于了解什么产品应该与可乐一起销售以提高薯片的销量！\n\n\n\n关联规则应用：超市货架管理\n\n目标：通过顾客的购买行为了解会被同时购买的商品.\n\n目的：处理扫码后的销售数据发现商品之间的依赖关系.\n\n一条规则 –\n\n如果一位顾客买了尿布和牛奶，他很有可能会买啤酒.\n\n所以超市在尿布货架旁布置啤酒！\n\n\n\n\n关联规则应用：清单管理\n\n目的：电器维修公司想预测维修消费者电器需要的原配件，那么可以事先带上，以减少往返和上门的次数.\n\n方法：对不同区域消费者之前维修所用原配件和工具的数据进行处理，发现共同出现的模式.\n\n\n\n\n回归\n\n在假定线性或非线性依赖关系的前提下，根据其他变量预测给定连续型变量的值.\n\n在统计学和神经网络领域应用最多.\n\n例子：\n\n基于广告费用预测新产品的销售量.\n\n利用温度、湿度和气压等构建函数预测风速.\n\n对股市指标进行时间序列预测.\n\n\n\n\n背离/异常检测\n\n从正常行为中检测显著的异常\n\n应用：\n\n信用卡欺诈检测\n\n网络入侵检测：一般大学每日网络访问量多达1亿个连接\n\n\n\n\n数据挖掘面临的挑战\n\n尺度扩展性\n\n高维度\n\n异种数据和复杂数据\n\n数据质量\n\n数据所有权和分布\n\n隐私保护\n\n实时动态数据流"
  },
  {
    "objectID": "dataanlyr/models/regmodels.html",
    "href": "dataanlyr/models/regmodels.html",
    "title": "回归模型",
    "section": "",
    "text": "本部分的重点包括：\n- 线性回归的假定\n- 探索性数据分析图和汇总统计数据\n- 使用残差诊断回归假设\n- 解释多元线性回归的参数和相关检验和区间\n- 自助法置信区间的原理\n\n\n\n\n\\[\\begin{aligned}\n  y_i & =  X_i \\beta+\\epsilon_i \\\\\n      & =  \\beta_1 X_{i1}+\\cdots + \\beta_k X_{ik} + \\epsilon_i\n\\end{aligned},\\qquad for \\quad i=1,\\cdots , n\\]\n其中， \\(\\epsilon_i\\) 相互独立，服从均值为0，标准差为 \\(\\sigma\\) 正态分布\n或者\n\\[y_i \\sim N(X_i \\beta, \\sigma^2),\\qquad for \\quad i=1,\\cdots , n\\]\n其中， \\(X\\) 是 \\(n\\times k\\) 的预测变量矩阵，第 \\(i\\) 行为 (X_{i})， \\(\\beta\\) 是长度为 \\(k\\) 的列向量\n\n\n\n\n\n\n\n\n\n\n\n\n假定\n表达式\n假设不成立\n解决方案\n\n\n\n\n1 无完全共线性\n\\(rank(X)=k,k&lt;n\\)\n无法识别回归系数\n减少变量或增加样本\n\n\n2 X外生\n\\(E(X\\epsilon )=0\\)\n有偏，且不随N增加而改善\n改变研究设计，因果推断技术\n\n\n3 误差项零均值\n\\(E(\\epsilon)=0\\)\n有偏，且不随N增加而改善\n改变模型设定\n\n\n4 线性关系\n\\(y=\\beta_{1}x_{1}+\\beta_{2} x_{2}+\\cdots+\\epsilon\\)\n模型设定错误\n改变模型设定\n\n\n5 误差项不相关（观测独立）\n\\(E(\\epsilon_{i} \\epsilon_{j})=0, i\\neq j\\)\n无偏但无效，标准误不正确\n多层线性模型\n\n\n6 同方差\n\\(E(\\epsilon^{'}\\epsilon)=\\sigma^{2}\\boldsymbol{I}\\)\n无偏但无效，标准误不正确\n广义线性模型或稳健性标准误\n\n\n7 误差项服从正态分布\n\\(\\epsilon\\sim N(0,\\sigma^{2})\\)\n标准误不正确，但随N增加而改善\n广义线性模型\n\n\n\n\n如果假定1-6都成立，则最小二乘法的估计量是最优线性无偏估计量（best linear unbiased estimator, BLUE）\n如果假定7也成立，则最小二乘法得到参数估计为最小方差无偏的，即在所有线性和非线性估计量中方差最小。\n\n\n\n\n\n\n线性回归最小二乘法的假定\n\n\n\n\n哪些研究设计可能导致违背回归假定？\n\n随机选择驾驶员，改变车内音乐的音量，测试驾驶员在同一路段的反应时间\n\n随机选择学生，通过学习时间预测考试能否通过\n\n随机选择家庭，通过家庭收入预测家庭生育子女的数量\n\n通过父亲的身高，预测他长子的身高\n随机选择稻田，通过降雨量预测水稻产量\n\n随机选择大学生，通过性别、每周锻炼时间预测体重\n\n在10家有多名外科手术医生的医院选择接受某项外科手术的患者，通过患者的年龄预测手术的治疗效果，治疗效果采用1-10分记录。\n\n如果违背正态性假定，那么需要广义线性模型；如果违背独立性假定，那么需要采用分层线性模型。\n\n\n\n\n示例：母亲教育与子女认知能力数据集\n\n\n\nlibrary(\"foreign\")\nlibrary(\"dplyr\")\nlibrary(\"kableExtra\")\nlibrary(gridExtra)\nkidiq &lt;- read.dta(\"kidiq.dta\")\n\nkidiq &lt;- kidiq %&gt;% mutate(hsfactor = ifelse(mom_hs==1, \"高中及以上\", \"高中以下\"))\ntable1 &lt;- kidiq %&gt;%\n  filter(row_number() &lt; 6 | row_number() &gt; 428)\nkable(table1, booktabs=T,caption=\"示例数据概览\") %&gt;%\n  kable_styling(latex_options = \"scale_down\")\n\n\n示例数据概览\n\n\n\nkid_score\nmom_hs\nmom_iq\nmom_work\nmom_age\nhsfactor\n\n\n\n\n1\n65\n1\n121.11753\n4\n27\n高中及以上 |\n\n\n2\n98\n1\n89.36188\n4\n25\n高中及以上 |\n\n\n3\n85\n1\n115.44316\n4\n27\n高中及以上 |\n\n\n4\n83\n1\n99.44964\n3\n25\n高中及以上 |\n\n\n5\n115\n1\n92.74571\n4\n27\n高中及以上 |\n\n\n429\n93\n0\n74.86073\n2\n25\n高中以下 |\n\n\n430\n94\n0\n84.87741\n4\n21\n高中以下 |\n\n\n431\n76\n1\n92.99039\n4\n23\n高中及以上 |\n\n\n432\n50\n0\n94.85971\n2\n24\n高中以下 |\n\n\n433\n88\n1\n96.85662\n2\n21\n高中及以上 |\n\n\n434\n70\n1\n91.25334\n2\n25\n高中及以上 |\n\n\n\n\n\n有哪些变量类型？\n\n\n\n\n\n\n\n\n示例中尺度变量的直方图\n\n\n\n\n这些变量的分布有什么特征？\n\n\n\n\nlibrary(GGally)\ngg &lt;- ggpairs(data = kidiq, \n              columns = c(\"hsfactor\", \"kid_score\", \"mom_iq\", \"mom_age\"))\n gg[4,1] &lt;- gg[4,1] + geom_histogram( binwidth = 5)\n gg[2,1] &lt;- gg[2,1] + geom_histogram( binwidth = 10)\n gg[3,1] &lt;- gg[3,1] + geom_histogram( binwidth = 5)\ngg\n\n\n\n\n探索变量间的关系\n\n\n\n\n对角线上为单变量的分布，箱线图和直方图为按是否上过高中分类的其他尺度变量的分布，散点图为两个尺度变量的相关性，对角线上方为相关系数。\n变量之间的关系是怎样的？\n\n# Coded scatterplot\nggplot(kidiq, aes(x = mom_iq, y = kid_score, colour = hsfactor)) +\n  geom_point(aes(shape = hsfactor)) +\n  geom_smooth(aes(linetype = hsfactor), method = lm, se = FALSE)\n\n\n\n\n子女测试分数的线性趋势\n\n\n\n\n通过探索性分析，可以尝试问这样的问题：\n\n子女测试分数是线性增加的吗？\n\n测试分数增长的快慢受到母亲是否上过高中影响吗？\n\n上过高中的母亲其子女测试分数一定更高吗？\n\n这些关系在统计上是显著的吗？\n\n预测子女测试分数的效果如何？\n\n\n\n\n\n\n\n\nfit1 &lt;- lm (kid_score ~ mom_iq, data = kidiq)\n\nsummary(fit1)\n\n\nCall:\nlm(formula = kid_score ~ mom_iq, data = kidiq)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-56.753 -12.074   2.217  11.710  47.691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 25.79978    5.91741    4.36 1.63e-05 ***\nmom_iq       0.60997    0.05852   10.42  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.27 on 432 degrees of freedom\nMultiple R-squared:  0.201, Adjusted R-squared:  0.1991 \nF-statistic: 108.6 on 1 and 432 DF,  p-value: &lt; 2.2e-16\n\n\n\n系数的解释：保持其他变量不变，预测变量单位变动相应的响应变量变动大小。\n\n但是注意：有时候在保持其他变量不变的情况下是不可能变动某个变量的，例如回归中同时包括IQ和 (IQ^{2})，或者有交互项 mom_hs*mom_i q.\n\n回归模型诊断的常用方法，plot函数会自动绘制4张诊断图\n\n线性假定：残差与拟合值没有系统关联\n\n正态性：正态Q-Q图是与理论正态分布相比，标准化残差的概率图，应落在呈45度角的直线上\n\n同方差性：位置尺度图水平线周围的点应该随机分布\n\n离群点、高杠杆点和强影响点：残差杠杆图采用cook距离来识别对模型参数的估计产生过大影响（高杠杆值）的观测点，一般Cook距离大于 \\(4/(n-k-1)\\)，则表明是强影响点\n\n\n\npar(mfrow=c(2,2))\nplot(fit1)\n\n\n\n\n\n\n\n\n\n比较线性和2次曲线拟合\n\n\n# Fitted models for Model 2 and Model 2Q\nggplot(kidiq, aes(x = mom_iq, y = kid_score)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = y ~ x, \n              se = FALSE, linetype = 1) +\n  stat_smooth(method = \"lm\", formula = y ~ x + I(x^2), \n              se = FALSE, linetype = 2)\n\n\n\n\n比较线性和2次曲线拟合\n\n\n\n\n\n修正模型后的诊断\n\n\nkidiq &lt;- kidiq %&gt;% mutate(mom_iq2 = mom_iq*mom_iq)\nfit2 &lt;- lm (kid_score ~ mom_iq + mom_iq2, data = kidiq)\nsummary(fit2)\n\n\nCall:\nlm(formula = kid_score ~ mom_iq + mom_iq2, data = kidiq)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-54.824 -11.640   2.883  11.372  50.813 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -99.033675  37.301385  -2.655 0.008226 ** \nmom_iq        3.076800   0.730291   4.213 3.07e-05 ***\nmom_iq2      -0.011917   0.003517  -3.389 0.000767 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.05 on 431 degrees of freedom\nMultiple R-squared:  0.2217,    Adjusted R-squared:  0.2181 \nF-statistic: 61.38 on 2 and 431 DF,  p-value: &lt; 2.2e-16\n\npar(mfrow=c(2,2))\nplot(fit2)\n\n\n\n\n\n\n\n\n\n\n\n\nfit3 &lt;- lm(kid_score ~ mom_hs, data = kidiq)\nsummary(fit3)\n\n\nCall:\nlm(formula = kid_score ~ mom_hs, data = kidiq)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-57.55 -13.32   2.68  14.68  58.45 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   77.548      2.059  37.670  &lt; 2e-16 ***\nmom_hs        11.771      2.322   5.069 5.96e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 432 degrees of freedom\nMultiple R-squared:  0.05613,   Adjusted R-squared:  0.05394 \nF-statistic: 25.69 on 1 and 432 DF,  p-value: 5.957e-07\n\n\n此种情况下，回归系统和截距很容易解释。这个回归和两个独立样本t检验一致吗？需要满足等方差的假设吗？\n\n\n\n\nfit4 &lt;- lm(kid_score ~ mom_hs+ mom_iq, data = kidiq)\nsummary(fit4)\n\n\nCall:\nlm(formula = kid_score ~ mom_hs + mom_iq, data = kidiq)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.873 -12.663   2.404  11.356  49.545 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 25.73154    5.87521   4.380 1.49e-05 ***\nmom_hs       5.95012    2.21181   2.690  0.00742 ** \nmom_iq       0.56391    0.06057   9.309  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.14 on 431 degrees of freedom\nMultiple R-squared:  0.2141,    Adjusted R-squared:  0.2105 \nF-statistic: 58.72 on 2 and 431 DF,  p-value: &lt; 2.2e-16\n\n\n\n相对于模型3，估计在考虑母亲智商影响的情况下，是否上过高中对子女测试分数的影响差异。平均而言，上过高中的母亲其子女测试分数高5.95分。上面模型的11.77分是高估了高中对分数的影响。\n\n类似的，在考虑母亲高中的影响的情况下，母亲智商对子女测试分数的影响平均而言是0.56，也小于第1个模型的系数0.61。\n\n模型的拟合优度R方为0.21，远高于只用高中变量的模型拟合优度0.05。\n\n\n\n\n除了了解效应的显著性，还需要采用置信区间量化效应量的不确定性和量化模型预测值的不确定性。\n\nconfint(fit4)\n\n                 2.5 %     97.5 %\n(Intercept) 14.1839148 37.2791615\nmom_hs       1.6028370 10.2973969\nmom_iq       0.4448487  0.6829634\n\nnew.data &lt;- data.frame(mom_iq=190, mom_hs = 0) \npredict(fit4, new = new.data, interval = \"prediction\")\n\n       fit      lwr      upr\n1 132.8737 95.18156 170.5658\n\n\n\n95%的信心，母亲是否上过高中造成子女测试分数的差异在1.6分到10.3分之间。\n\n基于模型4，有95%的信心，如果母亲智商190，但是没有上过高中，则子女的测试分数在95到171之间。\n\n\n\n\n如果回归模型违背了假设，自助法是一种更稳健的统计推断方法。其原理是原始数据样本能够代表总体，所以可以通过从原始样本数据中再抽样，获得多个子样本，从子样本的统计特征了解估计参数的不确定性。\n自助法的步骤如下：\n\n在原始数据中进行重抽样\n\n用新的子样本重新拟合原来的回归模型得到新的回归系数、截距\n\n重复上面的两个步骤足够多的次数（比如1000次）\n\n采用1000次重抽样回归模型的估计量绘制参数自助法分布\n\n通过2.5%和97.5%的百分位数获得每个参数自助法分布的置信区间\n\n\nlibrary(rsample)\nlibrary(tidyverse)\nset.seed(666)\nbootreg &lt;- kidiq %&gt;% \n  bootstraps(1000) %&gt;%\n  pull(splits) %&gt;% \n  map(\\(df) lm(kid_score ~ mom_iq + mom_hs, data = df) ) %&gt;% \n  map(\\(mod) as.data.frame(t(as.matrix(coef(mod))))) %&gt;%\n  list_rbind() %&gt;% pivot_longer(everything(), names_to = \"variable\", values_to = \"value\")\n\nbootregresult &lt;- bootreg %&gt;%\n  group_by(variable) %&gt;% \nsummarise(across( where(is.numeric),  \n          list(low = ~quantile(., probs = 0.025, na.rm = TRUE),  \n               high = ~quantile(., probs = 0.975, na.rm = TRUE)))) \nbootregresult\n\n# A tibble: 3 × 3\n  variable    value_low value_high\n  &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)    14.1       37.5  \n2 mom_hs          1.51      10.5  \n3 mom_iq          0.439      0.681\n\n\n\nggplot(bootreg, aes(x = value)) +\n  geom_histogram(fill = \"steelblue\", color = \"black\", bins = 30) +\n  facet_wrap(~variable, scales = \"free\") \n\n\n\n\n回归系数的自助法分布\n\n\n\n\n结果与正态分布理论假定下的结果相似。此外，还有多种其他计算自助法置信区间的方法，但是百分位法是应用最广泛的。\n\n\n\n\n没有截距的模型（常数项）\n\n\nfit5 &lt;- lm (kid_score ~ mom_hs + mom_iq - 1, data = kidiq)\n\nsummary(fit5)\n\n\nCall:\nlm(formula = kid_score ~ mom_hs + mom_iq - 1, data = kidiq)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-55.348 -10.796   2.187  12.789  50.838 \n\nCoefficients:\n       Estimate Std. Error t value Pr(&gt;|t|)    \nmom_hs  5.99194    2.25786   2.654  0.00825 ** \nmom_iq  0.81524    0.01979  41.189  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.51 on 432 degrees of freedom\nMultiple R-squared:  0.9571,    Adjusted R-squared:  0.9569 \nF-statistic:  4817 on 2 and 432 DF,  p-value: &lt; 2.2e-16\n\n\n\n添加交互项 增加一个交互项以允许IQ的回归系数随高中完成情况不同而变动\n\n\nfit6 &lt;- lm (kid_score ~ mom_hs + mom_iq + mom_hs:mom_iq, data = kidiq)\n\nsummary(fit6)\n\n\nCall:\nlm(formula = kid_score ~ mom_hs + mom_iq + mom_hs:mom_iq, data = kidiq)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.092 -11.332   2.066  11.663  43.880 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -11.4820    13.7580  -0.835 0.404422    \nmom_hs         51.2682    15.3376   3.343 0.000902 ***\nmom_iq          0.9689     0.1483   6.531 1.84e-10 ***\nmom_hs:mom_iq  -0.4843     0.1622  -2.985 0.002994 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.97 on 430 degrees of freedom\nMultiple R-squared:  0.2301,    Adjusted R-squared:  0.2247 \nF-statistic: 42.84 on 3 and 430 DF,  p-value: &lt; 2.2e-16\n\n\n\n解释系数和截距\n\n截距： \\(IQ=0 \\quad hs=0\\)（无意义）\n\nmom_hs的系数： IQ=0时，是否完成高中相应的孩子得分差异\n\nmom_iq的系数： 未完成高中母亲中，IQ增加所相应的孩子得分差异，红线斜率\n\n交互项的系数：母亲是否完成高中两组之间在IQ斜率上的差别\n\n\n回到图4，交互项体现了图中的什么关系？\n\n快速添加交互项\n\n\nfit6 &lt;- lm (kid_score ~ mom_hs * mom_iq, data = kidiq)\n\nsummary(fit6)\n\n\nCall:\nlm(formula = kid_score ~ mom_hs * mom_iq, data = kidiq)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.092 -11.332   2.066  11.663  43.880 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -11.4820    13.7580  -0.835 0.404422    \nmom_hs         51.2682    15.3376   3.343 0.000902 ***\nmom_iq          0.9689     0.1483   6.531 1.84e-10 ***\nmom_hs:mom_iq  -0.4843     0.1622  -2.985 0.002994 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.97 on 430 degrees of freedom\nMultiple R-squared:  0.2301,    Adjusted R-squared:  0.2247 \nF-statistic: 42.84 on 3 and 430 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n最终多元线性回归模型的典型特征包括：\n\n解释变量能够针对主要的研究问题\n\n解释变量控制了重要的协变量\n\n调查了潜在交互作用\n\n中心化变量方便解释\n\n剔除了不必要的变量\n\n使用残差图检验了回归假定和影响点\n\n模型以简洁地方式讲述了一个有说服力的故事\n\n虽然在报告研究结果时，一般要求选择一个合理的最终模型，但是值得注意：\n1 研究者在得出结论时通常会检查并考虑一系列相关的模型\n2 不同的研究者有时会针对同一组数据选择不同的模型作为“最终模型”。“最终模型”的选择取决于许多因素，例如主要研究问题、建模目的、简约性和拟合模型质量之间的权衡、基本假设等。建模决策不应自动化或完全基于统计检验；学科领域知识应始终在建模过程中发挥作用。要能够捍卫所选择的任何最终模型，但不应该感到有压力要找到唯一的“正确模型”，尽管大多数好的模型都会得出类似的结论。\n在比较不同的模型构建时，可以使用的检验方法：\n\nR方能够测量模型解释的响应变量的变异程度。但是会随着增加自变量而增大，即使自变量增加的信息很少。\n\n调整后R方，可以增加模型复杂度的惩罚。\n\nAIC（Akaike Information Criterion, 赤池信息准则）来自信息论， \\(AIC = -2 ( ln ( likelihood )) + 2 K\\) ，其中likelihood为给定模型的条件下数据的概率，k为参数个数。同样是为了平衡模型性能和模型复杂度，无论模型大小，AIC值越小越好。BIC（贝叶斯信息准则）与 AIC 类似，但对附加模型项的惩罚更大。\n\n平方和 F 检验。是对单个模型系数 t 检验的推广，可用于对嵌套模型进行显著性检验，即其中一个模型是另一个模型的简化版本。\n\n一个可能的最终模型如下：\n\nfit7 &lt;- lm (kid_score ~ mom_hs + mom_iq + mom_iq2, data = kidiq)\n\nsummary(fit7)\n\n\nCall:\nlm(formula = kid_score ~ mom_hs + mom_iq + mom_iq2, data = kidiq)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-54.553 -10.940   2.502  11.095  48.710 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -88.541848  37.390233  -2.368 0.018324 *  \nmom_hs        5.107323   2.207015   2.314 0.021131 *  \nmom_iq        2.828771   0.734492   3.851 0.000135 ***\nmom_iq2      -0.010910   0.003526  -3.094 0.002104 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.96 on 430 degrees of freedom\nMultiple R-squared:  0.2313,    Adjusted R-squared:  0.2259 \nF-statistic: 43.12 on 3 and 430 DF,  p-value: &lt; 2.2e-16\n\n\n\n模型的比较\n\n\nanova(fit7, fit4)\n\nAnalysis of Variance Table\n\nModel 1: kid_score ~ mom_hs + mom_iq + mom_iq2\nModel 2: kid_score ~ mom_hs + mom_iq\n  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   \n1    430 138670                                \n2    431 141757 -1     -3087 9.5723 0.002104 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n预期寿命的影响因素\n\n数据来源：罗伯特·J.巴罗，夏威尔·萨拉-伊-马丁《经济增长》所引用的Barro-Lee数据集（我们只使用整理后85年数据)\n135个国家的人均GDP（gdpcap）、25岁以上平均受教育年限（school）、公民自由度（civlib，由低到高1-7分）、近期战争时间比例（wartime）。\n\n先来一个简单的回归，只有教育系数显著，是否符合你的预期？会不会模型设定错误？\n\n\nlife &lt;- read.csv(file = \"./life.csv\")\nlife &lt;- na.omit(life)\nfit &lt;- lm(formula = lifeexp ~ gdpcap + school + civlib + wartime,data = life)\nsummary(fit)\n\n\nCall:\nlm(formula = lifeexp ~ gdpcap + school + civlib + wartime, data = life)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.2871  -3.0705  -0.0368   4.6629  12.1285 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 52.9671697  3.0379356  17.435  &lt; 2e-16 ***\ngdpcap       0.0003216  0.0002551   1.260    0.211    \nschool       2.3866392  0.4102266   5.818 9.01e-08 ***\ncivlib      -0.7522863  0.4861266  -1.548    0.125    \nwartime      1.0936902  5.5354286   0.198    0.844    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.43 on 90 degrees of freedom\nMultiple R-squared:  0.7509,    Adjusted R-squared:  0.7399 \nF-statistic: 67.84 on 4 and 90 DF,  p-value: &lt; 2.2e-16\n\n\n\n我们所看到的效应关系是真实的吗？三个途径去验证\n\n标准误和相关检验：t检验和F检验（见回归输出结果）\n\n残差的模式\n\n\n残差的理想模式：\n\nx &lt;- runif(1000,1,100)\nerror &lt;- rnorm(1000)\ny &lt;- 10+ 3*x +error\nfit0 &lt;- lm(y~x)\nplot(x=x,y=fit0$residuals)\n\n\n\n\n\n\n\n\n存在异方差时的模式：\n\nx &lt;- runif(1000,1,100)\nerror &lt;- rnorm(1000,mean=0,sd=1*x)\ny &lt;- 10+ 3*x +error\nfit0 &lt;- lm(y~x)\nplot(x=x,y=fit0$residuals)\n\n\n\n\n\n\n\n\n\n存在离群值\n加权最小二乘法\n\n存在异方差，则具有较高误差方差的观测含有较少的信息，而具有较低误差方差的观测含有较多的信息。\n\n如果我们能在估计中给予低误差方差观察较高权重，那么能得到更有效的估计。那么相应的最小化误差平方和也变为最小化 加权误差平方和 。\n[{n}{i=1} w_i {i}{2}=]\n\n回归系数加权最小二乘估计：\n[_{WLS}=()^{-1}]\n\n上式其中W为对角线为权重的对角阵，每个权重应该与每个y观测的标准差成反比，即\n[_iN(0,^2_i )^2_i = 1/w^2_i ]\n\n如果我们通过样本性质能够确定权重，lm()函数中可以通过weights参数设定权重。\n\n如果不知道权重，可以利用残差估计权重（残差平方对自变量回归的拟合值的倒数），即可行广义最小二乘法。\n\n通过该方法获取的标准误，即为怀特标准误、稳健标准误、三明治标准误、异方差一致标准误等。\n\n\n\nlibrary(lmtest); library(car)\n\n载入需要的程序包：zoo\n\n\n\n载入程序包：'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n载入需要的程序包：carData\n\n\n\n载入程序包：'car'\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nvc &lt;- hccm(fit, type = \"hc1\")                \nse.corrected &lt;- sqrt(diag(vc))\ncoeftest(fit, vcov=vc)\n\n\nt test of coefficients:\n\n               Estimate  Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 52.96716974  3.29289359 16.0853 &lt; 2.2e-16 ***\ngdpcap       0.00032156  0.00026753  1.2020    0.2325    \nschool       2.38663924  0.42745500  5.5834 2.473e-07 ***\ncivlib      -0.75228629  0.50324260 -1.4949    0.1384    \nwartime      1.09369023  5.30329592  0.2062    0.8371    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n示例的残差图\n\n\nplot(fit$fitted.values,fit$residuals, main=\"life expectancy against fitted Y\")\n\n\n\n\n\n\n\n\n\n有异方差问题吗？还有什么问题？\n\n残差与拟合值相关，意味着什么？\n\n分别检查残差与各个协变量的关系\n\n残差与公民自由度的关系\n\n\n\nplot(life$civlib,fit$residuals)\n\n\n\n\n\n\n\n\n\n残差与战时比例的关系\n\n\nplot(life$wartime,fit$residuals)\n\n\n\n\n\n\n\n\n\n残差与教育的关系\n\n\nplot(life$school,fit$residuals)\n\n\n\n\n\n\n\n\n\n残差与GDP的关系\n\n\nplot(life$gdpcap,fit$residuals)\n\n\n\n\n\n\n\n\n\n示例修正模型设定\n\n公民自由度、战时比例与残差似乎没有明显模式，而教育和GDP和残差有明显的非线性相关关系。\n\n残差中的模式意味着模型设定错误。\n\n如果看到有曲线相关关系，可以考虑添加二次项或进行对话变换。\n\n重新设定模型，将教育和GDP变换为对数项。\n\n\n\nfit &lt;- lm(formula = lifeexp ~ I(log(gdpcap)) + I(log(school)) + civlib + wartime, data=life)\nsummary(fit)\n\n\nCall:\nlm(formula = lifeexp ~ I(log(gdpcap)) + I(log(school)) + civlib + \n    wartime, data = life)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.5038  -1.9584   0.2405   2.0338  11.0816 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     13.1893     5.4439   2.423   0.0174 *  \nI(log(gdpcap))   5.3730     0.6867   7.825 9.35e-12 ***\nI(log(school))   6.0431     0.9040   6.685 1.88e-09 ***\ncivlib          -0.1843     0.3084  -0.598   0.5517    \nwartime         -0.2174     3.6894  -0.059   0.9531    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.627 on 90 degrees of freedom\nMultiple R-squared:  0.8889,    Adjusted R-squared:  0.8839 \nF-statistic:   180 on 4 and 90 DF,  p-value: &lt; 2.2e-16\n\n\n\n再观察残差图\n\n\nplot(fit$fitted.values,fit$residuals, main=\"life expectancy against fitted Y\")\n\n\n\n\n\n\n\n\n\n残差与公民自由度的关系\n\n\nplot(life$civlib,fit$residuals)\n\n\n\n\n\n\n\n\n\n残差与战时比例的关系\n\n\nplot(life$wartime,fit$residuals)\n\n\n\n\n\n\n\n\n\n残差与教育的关系\n\n\nplot(life$school,fit$residuals)\n\n\n\n\n\n\n\n\n\n残差与GDP的关系\n\n\nplot(life$gdpcap,fit$residuals)\n\n\n\n\n\n\n\n\n\n观察残差图，可以看出相关模式已经消失了，但是还有异方差的可能（喇叭形）\n比较稳健性标准误\n\n\nlibrary(orgutils)\nfitsum &lt;- summary(fit)\nvc &lt;- hccm(fit, type = \"hc1\") \nfitout &lt;- data.frame(est=fitsum$coefficients[,1], se=fitsum$coefficients[,2], robust_se=sqrt(diag(vc)))\ntoOrg(round(fitout,digits=2))\n\n| row.names      |   est |   se | robust_se |\n|----------------+-------+------+-----------|\n| (Intercept)    | 13.19 | 5.44 |      5.23 |\n| I(log(gdpcap)) |  5.37 | 0.69 |      0.65 |\n| I(log(school)) |  6.04 |  0.9 |      0.81 |\n| civlib         | -0.18 | 0.31 |      0.28 |\n| wartime        | -0.22 | 3.69 |      3.43 |\n\n\n\n模型的拟合优度\n\n模型解释力的评价\n\nR方\n\n回归标准误\n\n样本外检验\n\n交叉验证\n\n回归标准误\n\n(^2)反映拟合值与真实值之间的平均差距，并且它与y的单位尺度是相同的，非常容易解释。\n\n(^2)越小，说明预测效果越好，(y)越接近真实值。\n\n\n\n\n\n协变量\n(R^2)\n()\n\n\n\n\nGDP, School, Civlib, Wartime\n0.75\n5.43\n\n\nlog(GDP), log(School), Civlib, Wartime\n0.88\n3.63\n\n\n\n\n相同的数据，相同的响应变量\n\n每个值是什么含义？哪个模型好一些？\n样本外拟合优度\n\n假设我们用CGSS2010的数据建立了某个模型，那么我们可能想知道这个模型来预测CGSS2015的数据效果如何？\n\n如果我们用模型拟合样本外数据和训练样本同样好，那么我们更有信心相信我们找到了真实的模型。\n\n样本外检验步骤：\n\n采用训练样本拟合模型，获取回归系数 (_{training})\n\n用测试样本的数据计算拟合值 (y_{test})\n\n比较训练样本的拟合度（例如，({training})）与测试样本的拟合度 (std dev(y{test} - _{test}) ) ，如果两者相等则说明模型预测能力高。\n\n\n交叉验证\n\n如果我们样本量有限，并且很难找到其他的合格样本，可以采取交叉验证。\n\n交叉验证的步骤：\n\n将数据分成k等分，留下一份作为测试集，k-1份作为训练集。特别是Leave-one-out cross validataion (LOOCV) 留一法是比较常用的技术，即将样本量为n的样本分成n等分，只留1个观测作为测试集。\n\n采用训练集来进行回归，获取回归系数。\n\n利用测试集和回归系数来后去测试集预测值\n\n比较测试集预测值与真实值，计算平均预测误差，即由预测值与真实值获取的标准差。\n\n重复1-4步 k 次，计算平均预测误差平方的均值，再取平方根，即得到了 k重交叉验证的预测误差。\n\n\n\n\nlibrary(boot)\n\n\n载入程序包：'boot'\n\n\nThe following object is masked from 'package:car':\n\n    logit\n\n# 注意要采用广义线性回归函数\nfitglm &lt;- glm(lifeexp~gdpcap+school+civlib+wartime, data=life)\n# 5重交叉验证\ncv.err &lt;- cv.glm(life,fitglm,K=5)\n# 报告交叉验证的预测误差：第1个是原始值，第2个是调整值（与留一法比较）\ncv.err$delta\n\n[1] 31.6745 31.2306\n\n\n\n\n\n\n\n\n\n\n\n协变量\n(R^2)\n()\n5重交叉验证误差\n\n\n\n\nGDP, School, Civlib, Wartime\n0.75\n5.43\n32.3\n\n\nlog(GDP), log(School), Civlib, Wartime\n0.88\n3.63\n13.8\n\n\n\n\nCV预测误差一般比回归标准误要高，为什么？\n\n模型设定正确与否远比正确预测更重要，为什么？失去32年的预期寿命 vs. 失去14年预期寿命哪个更严重？\n进一步改进模型\n\n\nfit &lt;- lm(formula = lifeexp ~ I(log(gdpcap)) + I(log(school)) + I(log(civlib)) + wartime + I(wartime^2), data=life)\nsummary(fit)\n\n\nCall:\nlm(formula = lifeexp ~ I(log(gdpcap)) + I(log(school)) + I(log(civlib)) + \n    wartime + I(wartime^2), data = life)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.7938  -1.7693   0.0006   2.0123  10.5086 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       8.0854     5.4620   1.480  0.14233    \nI(log(gdpcap))    5.8587     0.6872   8.525 3.60e-13 ***\nI(log(school))    6.0283     0.8606   7.005 4.53e-10 ***\nI(log(civlib))    0.1266     0.8955   0.141  0.88790    \nwartime          36.3329    12.8663   2.824  0.00585 ** \nI(wartime^2)   -110.8191    36.8656  -3.006  0.00344 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.482 on 89 degrees of freedom\nMultiple R-squared:  0.8987,    Adjusted R-squared:  0.8931 \nF-statistic:   158 on 5 and 89 DF,  p-value: &lt; 2.2e-16\n\nfitglm &lt;- glm(lifeexp~ I(log(gdpcap)) + I(log(school)) + I(log(civlib)) + wartime + I(wartime^2), data=life)\ncv.err &lt;- cv.glm(life,fitglm,K=5)\ncv.err$delta\n\n[1] 12.95672 12.77216\n\n\n\n\n\n\n\n\n\n\n\n协变量\n(R^2)\n()\n5重交叉验证误差\n\n\n\n\nGDP, School, Civlib, Wartime\n0.75\n5.43\n32.3\n\n\nlog(GDP), log(School), Civlib, Wartime\n0.88\n3.63\n13.8\n\n\nlog(GDP), log(School),log(Civlib), Wartime, (Wartime^2)\n0.90\n3.48\n12.6"
  },
  {
    "objectID": "dataanlyr/models/regmodels.html#多元线性回归",
    "href": "dataanlyr/models/regmodels.html#多元线性回归",
    "title": "回归模型",
    "section": "",
    "text": "本部分的重点包括：\n- 线性回归的假定\n- 探索性数据分析图和汇总统计数据\n- 使用残差诊断回归假设\n- 解释多元线性回归的参数和相关检验和区间\n- 自助法置信区间的原理\n\n\n\n\n\\[\\begin{aligned}\n  y_i & =  X_i \\beta+\\epsilon_i \\\\\n      & =  \\beta_1 X_{i1}+\\cdots + \\beta_k X_{ik} + \\epsilon_i\n\\end{aligned},\\qquad for \\quad i=1,\\cdots , n\\]\n其中， \\(\\epsilon_i\\) 相互独立，服从均值为0，标准差为 \\(\\sigma\\) 正态分布\n或者\n\\[y_i \\sim N(X_i \\beta, \\sigma^2),\\qquad for \\quad i=1,\\cdots , n\\]\n其中， \\(X\\) 是 \\(n\\times k\\) 的预测变量矩阵，第 \\(i\\) 行为 (X_{i})， \\(\\beta\\) 是长度为 \\(k\\) 的列向量\n\n\n\n\n\n\n\n\n\n\n\n\n假定\n表达式\n假设不成立\n解决方案\n\n\n\n\n1 无完全共线性\n\\(rank(X)=k,k&lt;n\\)\n无法识别回归系数\n减少变量或增加样本\n\n\n2 X外生\n\\(E(X\\epsilon )=0\\)\n有偏，且不随N增加而改善\n改变研究设计，因果推断技术\n\n\n3 误差项零均值\n\\(E(\\epsilon)=0\\)\n有偏，且不随N增加而改善\n改变模型设定\n\n\n4 线性关系\n\\(y=\\beta_{1}x_{1}+\\beta_{2} x_{2}+\\cdots+\\epsilon\\)\n模型设定错误\n改变模型设定\n\n\n5 误差项不相关（观测独立）\n\\(E(\\epsilon_{i} \\epsilon_{j})=0, i\\neq j\\)\n无偏但无效，标准误不正确\n多层线性模型\n\n\n6 同方差\n\\(E(\\epsilon^{'}\\epsilon)=\\sigma^{2}\\boldsymbol{I}\\)\n无偏但无效，标准误不正确\n广义线性模型或稳健性标准误\n\n\n7 误差项服从正态分布\n\\(\\epsilon\\sim N(0,\\sigma^{2})\\)\n标准误不正确，但随N增加而改善\n广义线性模型\n\n\n\n\n如果假定1-6都成立，则最小二乘法的估计量是最优线性无偏估计量（best linear unbiased estimator, BLUE）\n如果假定7也成立，则最小二乘法得到参数估计为最小方差无偏的，即在所有线性和非线性估计量中方差最小。\n\n\n\n\n\n\n线性回归最小二乘法的假定\n\n\n\n\n哪些研究设计可能导致违背回归假定？\n\n随机选择驾驶员，改变车内音乐的音量，测试驾驶员在同一路段的反应时间\n\n随机选择学生，通过学习时间预测考试能否通过\n\n随机选择家庭，通过家庭收入预测家庭生育子女的数量\n\n通过父亲的身高，预测他长子的身高\n随机选择稻田，通过降雨量预测水稻产量\n\n随机选择大学生，通过性别、每周锻炼时间预测体重\n\n在10家有多名外科手术医生的医院选择接受某项外科手术的患者，通过患者的年龄预测手术的治疗效果，治疗效果采用1-10分记录。\n\n如果违背正态性假定，那么需要广义线性模型；如果违背独立性假定，那么需要采用分层线性模型。\n\n\n\n\n示例：母亲教育与子女认知能力数据集\n\n\n\nlibrary(\"foreign\")\nlibrary(\"dplyr\")\nlibrary(\"kableExtra\")\nlibrary(gridExtra)\nkidiq &lt;- read.dta(\"kidiq.dta\")\n\nkidiq &lt;- kidiq %&gt;% mutate(hsfactor = ifelse(mom_hs==1, \"高中及以上\", \"高中以下\"))\ntable1 &lt;- kidiq %&gt;%\n  filter(row_number() &lt; 6 | row_number() &gt; 428)\nkable(table1, booktabs=T,caption=\"示例数据概览\") %&gt;%\n  kable_styling(latex_options = \"scale_down\")\n\n\n示例数据概览\n\n\n\nkid_score\nmom_hs\nmom_iq\nmom_work\nmom_age\nhsfactor\n\n\n\n\n1\n65\n1\n121.11753\n4\n27\n高中及以上 |\n\n\n2\n98\n1\n89.36188\n4\n25\n高中及以上 |\n\n\n3\n85\n1\n115.44316\n4\n27\n高中及以上 |\n\n\n4\n83\n1\n99.44964\n3\n25\n高中及以上 |\n\n\n5\n115\n1\n92.74571\n4\n27\n高中及以上 |\n\n\n429\n93\n0\n74.86073\n2\n25\n高中以下 |\n\n\n430\n94\n0\n84.87741\n4\n21\n高中以下 |\n\n\n431\n76\n1\n92.99039\n4\n23\n高中及以上 |\n\n\n432\n50\n0\n94.85971\n2\n24\n高中以下 |\n\n\n433\n88\n1\n96.85662\n2\n21\n高中及以上 |\n\n\n434\n70\n1\n91.25334\n2\n25\n高中及以上 |\n\n\n\n\n\n有哪些变量类型？\n\n\n\n\n\n\n\n\n示例中尺度变量的直方图\n\n\n\n\n这些变量的分布有什么特征？\n\n\n\n\nlibrary(GGally)\ngg &lt;- ggpairs(data = kidiq, \n              columns = c(\"hsfactor\", \"kid_score\", \"mom_iq\", \"mom_age\"))\n gg[4,1] &lt;- gg[4,1] + geom_histogram( binwidth = 5)\n gg[2,1] &lt;- gg[2,1] + geom_histogram( binwidth = 10)\n gg[3,1] &lt;- gg[3,1] + geom_histogram( binwidth = 5)\ngg\n\n\n\n\n探索变量间的关系\n\n\n\n\n对角线上为单变量的分布，箱线图和直方图为按是否上过高中分类的其他尺度变量的分布，散点图为两个尺度变量的相关性，对角线上方为相关系数。\n变量之间的关系是怎样的？\n\n# Coded scatterplot\nggplot(kidiq, aes(x = mom_iq, y = kid_score, colour = hsfactor)) +\n  geom_point(aes(shape = hsfactor)) +\n  geom_smooth(aes(linetype = hsfactor), method = lm, se = FALSE)\n\n\n\n\n子女测试分数的线性趋势\n\n\n\n\n通过探索性分析，可以尝试问这样的问题：\n\n子女测试分数是线性增加的吗？\n\n测试分数增长的快慢受到母亲是否上过高中影响吗？\n\n上过高中的母亲其子女测试分数一定更高吗？\n\n这些关系在统计上是显著的吗？\n\n预测子女测试分数的效果如何？\n\n\n\n\n\n\n\n\nfit1 &lt;- lm (kid_score ~ mom_iq, data = kidiq)\n\nsummary(fit1)\n\n\nCall:\nlm(formula = kid_score ~ mom_iq, data = kidiq)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-56.753 -12.074   2.217  11.710  47.691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 25.79978    5.91741    4.36 1.63e-05 ***\nmom_iq       0.60997    0.05852   10.42  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.27 on 432 degrees of freedom\nMultiple R-squared:  0.201, Adjusted R-squared:  0.1991 \nF-statistic: 108.6 on 1 and 432 DF,  p-value: &lt; 2.2e-16\n\n\n\n系数的解释：保持其他变量不变，预测变量单位变动相应的响应变量变动大小。\n\n但是注意：有时候在保持其他变量不变的情况下是不可能变动某个变量的，例如回归中同时包括IQ和 (IQ^{2})，或者有交互项 mom_hs*mom_i q.\n\n回归模型诊断的常用方法，plot函数会自动绘制4张诊断图\n\n线性假定：残差与拟合值没有系统关联\n\n正态性：正态Q-Q图是与理论正态分布相比，标准化残差的概率图，应落在呈45度角的直线上\n\n同方差性：位置尺度图水平线周围的点应该随机分布\n\n离群点、高杠杆点和强影响点：残差杠杆图采用cook距离来识别对模型参数的估计产生过大影响（高杠杆值）的观测点，一般Cook距离大于 \\(4/(n-k-1)\\)，则表明是强影响点\n\n\n\npar(mfrow=c(2,2))\nplot(fit1)\n\n\n\n\n\n\n\n\n\n比较线性和2次曲线拟合\n\n\n# Fitted models for Model 2 and Model 2Q\nggplot(kidiq, aes(x = mom_iq, y = kid_score)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = y ~ x, \n              se = FALSE, linetype = 1) +\n  stat_smooth(method = \"lm\", formula = y ~ x + I(x^2), \n              se = FALSE, linetype = 2)\n\n\n\n\n比较线性和2次曲线拟合\n\n\n\n\n\n修正模型后的诊断\n\n\nkidiq &lt;- kidiq %&gt;% mutate(mom_iq2 = mom_iq*mom_iq)\nfit2 &lt;- lm (kid_score ~ mom_iq + mom_iq2, data = kidiq)\nsummary(fit2)\n\n\nCall:\nlm(formula = kid_score ~ mom_iq + mom_iq2, data = kidiq)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-54.824 -11.640   2.883  11.372  50.813 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -99.033675  37.301385  -2.655 0.008226 ** \nmom_iq        3.076800   0.730291   4.213 3.07e-05 ***\nmom_iq2      -0.011917   0.003517  -3.389 0.000767 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.05 on 431 degrees of freedom\nMultiple R-squared:  0.2217,    Adjusted R-squared:  0.2181 \nF-statistic: 61.38 on 2 and 431 DF,  p-value: &lt; 2.2e-16\n\npar(mfrow=c(2,2))\nplot(fit2)\n\n\n\n\n\n\n\n\n\n\n\n\nfit3 &lt;- lm(kid_score ~ mom_hs, data = kidiq)\nsummary(fit3)\n\n\nCall:\nlm(formula = kid_score ~ mom_hs, data = kidiq)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-57.55 -13.32   2.68  14.68  58.45 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   77.548      2.059  37.670  &lt; 2e-16 ***\nmom_hs        11.771      2.322   5.069 5.96e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 432 degrees of freedom\nMultiple R-squared:  0.05613,   Adjusted R-squared:  0.05394 \nF-statistic: 25.69 on 1 and 432 DF,  p-value: 5.957e-07\n\n\n此种情况下，回归系统和截距很容易解释。这个回归和两个独立样本t检验一致吗？需要满足等方差的假设吗？\n\n\n\n\nfit4 &lt;- lm(kid_score ~ mom_hs+ mom_iq, data = kidiq)\nsummary(fit4)\n\n\nCall:\nlm(formula = kid_score ~ mom_hs + mom_iq, data = kidiq)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.873 -12.663   2.404  11.356  49.545 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 25.73154    5.87521   4.380 1.49e-05 ***\nmom_hs       5.95012    2.21181   2.690  0.00742 ** \nmom_iq       0.56391    0.06057   9.309  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.14 on 431 degrees of freedom\nMultiple R-squared:  0.2141,    Adjusted R-squared:  0.2105 \nF-statistic: 58.72 on 2 and 431 DF,  p-value: &lt; 2.2e-16\n\n\n\n相对于模型3，估计在考虑母亲智商影响的情况下，是否上过高中对子女测试分数的影响差异。平均而言，上过高中的母亲其子女测试分数高5.95分。上面模型的11.77分是高估了高中对分数的影响。\n\n类似的，在考虑母亲高中的影响的情况下，母亲智商对子女测试分数的影响平均而言是0.56，也小于第1个模型的系数0.61。\n\n模型的拟合优度R方为0.21，远高于只用高中变量的模型拟合优度0.05。\n\n\n\n\n除了了解效应的显著性，还需要采用置信区间量化效应量的不确定性和量化模型预测值的不确定性。\n\nconfint(fit4)\n\n                 2.5 %     97.5 %\n(Intercept) 14.1839148 37.2791615\nmom_hs       1.6028370 10.2973969\nmom_iq       0.4448487  0.6829634\n\nnew.data &lt;- data.frame(mom_iq=190, mom_hs = 0) \npredict(fit4, new = new.data, interval = \"prediction\")\n\n       fit      lwr      upr\n1 132.8737 95.18156 170.5658\n\n\n\n95%的信心，母亲是否上过高中造成子女测试分数的差异在1.6分到10.3分之间。\n\n基于模型4，有95%的信心，如果母亲智商190，但是没有上过高中，则子女的测试分数在95到171之间。\n\n\n\n\n如果回归模型违背了假设，自助法是一种更稳健的统计推断方法。其原理是原始数据样本能够代表总体，所以可以通过从原始样本数据中再抽样，获得多个子样本，从子样本的统计特征了解估计参数的不确定性。\n自助法的步骤如下：\n\n在原始数据中进行重抽样\n\n用新的子样本重新拟合原来的回归模型得到新的回归系数、截距\n\n重复上面的两个步骤足够多的次数（比如1000次）\n\n采用1000次重抽样回归模型的估计量绘制参数自助法分布\n\n通过2.5%和97.5%的百分位数获得每个参数自助法分布的置信区间\n\n\nlibrary(rsample)\nlibrary(tidyverse)\nset.seed(666)\nbootreg &lt;- kidiq %&gt;% \n  bootstraps(1000) %&gt;%\n  pull(splits) %&gt;% \n  map(\\(df) lm(kid_score ~ mom_iq + mom_hs, data = df) ) %&gt;% \n  map(\\(mod) as.data.frame(t(as.matrix(coef(mod))))) %&gt;%\n  list_rbind() %&gt;% pivot_longer(everything(), names_to = \"variable\", values_to = \"value\")\n\nbootregresult &lt;- bootreg %&gt;%\n  group_by(variable) %&gt;% \nsummarise(across( where(is.numeric),  \n          list(low = ~quantile(., probs = 0.025, na.rm = TRUE),  \n               high = ~quantile(., probs = 0.975, na.rm = TRUE)))) \nbootregresult\n\n# A tibble: 3 × 3\n  variable    value_low value_high\n  &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)    14.1       37.5  \n2 mom_hs          1.51      10.5  \n3 mom_iq          0.439      0.681\n\n\n\nggplot(bootreg, aes(x = value)) +\n  geom_histogram(fill = \"steelblue\", color = \"black\", bins = 30) +\n  facet_wrap(~variable, scales = \"free\") \n\n\n\n\n回归系数的自助法分布\n\n\n\n\n结果与正态分布理论假定下的结果相似。此外，还有多种其他计算自助法置信区间的方法，但是百分位法是应用最广泛的。\n\n\n\n\n没有截距的模型（常数项）\n\n\nfit5 &lt;- lm (kid_score ~ mom_hs + mom_iq - 1, data = kidiq)\n\nsummary(fit5)\n\n\nCall:\nlm(formula = kid_score ~ mom_hs + mom_iq - 1, data = kidiq)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-55.348 -10.796   2.187  12.789  50.838 \n\nCoefficients:\n       Estimate Std. Error t value Pr(&gt;|t|)    \nmom_hs  5.99194    2.25786   2.654  0.00825 ** \nmom_iq  0.81524    0.01979  41.189  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.51 on 432 degrees of freedom\nMultiple R-squared:  0.9571,    Adjusted R-squared:  0.9569 \nF-statistic:  4817 on 2 and 432 DF,  p-value: &lt; 2.2e-16\n\n\n\n添加交互项 增加一个交互项以允许IQ的回归系数随高中完成情况不同而变动\n\n\nfit6 &lt;- lm (kid_score ~ mom_hs + mom_iq + mom_hs:mom_iq, data = kidiq)\n\nsummary(fit6)\n\n\nCall:\nlm(formula = kid_score ~ mom_hs + mom_iq + mom_hs:mom_iq, data = kidiq)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.092 -11.332   2.066  11.663  43.880 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -11.4820    13.7580  -0.835 0.404422    \nmom_hs         51.2682    15.3376   3.343 0.000902 ***\nmom_iq          0.9689     0.1483   6.531 1.84e-10 ***\nmom_hs:mom_iq  -0.4843     0.1622  -2.985 0.002994 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.97 on 430 degrees of freedom\nMultiple R-squared:  0.2301,    Adjusted R-squared:  0.2247 \nF-statistic: 42.84 on 3 and 430 DF,  p-value: &lt; 2.2e-16\n\n\n\n解释系数和截距\n\n截距： \\(IQ=0 \\quad hs=0\\)（无意义）\n\nmom_hs的系数： IQ=0时，是否完成高中相应的孩子得分差异\n\nmom_iq的系数： 未完成高中母亲中，IQ增加所相应的孩子得分差异，红线斜率\n\n交互项的系数：母亲是否完成高中两组之间在IQ斜率上的差别\n\n\n回到图4，交互项体现了图中的什么关系？\n\n快速添加交互项\n\n\nfit6 &lt;- lm (kid_score ~ mom_hs * mom_iq, data = kidiq)\n\nsummary(fit6)\n\n\nCall:\nlm(formula = kid_score ~ mom_hs * mom_iq, data = kidiq)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.092 -11.332   2.066  11.663  43.880 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -11.4820    13.7580  -0.835 0.404422    \nmom_hs         51.2682    15.3376   3.343 0.000902 ***\nmom_iq          0.9689     0.1483   6.531 1.84e-10 ***\nmom_hs:mom_iq  -0.4843     0.1622  -2.985 0.002994 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.97 on 430 degrees of freedom\nMultiple R-squared:  0.2301,    Adjusted R-squared:  0.2247 \nF-statistic: 42.84 on 3 and 430 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n最终多元线性回归模型的典型特征包括：\n\n解释变量能够针对主要的研究问题\n\n解释变量控制了重要的协变量\n\n调查了潜在交互作用\n\n中心化变量方便解释\n\n剔除了不必要的变量\n\n使用残差图检验了回归假定和影响点\n\n模型以简洁地方式讲述了一个有说服力的故事\n\n虽然在报告研究结果时，一般要求选择一个合理的最终模型，但是值得注意：\n1 研究者在得出结论时通常会检查并考虑一系列相关的模型\n2 不同的研究者有时会针对同一组数据选择不同的模型作为“最终模型”。“最终模型”的选择取决于许多因素，例如主要研究问题、建模目的、简约性和拟合模型质量之间的权衡、基本假设等。建模决策不应自动化或完全基于统计检验；学科领域知识应始终在建模过程中发挥作用。要能够捍卫所选择的任何最终模型，但不应该感到有压力要找到唯一的“正确模型”，尽管大多数好的模型都会得出类似的结论。\n在比较不同的模型构建时，可以使用的检验方法：\n\nR方能够测量模型解释的响应变量的变异程度。但是会随着增加自变量而增大，即使自变量增加的信息很少。\n\n调整后R方，可以增加模型复杂度的惩罚。\n\nAIC（Akaike Information Criterion, 赤池信息准则）来自信息论， \\(AIC = -2 ( ln ( likelihood )) + 2 K\\) ，其中likelihood为给定模型的条件下数据的概率，k为参数个数。同样是为了平衡模型性能和模型复杂度，无论模型大小，AIC值越小越好。BIC（贝叶斯信息准则）与 AIC 类似，但对附加模型项的惩罚更大。\n\n平方和 F 检验。是对单个模型系数 t 检验的推广，可用于对嵌套模型进行显著性检验，即其中一个模型是另一个模型的简化版本。\n\n一个可能的最终模型如下：\n\nfit7 &lt;- lm (kid_score ~ mom_hs + mom_iq + mom_iq2, data = kidiq)\n\nsummary(fit7)\n\n\nCall:\nlm(formula = kid_score ~ mom_hs + mom_iq + mom_iq2, data = kidiq)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-54.553 -10.940   2.502  11.095  48.710 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -88.541848  37.390233  -2.368 0.018324 *  \nmom_hs        5.107323   2.207015   2.314 0.021131 *  \nmom_iq        2.828771   0.734492   3.851 0.000135 ***\nmom_iq2      -0.010910   0.003526  -3.094 0.002104 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.96 on 430 degrees of freedom\nMultiple R-squared:  0.2313,    Adjusted R-squared:  0.2259 \nF-statistic: 43.12 on 3 and 430 DF,  p-value: &lt; 2.2e-16\n\n\n\n模型的比较\n\n\nanova(fit7, fit4)\n\nAnalysis of Variance Table\n\nModel 1: kid_score ~ mom_hs + mom_iq + mom_iq2\nModel 2: kid_score ~ mom_hs + mom_iq\n  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   \n1    430 138670                                \n2    431 141757 -1     -3087 9.5723 0.002104 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n预期寿命的影响因素\n\n数据来源：罗伯特·J.巴罗，夏威尔·萨拉-伊-马丁《经济增长》所引用的Barro-Lee数据集（我们只使用整理后85年数据)\n135个国家的人均GDP（gdpcap）、25岁以上平均受教育年限（school）、公民自由度（civlib，由低到高1-7分）、近期战争时间比例（wartime）。\n\n先来一个简单的回归，只有教育系数显著，是否符合你的预期？会不会模型设定错误？\n\n\nlife &lt;- read.csv(file = \"./life.csv\")\nlife &lt;- na.omit(life)\nfit &lt;- lm(formula = lifeexp ~ gdpcap + school + civlib + wartime,data = life)\nsummary(fit)\n\n\nCall:\nlm(formula = lifeexp ~ gdpcap + school + civlib + wartime, data = life)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.2871  -3.0705  -0.0368   4.6629  12.1285 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 52.9671697  3.0379356  17.435  &lt; 2e-16 ***\ngdpcap       0.0003216  0.0002551   1.260    0.211    \nschool       2.3866392  0.4102266   5.818 9.01e-08 ***\ncivlib      -0.7522863  0.4861266  -1.548    0.125    \nwartime      1.0936902  5.5354286   0.198    0.844    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.43 on 90 degrees of freedom\nMultiple R-squared:  0.7509,    Adjusted R-squared:  0.7399 \nF-statistic: 67.84 on 4 and 90 DF,  p-value: &lt; 2.2e-16\n\n\n\n我们所看到的效应关系是真实的吗？三个途径去验证\n\n标准误和相关检验：t检验和F检验（见回归输出结果）\n\n残差的模式\n\n\n残差的理想模式：\n\nx &lt;- runif(1000,1,100)\nerror &lt;- rnorm(1000)\ny &lt;- 10+ 3*x +error\nfit0 &lt;- lm(y~x)\nplot(x=x,y=fit0$residuals)\n\n\n\n\n\n\n\n\n存在异方差时的模式：\n\nx &lt;- runif(1000,1,100)\nerror &lt;- rnorm(1000,mean=0,sd=1*x)\ny &lt;- 10+ 3*x +error\nfit0 &lt;- lm(y~x)\nplot(x=x,y=fit0$residuals)\n\n\n\n\n\n\n\n\n\n存在离群值\n加权最小二乘法\n\n存在异方差，则具有较高误差方差的观测含有较少的信息，而具有较低误差方差的观测含有较多的信息。\n\n如果我们能在估计中给予低误差方差观察较高权重，那么能得到更有效的估计。那么相应的最小化误差平方和也变为最小化 加权误差平方和 。\n[{n}{i=1} w_i {i}{2}=]\n\n回归系数加权最小二乘估计：\n[_{WLS}=()^{-1}]\n\n上式其中W为对角线为权重的对角阵，每个权重应该与每个y观测的标准差成反比，即\n[_iN(0,^2_i )^2_i = 1/w^2_i ]\n\n如果我们通过样本性质能够确定权重，lm()函数中可以通过weights参数设定权重。\n\n如果不知道权重，可以利用残差估计权重（残差平方对自变量回归的拟合值的倒数），即可行广义最小二乘法。\n\n通过该方法获取的标准误，即为怀特标准误、稳健标准误、三明治标准误、异方差一致标准误等。\n\n\n\nlibrary(lmtest); library(car)\n\n载入需要的程序包：zoo\n\n\n\n载入程序包：'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n载入需要的程序包：carData\n\n\n\n载入程序包：'car'\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nvc &lt;- hccm(fit, type = \"hc1\")                \nse.corrected &lt;- sqrt(diag(vc))\ncoeftest(fit, vcov=vc)\n\n\nt test of coefficients:\n\n               Estimate  Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 52.96716974  3.29289359 16.0853 &lt; 2.2e-16 ***\ngdpcap       0.00032156  0.00026753  1.2020    0.2325    \nschool       2.38663924  0.42745500  5.5834 2.473e-07 ***\ncivlib      -0.75228629  0.50324260 -1.4949    0.1384    \nwartime      1.09369023  5.30329592  0.2062    0.8371    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n示例的残差图\n\n\nplot(fit$fitted.values,fit$residuals, main=\"life expectancy against fitted Y\")\n\n\n\n\n\n\n\n\n\n有异方差问题吗？还有什么问题？\n\n残差与拟合值相关，意味着什么？\n\n分别检查残差与各个协变量的关系\n\n残差与公民自由度的关系\n\n\n\nplot(life$civlib,fit$residuals)\n\n\n\n\n\n\n\n\n\n残差与战时比例的关系\n\n\nplot(life$wartime,fit$residuals)\n\n\n\n\n\n\n\n\n\n残差与教育的关系\n\n\nplot(life$school,fit$residuals)\n\n\n\n\n\n\n\n\n\n残差与GDP的关系\n\n\nplot(life$gdpcap,fit$residuals)\n\n\n\n\n\n\n\n\n\n示例修正模型设定\n\n公民自由度、战时比例与残差似乎没有明显模式，而教育和GDP和残差有明显的非线性相关关系。\n\n残差中的模式意味着模型设定错误。\n\n如果看到有曲线相关关系，可以考虑添加二次项或进行对话变换。\n\n重新设定模型，将教育和GDP变换为对数项。\n\n\n\nfit &lt;- lm(formula = lifeexp ~ I(log(gdpcap)) + I(log(school)) + civlib + wartime, data=life)\nsummary(fit)\n\n\nCall:\nlm(formula = lifeexp ~ I(log(gdpcap)) + I(log(school)) + civlib + \n    wartime, data = life)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.5038  -1.9584   0.2405   2.0338  11.0816 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     13.1893     5.4439   2.423   0.0174 *  \nI(log(gdpcap))   5.3730     0.6867   7.825 9.35e-12 ***\nI(log(school))   6.0431     0.9040   6.685 1.88e-09 ***\ncivlib          -0.1843     0.3084  -0.598   0.5517    \nwartime         -0.2174     3.6894  -0.059   0.9531    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.627 on 90 degrees of freedom\nMultiple R-squared:  0.8889,    Adjusted R-squared:  0.8839 \nF-statistic:   180 on 4 and 90 DF,  p-value: &lt; 2.2e-16\n\n\n\n再观察残差图\n\n\nplot(fit$fitted.values,fit$residuals, main=\"life expectancy against fitted Y\")\n\n\n\n\n\n\n\n\n\n残差与公民自由度的关系\n\n\nplot(life$civlib,fit$residuals)\n\n\n\n\n\n\n\n\n\n残差与战时比例的关系\n\n\nplot(life$wartime,fit$residuals)\n\n\n\n\n\n\n\n\n\n残差与教育的关系\n\n\nplot(life$school,fit$residuals)\n\n\n\n\n\n\n\n\n\n残差与GDP的关系\n\n\nplot(life$gdpcap,fit$residuals)\n\n\n\n\n\n\n\n\n\n观察残差图，可以看出相关模式已经消失了，但是还有异方差的可能（喇叭形）\n比较稳健性标准误\n\n\nlibrary(orgutils)\nfitsum &lt;- summary(fit)\nvc &lt;- hccm(fit, type = \"hc1\") \nfitout &lt;- data.frame(est=fitsum$coefficients[,1], se=fitsum$coefficients[,2], robust_se=sqrt(diag(vc)))\ntoOrg(round(fitout,digits=2))\n\n| row.names      |   est |   se | robust_se |\n|----------------+-------+------+-----------|\n| (Intercept)    | 13.19 | 5.44 |      5.23 |\n| I(log(gdpcap)) |  5.37 | 0.69 |      0.65 |\n| I(log(school)) |  6.04 |  0.9 |      0.81 |\n| civlib         | -0.18 | 0.31 |      0.28 |\n| wartime        | -0.22 | 3.69 |      3.43 |\n\n\n\n模型的拟合优度\n\n模型解释力的评价\n\nR方\n\n回归标准误\n\n样本外检验\n\n交叉验证\n\n回归标准误\n\n(^2)反映拟合值与真实值之间的平均差距，并且它与y的单位尺度是相同的，非常容易解释。\n\n(^2)越小，说明预测效果越好，(y)越接近真实值。\n\n\n\n\n\n协变量\n(R^2)\n()\n\n\n\n\nGDP, School, Civlib, Wartime\n0.75\n5.43\n\n\nlog(GDP), log(School), Civlib, Wartime\n0.88\n3.63\n\n\n\n\n相同的数据，相同的响应变量\n\n每个值是什么含义？哪个模型好一些？\n样本外拟合优度\n\n假设我们用CGSS2010的数据建立了某个模型，那么我们可能想知道这个模型来预测CGSS2015的数据效果如何？\n\n如果我们用模型拟合样本外数据和训练样本同样好，那么我们更有信心相信我们找到了真实的模型。\n\n样本外检验步骤：\n\n采用训练样本拟合模型，获取回归系数 (_{training})\n\n用测试样本的数据计算拟合值 (y_{test})\n\n比较训练样本的拟合度（例如，({training})）与测试样本的拟合度 (std dev(y{test} - _{test}) ) ，如果两者相等则说明模型预测能力高。\n\n\n交叉验证\n\n如果我们样本量有限，并且很难找到其他的合格样本，可以采取交叉验证。\n\n交叉验证的步骤：\n\n将数据分成k等分，留下一份作为测试集，k-1份作为训练集。特别是Leave-one-out cross validataion (LOOCV) 留一法是比较常用的技术，即将样本量为n的样本分成n等分，只留1个观测作为测试集。\n\n采用训练集来进行回归，获取回归系数。\n\n利用测试集和回归系数来后去测试集预测值\n\n比较测试集预测值与真实值，计算平均预测误差，即由预测值与真实值获取的标准差。\n\n重复1-4步 k 次，计算平均预测误差平方的均值，再取平方根，即得到了 k重交叉验证的预测误差。\n\n\n\n\nlibrary(boot)\n\n\n载入程序包：'boot'\n\n\nThe following object is masked from 'package:car':\n\n    logit\n\n# 注意要采用广义线性回归函数\nfitglm &lt;- glm(lifeexp~gdpcap+school+civlib+wartime, data=life)\n# 5重交叉验证\ncv.err &lt;- cv.glm(life,fitglm,K=5)\n# 报告交叉验证的预测误差：第1个是原始值，第2个是调整值（与留一法比较）\ncv.err$delta\n\n[1] 31.6745 31.2306\n\n\n\n\n\n\n\n\n\n\n\n协变量\n(R^2)\n()\n5重交叉验证误差\n\n\n\n\nGDP, School, Civlib, Wartime\n0.75\n5.43\n32.3\n\n\nlog(GDP), log(School), Civlib, Wartime\n0.88\n3.63\n13.8\n\n\n\n\nCV预测误差一般比回归标准误要高，为什么？\n\n模型设定正确与否远比正确预测更重要，为什么？失去32年的预期寿命 vs. 失去14年预期寿命哪个更严重？\n进一步改进模型\n\n\nfit &lt;- lm(formula = lifeexp ~ I(log(gdpcap)) + I(log(school)) + I(log(civlib)) + wartime + I(wartime^2), data=life)\nsummary(fit)\n\n\nCall:\nlm(formula = lifeexp ~ I(log(gdpcap)) + I(log(school)) + I(log(civlib)) + \n    wartime + I(wartime^2), data = life)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.7938  -1.7693   0.0006   2.0123  10.5086 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       8.0854     5.4620   1.480  0.14233    \nI(log(gdpcap))    5.8587     0.6872   8.525 3.60e-13 ***\nI(log(school))    6.0283     0.8606   7.005 4.53e-10 ***\nI(log(civlib))    0.1266     0.8955   0.141  0.88790    \nwartime          36.3329    12.8663   2.824  0.00585 ** \nI(wartime^2)   -110.8191    36.8656  -3.006  0.00344 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.482 on 89 degrees of freedom\nMultiple R-squared:  0.8987,    Adjusted R-squared:  0.8931 \nF-statistic:   158 on 5 and 89 DF,  p-value: &lt; 2.2e-16\n\nfitglm &lt;- glm(lifeexp~ I(log(gdpcap)) + I(log(school)) + I(log(civlib)) + wartime + I(wartime^2), data=life)\ncv.err &lt;- cv.glm(life,fitglm,K=5)\ncv.err$delta\n\n[1] 12.95672 12.77216\n\n\n\n\n\n\n\n\n\n\n\n协变量\n(R^2)\n()\n5重交叉验证误差\n\n\n\n\nGDP, School, Civlib, Wartime\n0.75\n5.43\n32.3\n\n\nlog(GDP), log(School), Civlib, Wartime\n0.88\n3.63\n13.8\n\n\nlog(GDP), log(School),log(Civlib), Wartime, (Wartime^2)\n0.90\n3.48\n12.6"
  },
  {
    "objectID": "dataanlyr/models/regmodels.html#最大似然估计",
    "href": "dataanlyr/models/regmodels.html#最大似然估计",
    "title": "回归模型",
    "section": "最大似然估计",
    "text": "最大似然估计\n本部分知识重点：\n\n能够描述什么是似然\n\n了解和针对简单的例子应用最大似然原理\n\n了解获取或估算最大似然估计的三种方法\n\n使用似然值比较模型\n\n为简单模型构建似然计算公式\n\n\n引例\n投三次硬币决定来上课还是在宿舍睡觉，每出现一次正面代表上课多一票，每出现一次反面代表睡觉多一票。结果是前两次为正面、最后一次为反面。你可能会怀疑硬币是不是有问题，正面出现的概率真是1/2吗？可以计算正面的概率不同取值时，该事件的概率。\n\n\n\n\n\n\n\n\n\n()\n()\n(^{1s} (1-)^{0s})\n(f_B ( ))\n\n\n\n\n({1,1,0})\n( 0.00)\n( 0.002(1-0.00)1)\n( 0.000)\n\n\n({1,1,0})\n( 0.10)\n( 0.102(1-0.10)1)\n( 0.009)\n\n\n({1,1,0})\n( 0.20)\n( 0.202(1-0.20)1)\n( 0.032)\n\n\n({1,1,0})\n( 0.30)\n( 0.302(1-0.30)1)\n( 0.063)\n\n\n({1,1,0})\n( 0.40)\n( 0.402(1-0.40)1)\n( 0.096)\n\n\n({1,1,0})\n( 0.50)\n( 0.502(1-0.50)1)\n( 0.125)\n\n\n({1,1,0})\n( 0.60)\n( 0.602(1-0.60)1)\n( 0.144)\n\n\n({1,1,0})\n( 0.67)\n( 0.672(1-0.67)1)\n( 0.148)\n\n\n({1,1,0})\n( 0.70)\n( 0.702(1-0.70)1)\n( 0.147)\n\n\n({1,1,0})\n( 0.80)\n( 0.802(1-0.80)1)\n( 0.128)\n\n\n({1,1,0})\n( 0.90)\n( 0.902(1-0.90)1)\n( 0.081)\n\n\n({1,1,0})\n( 1.00)\n( 1.002(1-1.00)1)\n( 0.000)\n\n\n\n我们将以分布参数的函数来描述观察数据的联合分布概率称之为似然函数（likelihood function），表示为(L(;))。 [L=2(1-)1] [log L=2log+ 1log(1-)] [=-=0] [/3] 最大化似然函数的()值是最大似然估计（maximum likelihood estimate, MLE）\n\n\n\n\n\n\n\n\n\n\n\n最大似然推断\n最大似然推断并不是将数据视为随机的、参数视为固定的，而是将数据视为固定的，寻找什么样的参数最有可能生成这个数据，即将数据的联合分布视作某个分布密度或概率分布函数的参数的函数。\n\n线性回归系数的最大似然推断\n\n2012年189个国家的二氧化碳排放量与人均GDP（购买力平价），如果构建简单模型：(Y=_0+_1 X +)，其中Y为二氧化碳排放量的对数，X为人均GDP的对数。\n\n\n\n\n\n\n\n\n\n\n首先假定每个观测(Y_i)独立同分布，服从正态分布(f_{N}(y_i ;_i ,^2 ))\n\n根据线性模型，等同于(i f{N}(_i ;0,^2 ))\n[f_N (_i ) =exp[] =exp[] ]\n\n似然函数是样本数据的联合分布概率，每个样本观测都是独立的，其联合分布概率等于每个观测边缘分布概率的乘积 [L(0 ,1 ,)|{y_1,,y_n},{x_1,,x_n}) = (22){(-n/2)}{i=1}^{n}exp[] = (22){(-n/2)}exp[_{i=1}^{n}]] 取对数进行简化： [ log L = log{(22){(-n/2)}exp[_{i=1}^{n}]} ] [ = -log(2^2)-{i=1}^{n}(y_i-_0 - 1 x_i)^2 ] [ = -log(2) - nlog- ] 剔除与被估计参数无关的项： [logL -nlog- ] 计算机优化程序更擅长计算最小值，采用(-2logL)，并且采用固定或已知的 简化： [-2logL {i=1}^{n}(y_i-_0 - _1 x_i)^2 ] 上式显示最大似然估计与最小二乘法的参数结果是一致的。但是如果我们只是得到了一个与最小二乘法一样的结果，那么最大似然估计的意义在哪里？\n\n\nlibrary(ProfileLikelihood)\nwdi$lgdppc&lt;-log(wdi$gdp.pc.ppp)\nxx &lt;- profilelike.lm(formula = log(co2.kt)~1, data=wdi, profile.theta=\"lgdppc\",\nlo.theta=0.84, hi.theta=1.15, length=500)\n\nwith(xx, \n  plot(theta,profile.lik,las=1,lty=1,lwd=3,\n    type=\"l\",pch=19,xlab=substitute(beta[1]),\n    ylab=\"likelihood\",yaxt=\"n\",bty=\"l\",main=\"Least Squares as MLE\",\n    xlim=c(0.85,1.1))\n  )\nabline(v=coef(out)[2],col=\"gray50\",lwd=3)\nabline(h=max(xx$profile.lik),col=\"gray50\",lwd=4)\n\n\n\n\n\n\n\n\n\n利用R最大化似然函数\n\n\n# 建立自变量（含常数项）与因变量矩阵\nx &lt;- cbind(1,as.matrix(log(wdi$gdp.pc.ppp)))    # 增加1列含1的常数项\ny &lt;- as.matrix(log(wdi$co2.kt))\nK &lt;- ncol(x); n &lt;- nrow(x)                      # 观测数n和变量数K\n# 定义似然函数，可以选择多种参数化方式，此处采用logL的完整形式\nloglik.my &lt;- function(par,X,Y) {               \n  Y &lt;- as.vector(y)\n  X &lt;- as.matrix(x)\n  xbeta &lt;- X%*%par[1:K]\n  sigma &lt;- sqrt(sum(((X[,2]-mean(X[,2]))^2)/(n-K))) # 假定标准误已知，有多种设定形式\n  sum(-(1/2)*log(2*pi)-(1/2)*log(sigma^2)-(1/(2*sigma^2))*(y-xbeta)^2) # 对数似然函数，加负号变为最小化\n}\n# 将似然函数传递给最优化函数，提供初始值，选择算法，设定迭代次数等\nmle.fit &lt;- optim(c(5,5),loglik.my, method = \"BFGS\", control = \n            list(trace=TRUE,maxit=10000,fnscale = -1),hessian = TRUE)    \n\ninitial  value 150490.736339 \nfinal  value 375.927547 \nconverged\n\nif(mle.fit$convergence!=0) \n  print(\"MDW WARNING: Convergence Problems; Try again!\")\n# 计算标准诊断量\nstderrors&lt;- sqrt(diag(-solve(mle.fit$hessian)))\nz&lt;-mle.fit$par/stderrors\np.z &lt;- 2* (1 - pnorm(abs(z)))\nout.table &lt;- data.frame(Est=mle.fit$par, SE=stderrors, Z=z, pval=p.z)\nround(out.table, 2)\n\n   Est   SE    Z pval\n1 2.69 0.67 4.02 0.00\n2 0.18 0.07 2.48 0.01\n\n\n\n\n异方差数据的最大似然估计\n\n线性回归模型假定同方差，即对于所有的观测都有 (y_i N(_i , ^2 ) )，但是如果误差项是异方差的，即 (y_i N(_i , _i^2 ) )，则会导致(se())是有偏的，()的估计是无效的。\n\n一般采用稳健性标准误来替代“有问题”的标准误，但是，异方差被看做是数据存在的一个“问题”，仅仅是因为我们假定它不应该在“正常”的数据中存在。真实的可能是我们的线性回归假定框定了我们解决问题的范围。类比一下，为什么我们不会说“异均值”是一个问题，是因为线性回归的框架中允许了因变量均值随自变量变动（这也正是我们建模的基础），那如果我们能把(_i^2 ) 也纳入到模型中，即对因变量方差建模，也就能更好的利用自变量解释解释因变量的均值和方差两方面的变化。\n\n异方差暗含了自变量与因变量方差的关系，这可能正是我们想研究的内容。\n\n实力相当的两个人才会打架，所以实力相当使得是否打架的变异程度更大\n\n公立医院民营化可能不会降低平均的社会健康水平，但是会由于覆盖风险增加社会健康水平的变异程度\n\n\n异方差正态模型的最大似然估计\n\n随机部分：\n[y_i f_N (_i , _i^2 )]\n系统部分：\n[_i = X_i ] [_i^2 = exp(Z_i ) ]\n\n异方差最大似然估计的推导：\n\n首先假定每个观测(Y_i)独立同分布，服从正态分布: [f_{N}(y_i ;_i ,_i^2 )]\n\n样本数据的联合分布：[P(y|,2 ) = {i=1}^{n}f_{N}(y_i ;_i ,_i^2 ) ]\n\n根据正态分布密度函数：[P(y|,2 ) = {i=1}^{n}(2_i^2 )^(-1/2)exp[] ]\n\n对数似然函数：[logL(,^2 |y) -_{i=1}{n}logi^2 - {i=1}{n} ]\n\n代入系统参数：[logL(,|y) -{i=1}^{n}z_i - {i=1}^{n}) ]\n\n\n假想样本量为2000的异方差数据来自真实模型：\n[y_i N (_i , _i^2 )] [_i = 5 + 10x_i ] [_i^2 = exp(1+3x_i ) ]\n分别采用线性回归与异方差的最大似然估计去拟合\n\n\nset.seed(1234)\n\nobs &lt;- 2000\nx &lt;- runif(obs)\n\nmu &lt;- 5+10*x\nsigma2 &lt;- exp(1+3*x)\n\ny &lt;- rnorm(obs, mu, sqrt(sigma2))\n\n# 线性回归拟合\nlm.fit &lt;- lm(y~x)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21.0880  -2.2792   0.0612   2.2686  19.9045 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5.0546     0.1839   27.49   &lt;2e-16 ***\nx            10.0479     0.3206   31.34   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.108 on 1998 degrees of freedom\nMultiple R-squared:  0.3296,    Adjusted R-squared:  0.3292 \nF-statistic: 982.2 on 1 and 1998 DF,  p-value: &lt; 2.2e-16\n\n# 最大似然法拟合\n# 建立两个系统部分的自变量\nxcovariates &lt;- x\nzcovariates &lt;- x\n\n# 设置参数初始值beta0,beta1,gamma0,gamma1\nstval &lt;- c(0,0,0,0)\n\n# 设定似然函数\nllk.hetnormlin &lt;- function(param,y,x,z) {\n  x &lt;- as.matrix(x)\n  z &lt;- as.matrix(z)\n  x &lt;- cbind(1,x)\n  z &lt;- cbind(1,z)\n  b &lt;- param[ 1 : ncol(x) ]\n  g &lt;- param[ (ncol(x)+1) : (ncol(x) + ncol(z)) ]\n  xb &lt;- x%*%b\n  s2 &lt;- exp(z%*%g)\n  sum(0.5*(log(s2)+(y-xb)^2/s2))  # optim是最小化函数，所以公式要乘以-1\n}\n\n# 运行优化函数得到拟合结果\nmle.fit &lt;- optim(stval,llk.hetnormlin,method=\"BFGS\",hessian=T,y=y,x=xcovariates,z=zcovariates)\n# 提取估计值\npe &lt;- mle.fit$par   # 参数的点估计\nvc &lt;- solve(mle.fit$hessian)  # 协方差矩阵\nse &lt;- sqrt(diag(vc))    # 标准误\nz &lt;- mle.fit$par/se  # z值\np.z &lt;- 2* (1 - pnorm(abs(z))) # p值\nout.table &lt;- data.frame(Coefs=c(\"beta0\",\"beta1\",\"gamma0\",\"gamma1\"), Est=mle.fit$par, SE=se, Z=z, pval=p.z)\nprint(out.table, digits=2)\n\n   Coefs   Est    SE  Z pval\n1  beta0  5.05 0.102 50    0\n2  beta1 10.05 0.278 36    0\n3 gamma0  0.99 0.064 15    0\n4 gamma1  3.01 0.112 27    0\n\n\n\n哪个模型更好？可以比较95%的预测区间\n\n\n\n\n载入程序包：'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select"
  },
  {
    "objectID": "dataanlyr/index.html#课堂练习",
    "href": "dataanlyr/index.html#课堂练习",
    "title": "数据分析与R语言应用",
    "section": "课堂练习",
    "text": "课堂练习\n\nR语言基本设置与一般统计方法练习\n\n线性回归分析练习"
  },
  {
    "objectID": "dataanlyr/visualization/visualization.html",
    "href": "dataanlyr/visualization/visualization.html",
    "title": "数据可视化",
    "section": "",
    "text": "可视化数据之前，需要准备数据，包括导入数据并进行清洗。\n\n\nR 几乎可以导入任何格式的数据，包括文本、Excel、SPSS和STATA等统计软件等。\n\n\nreadr包提供了将分隔文本文件导入 R 数据框的函数选择。\n\nlibrary(readr)\n\n# 导入逗号分割的数据\n#cgss2017 &lt;- read_csv(\"cgss2017.csv\")\n\n# 导入制表符分割的数据\n#cgss2017 &lt;- read_tsv(\"cgss2017.txt\")\n\n\n\n\nreadxl包可以从 Excel 文件导入数据，支持 xls 和 xlsx 格式。\n\nlibrary(readxl)\n\n# 从excel工作表导入数据\n#cgss2017 &lt;- read_excel(\"cgss2017.xlsx\", sheet=1)\n\n由于excel文件可以包含多个工作表，因此您可以使用sheet选项指定所需的工作表。默认值为sheet=1。\n\n\n\nhaven包提供了从各种统计包导入数据的功能。示例采用的CGSS2015子数据集\n\nlibrary(haven)\n\n# 导入stata数据\n#cgss2017 &lt;- read_dta(\"cgss2017.dta\")\n\n# 导入SPSS数据\ncgss &lt;- read_sav(\"cgss2015subset.sav\")\n\n# 导入SAS数据\n#cgss2017 &lt;- read_sas(\"cgss2017.sas7bdat\")\n\n\n\n\n\n数据清理是数据分析中最耗时的任务。以下列出了最重要的步骤。虽然方法有很多，但使用dplyr和tidyr包是最快捷、最容易学习的方法之一。\n\n\n\n包\n函数\n用途\n\n\n\n\ndplyr\nselect\n选择变量/列\n\n\ndplyr\nfilter\n选择观察值/行\n\n\ndplyr\nmutate\n转换或重新编码变量\n\n\ndplyr\nsummarize\n汇总数据\n\n\ndplyr\ngroup_by\n识别需要进一步处理的子组\n\n\ntidyr\ngather\n将宽格式数据集转换为长格式\n\n\ntidyr\nspread\n将长格式数据集转换为宽格式\n\n\n\n\n\n该select函数从数据集选取指定的变量或列。\n\nlibrary(dplyr)\n\n# 从数据框cgss中选择变量id，sex和age构成新数据集\nnewdata &lt;- select(cgss, id, sex, age)\n\n# 选择变量id以及hp1与hp4之间的所有变量 \nnewdata &lt;- select(cgss, id, hp1:hp4)\n\n# 选择除了edu和lnincome以外的所有变量\nnewdata &lt;- select(cgss, -edu, -lnincome)\n\n\n\n\n该filter函数选择符合特定条件的观测（行）。可以使用&(AND) 和|(OR) 逻辑符号组合多个条件。\n\nlibrary(dplyr)\n\n# 为了方便展示，将带有值标签的变量转换为因子(使用值标签作为取值)   \ncgss &lt;- cgss %&gt;% mutate(across(where(is.labelled), as_factor))\n\n# 选择女性观测\nnewdata &lt;- filter(cgss, sex == \"女\")\n\n# 选择来自山东的女性\nnewdata &lt;- filter(cgss, sex == \"女\" & province == \"山东省\")\n\n# 选择来自东北三省的观测\nnewdata &lt;- filter(cgss, \n                  province == \"辽宁省\" | \n                  province == \"吉林省\" | \n                  province == \"黑龙江省\")\n\n# 更简洁的代码\nnewdata &lt;- filter(cgss, \n                  province %in% \n                    c(\"辽宁省\", \"吉林省\", \"黑龙江省\"))\n\n\n\n\nmutate函数可以创建新变量或转换现有变量。\n\nlibrary(dplyr)\n\n# 将身高从厘米转化成英寸，将体重从斤转化成磅 \nnewdata &lt;- mutate(cgss, \n                  height = height * 0.394,\n                  weight   = (weight/2) * 2.205)\n\n# 如果收入高于8万元，则变量incomecat取值为\"高\"，否则取值为\"低\"\n\nnewdata &lt;- mutate(cgss, \n                  incomecat = ifelse(income &gt; 80000, \n                                     \"高\", \n                                     \"低\"))\n                  \n# 将教育程度不是初中、普通高中、中专的转化为其他类别\nnewdata &lt;- mutate(cgss, \n                  edu = ifelse(edu %in% \n                                     c(\"初中\", \"普通高中\", \"中专\"),\n                                     edu,\n                                     \"其他\"))\n                  \n# 将身高大于200或小于75设定为缺失值\nnewdata &lt;- mutate(cgss, \n                  height = ifelse(height &lt; 75 | height &gt; 200,\n                                     NA,\n                                     height))\n\n\n\n\nsummarize函数可用于描述性统计的数据汇总。可与by_group函数结合使用，用于按组计算统计值。na.rm=TRUE选项用于在计算平均值之前删除缺失值。\n\nlibrary(dplyr)\n\n# 计算身高和体重的平均值\nnewdata &lt;- summarize(cgss, \n                     mean_ht = mean(height, na.rm=TRUE), \n                     mean_weight = mean(weight, na.rm=TRUE))\nnewdata\n\n# A tibble: 1 × 2\n  mean_ht mean_weight\n    &lt;dbl&gt;       &lt;dbl&gt;\n1    164.        122.\n\n# 分性别组计算身高和体重的平均值\nnewdata &lt;- group_by(cgss, sex)\nnewdata &lt;- summarize(newdata, \n                     mean_ht = mean(height, na.rm=TRUE), \n                     mean_wt = mean(weight, na.rm=TRUE))\nnewdata\n\n# A tibble: 2 × 3\n  sex   mean_ht mean_wt\n  &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 男       170.    133.\n2 女       159.    113.\n\n\n\n\n\ndplyr和tidyr软件包允许使用管道运算符以紧凑的格式编写代码%&gt;%，运算符%&gt;%将左边的结果传递给右边的函数的第一个参数。例如：\n\nlibrary(dplyr)\n\nnewdata &lt;- filter(cgss, \n                  sex == \"女\")\nnewdata &lt;- group_by(newdata, province)\nnewdata &lt;- summarize(newdata, \n                     mean_ht = mean(height, na.rm = TRUE))\n\n# 采用管道操作符更简洁，也更符合人类思维\nnewdata &lt;- cgss %&gt;%\n  filter(sex == \"女\") %&gt;%\n  group_by(province) %&gt;%\n  summarize(mean_ht = mean(height, na.rm = TRUE))\n\n\n\n\n在 R 中，日期值以字符的形式输入。例如，记录 3 个人出生日期的简单数据集。\n\ndf &lt;- data.frame(\n  dob = c(\"11/10/1963\", \"Jan-23-91\", \"12:1:2001\")\n)\n\nstr(df) \n\n'data.frame':   3 obs. of  1 variable:\n $ dob: chr  \"11/10/1963\" \"Jan-23-91\" \"12:1:2001\"\n\n\n将字符变量转换为日期变量的方法有很多。最简单的方法之一是使用lubridate包中提供的函数。这些函数包括ymd、dmy和 ，mdy分别用于导入年-月-日、日-月-年和月-日-年的格式。\n\nlibrary(lubridate)\n# 将dob变量值从字符转化为日期型数据\ndf$dob &lt;- mdy(df$dob)\nstr(df)\n\n'data.frame':   3 obs. of  1 variable:\n $ dob: Date, format: \"1963-11-10\" \"1991-01-23\" ...\n\n\n这些值在R的内部记录为自 1970 年 1 月 1日以来的天数，可以方便地执行日期运算，提取日期元素（月、日、年），重新格式化（例如，1963年10月11日）。 日期变量对于制作时间相关图表非常重要。\n\n\n\n有些图表要求数据为宽格式，而有些图表则要求数据为长格式，示例如下。\n将宽数据集转换为长数据集\n\nlibrary(tidyr)\nwide_data &lt;- data.frame(id = c(\"01\", \"02\", \"03\"),\n                        name = c(\"张三\", \"李四\", \"王五\"),\n                        sex = c(\"男\", \"男\", \"女\"),\n                        height = c(70, 72, 62),\n                        weight = c(180, 195, 130))\nknitr::kable(wide_data, caption = \"宽数据\")\n\n\n宽数据\n\n\nid\nname\nsex\nheight\nweight\n\n\n\n\n01\n张三\n男\n70\n180\n\n\n02\n李四\n男\n72\n195\n\n\n03\n王五\n女\n62\n130\n\n\n\n\nlong_data &lt;- pivot_longer(wide_data,\n                          cols = c(\"height\", \"weight\"),\n                          names_to = \"variable\", \n                          values_to =\"value\")\n\n将长数据转化为宽数据\n\nlibrary(tidyr)\nknitr::kable(long_data, caption = \"长数据\")\n\n\n长数据\n\n\nid\nname\nsex\nvariable\nvalue\n\n\n\n\n01\n张三\n男\nheight\n70\n\n\n01\n张三\n男\nweight\n180\n\n\n02\n李四\n男\nheight\n72\n\n\n02\n李四\n男\nweight\n195\n\n\n03\n王五\n女\nheight\n62\n\n\n03\n王五\n女\nweight\n130\n\n\n\n\nwide_data &lt;- pivot_wider(long_data,\n                         names_from = \"variable\",\n                         values_from = \"value\")\n\n\n\n\n真实数据很可能包含缺失值。处理缺失数据有三种基本方法：特征选择、列删除和插补。ggplot2包中的msleep数据集描述了哺乳动物的睡眠习惯，并且在多个变量上存在缺失值。\n\n特征选择 在特征选择中，可以删除包含太多缺失值的变量（列）。\n\n\ndata(msleep, package=\"ggplot2\")\n\n# 每个变量中缺失值的比例\npctmiss &lt;- colSums(is.na(msleep))/nrow(msleep)\nround(pctmiss, 2)\n\n        name        genus         vore        order conservation  sleep_total \n        0.00         0.00         0.08         0.00         0.35         0.00 \n   sleep_rem  sleep_cycle        awake      brainwt       bodywt \n        0.27         0.61         0.00         0.33         0.00 \n\n\n61% 的 sleep_cycle 值缺失。可以决定将其删除。\n\n按列删除\n整行删除包含缺失值的观测。\n\n\nnewdata &lt;- select(msleep, genus, vore, conservation)\nnewdata &lt;- na.omit(newdata)\n\n\n补值\n插补涉及用“合理”的猜测值（假设缺失值不存在时的值）来替换缺失值。有几种方法，详见VIM、mice、Amelia和missForest等包。这里将使用VIMkNN()包中的函数，用插补值替换缺失值。\n\n\n# 用5个最近邻的值插补缺失值\nlibrary(VIM)\nnewdata &lt;- kNN(msleep, k=5)\n\n基本上，对于每个有缺失值的案例，都会选择k个最相似的、没有缺失值的案例。如果缺失值为数值型，则使用这k 个案例的中位数作为插补值。如果缺失值为类别值，则使用这k 个案例中出现频率最高的值。该过程会迭代所有观测和变量，直到结果收敛（趋于稳定）。\n重要提示：缺失值可能会对研究结果造成偏差（有时甚至非常严重）。如果有大量缺失数据，在删除观测或填补缺失值之前，要慎重考虑其合理性。"
  },
  {
    "objectID": "dataanlyr/visualization/visualization.html#数据准备",
    "href": "dataanlyr/visualization/visualization.html#数据准备",
    "title": "数据可视化",
    "section": "",
    "text": "可视化数据之前，需要准备数据，包括导入数据并进行清洗。\n\n\nR 几乎可以导入任何格式的数据，包括文本、Excel、SPSS和STATA等统计软件等。\n\n\nreadr包提供了将分隔文本文件导入 R 数据框的函数选择。\n\nlibrary(readr)\n\n# 导入逗号分割的数据\n#cgss2017 &lt;- read_csv(\"cgss2017.csv\")\n\n# 导入制表符分割的数据\n#cgss2017 &lt;- read_tsv(\"cgss2017.txt\")\n\n\n\n\nreadxl包可以从 Excel 文件导入数据，支持 xls 和 xlsx 格式。\n\nlibrary(readxl)\n\n# 从excel工作表导入数据\n#cgss2017 &lt;- read_excel(\"cgss2017.xlsx\", sheet=1)\n\n由于excel文件可以包含多个工作表，因此您可以使用sheet选项指定所需的工作表。默认值为sheet=1。\n\n\n\nhaven包提供了从各种统计包导入数据的功能。示例采用的CGSS2015子数据集\n\nlibrary(haven)\n\n# 导入stata数据\n#cgss2017 &lt;- read_dta(\"cgss2017.dta\")\n\n# 导入SPSS数据\ncgss &lt;- read_sav(\"cgss2015subset.sav\")\n\n# 导入SAS数据\n#cgss2017 &lt;- read_sas(\"cgss2017.sas7bdat\")\n\n\n\n\n\n数据清理是数据分析中最耗时的任务。以下列出了最重要的步骤。虽然方法有很多，但使用dplyr和tidyr包是最快捷、最容易学习的方法之一。\n\n\n\n包\n函数\n用途\n\n\n\n\ndplyr\nselect\n选择变量/列\n\n\ndplyr\nfilter\n选择观察值/行\n\n\ndplyr\nmutate\n转换或重新编码变量\n\n\ndplyr\nsummarize\n汇总数据\n\n\ndplyr\ngroup_by\n识别需要进一步处理的子组\n\n\ntidyr\ngather\n将宽格式数据集转换为长格式\n\n\ntidyr\nspread\n将长格式数据集转换为宽格式\n\n\n\n\n\n该select函数从数据集选取指定的变量或列。\n\nlibrary(dplyr)\n\n# 从数据框cgss中选择变量id，sex和age构成新数据集\nnewdata &lt;- select(cgss, id, sex, age)\n\n# 选择变量id以及hp1与hp4之间的所有变量 \nnewdata &lt;- select(cgss, id, hp1:hp4)\n\n# 选择除了edu和lnincome以外的所有变量\nnewdata &lt;- select(cgss, -edu, -lnincome)\n\n\n\n\n该filter函数选择符合特定条件的观测（行）。可以使用&(AND) 和|(OR) 逻辑符号组合多个条件。\n\nlibrary(dplyr)\n\n# 为了方便展示，将带有值标签的变量转换为因子(使用值标签作为取值)   \ncgss &lt;- cgss %&gt;% mutate(across(where(is.labelled), as_factor))\n\n# 选择女性观测\nnewdata &lt;- filter(cgss, sex == \"女\")\n\n# 选择来自山东的女性\nnewdata &lt;- filter(cgss, sex == \"女\" & province == \"山东省\")\n\n# 选择来自东北三省的观测\nnewdata &lt;- filter(cgss, \n                  province == \"辽宁省\" | \n                  province == \"吉林省\" | \n                  province == \"黑龙江省\")\n\n# 更简洁的代码\nnewdata &lt;- filter(cgss, \n                  province %in% \n                    c(\"辽宁省\", \"吉林省\", \"黑龙江省\"))\n\n\n\n\nmutate函数可以创建新变量或转换现有变量。\n\nlibrary(dplyr)\n\n# 将身高从厘米转化成英寸，将体重从斤转化成磅 \nnewdata &lt;- mutate(cgss, \n                  height = height * 0.394,\n                  weight   = (weight/2) * 2.205)\n\n# 如果收入高于8万元，则变量incomecat取值为\"高\"，否则取值为\"低\"\n\nnewdata &lt;- mutate(cgss, \n                  incomecat = ifelse(income &gt; 80000, \n                                     \"高\", \n                                     \"低\"))\n                  \n# 将教育程度不是初中、普通高中、中专的转化为其他类别\nnewdata &lt;- mutate(cgss, \n                  edu = ifelse(edu %in% \n                                     c(\"初中\", \"普通高中\", \"中专\"),\n                                     edu,\n                                     \"其他\"))\n                  \n# 将身高大于200或小于75设定为缺失值\nnewdata &lt;- mutate(cgss, \n                  height = ifelse(height &lt; 75 | height &gt; 200,\n                                     NA,\n                                     height))\n\n\n\n\nsummarize函数可用于描述性统计的数据汇总。可与by_group函数结合使用，用于按组计算统计值。na.rm=TRUE选项用于在计算平均值之前删除缺失值。\n\nlibrary(dplyr)\n\n# 计算身高和体重的平均值\nnewdata &lt;- summarize(cgss, \n                     mean_ht = mean(height, na.rm=TRUE), \n                     mean_weight = mean(weight, na.rm=TRUE))\nnewdata\n\n# A tibble: 1 × 2\n  mean_ht mean_weight\n    &lt;dbl&gt;       &lt;dbl&gt;\n1    164.        122.\n\n# 分性别组计算身高和体重的平均值\nnewdata &lt;- group_by(cgss, sex)\nnewdata &lt;- summarize(newdata, \n                     mean_ht = mean(height, na.rm=TRUE), \n                     mean_wt = mean(weight, na.rm=TRUE))\nnewdata\n\n# A tibble: 2 × 3\n  sex   mean_ht mean_wt\n  &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 男       170.    133.\n2 女       159.    113.\n\n\n\n\n\ndplyr和tidyr软件包允许使用管道运算符以紧凑的格式编写代码%&gt;%，运算符%&gt;%将左边的结果传递给右边的函数的第一个参数。例如：\n\nlibrary(dplyr)\n\nnewdata &lt;- filter(cgss, \n                  sex == \"女\")\nnewdata &lt;- group_by(newdata, province)\nnewdata &lt;- summarize(newdata, \n                     mean_ht = mean(height, na.rm = TRUE))\n\n# 采用管道操作符更简洁，也更符合人类思维\nnewdata &lt;- cgss %&gt;%\n  filter(sex == \"女\") %&gt;%\n  group_by(province) %&gt;%\n  summarize(mean_ht = mean(height, na.rm = TRUE))\n\n\n\n\n在 R 中，日期值以字符的形式输入。例如，记录 3 个人出生日期的简单数据集。\n\ndf &lt;- data.frame(\n  dob = c(\"11/10/1963\", \"Jan-23-91\", \"12:1:2001\")\n)\n\nstr(df) \n\n'data.frame':   3 obs. of  1 variable:\n $ dob: chr  \"11/10/1963\" \"Jan-23-91\" \"12:1:2001\"\n\n\n将字符变量转换为日期变量的方法有很多。最简单的方法之一是使用lubridate包中提供的函数。这些函数包括ymd、dmy和 ，mdy分别用于导入年-月-日、日-月-年和月-日-年的格式。\n\nlibrary(lubridate)\n# 将dob变量值从字符转化为日期型数据\ndf$dob &lt;- mdy(df$dob)\nstr(df)\n\n'data.frame':   3 obs. of  1 variable:\n $ dob: Date, format: \"1963-11-10\" \"1991-01-23\" ...\n\n\n这些值在R的内部记录为自 1970 年 1 月 1日以来的天数，可以方便地执行日期运算，提取日期元素（月、日、年），重新格式化（例如，1963年10月11日）。 日期变量对于制作时间相关图表非常重要。\n\n\n\n有些图表要求数据为宽格式，而有些图表则要求数据为长格式，示例如下。\n将宽数据集转换为长数据集\n\nlibrary(tidyr)\nwide_data &lt;- data.frame(id = c(\"01\", \"02\", \"03\"),\n                        name = c(\"张三\", \"李四\", \"王五\"),\n                        sex = c(\"男\", \"男\", \"女\"),\n                        height = c(70, 72, 62),\n                        weight = c(180, 195, 130))\nknitr::kable(wide_data, caption = \"宽数据\")\n\n\n宽数据\n\n\nid\nname\nsex\nheight\nweight\n\n\n\n\n01\n张三\n男\n70\n180\n\n\n02\n李四\n男\n72\n195\n\n\n03\n王五\n女\n62\n130\n\n\n\n\nlong_data &lt;- pivot_longer(wide_data,\n                          cols = c(\"height\", \"weight\"),\n                          names_to = \"variable\", \n                          values_to =\"value\")\n\n将长数据转化为宽数据\n\nlibrary(tidyr)\nknitr::kable(long_data, caption = \"长数据\")\n\n\n长数据\n\n\nid\nname\nsex\nvariable\nvalue\n\n\n\n\n01\n张三\n男\nheight\n70\n\n\n01\n张三\n男\nweight\n180\n\n\n02\n李四\n男\nheight\n72\n\n\n02\n李四\n男\nweight\n195\n\n\n03\n王五\n女\nheight\n62\n\n\n03\n王五\n女\nweight\n130\n\n\n\n\nwide_data &lt;- pivot_wider(long_data,\n                         names_from = \"variable\",\n                         values_from = \"value\")\n\n\n\n\n真实数据很可能包含缺失值。处理缺失数据有三种基本方法：特征选择、列删除和插补。ggplot2包中的msleep数据集描述了哺乳动物的睡眠习惯，并且在多个变量上存在缺失值。\n\n特征选择 在特征选择中，可以删除包含太多缺失值的变量（列）。\n\n\ndata(msleep, package=\"ggplot2\")\n\n# 每个变量中缺失值的比例\npctmiss &lt;- colSums(is.na(msleep))/nrow(msleep)\nround(pctmiss, 2)\n\n        name        genus         vore        order conservation  sleep_total \n        0.00         0.00         0.08         0.00         0.35         0.00 \n   sleep_rem  sleep_cycle        awake      brainwt       bodywt \n        0.27         0.61         0.00         0.33         0.00 \n\n\n61% 的 sleep_cycle 值缺失。可以决定将其删除。\n\n按列删除\n整行删除包含缺失值的观测。\n\n\nnewdata &lt;- select(msleep, genus, vore, conservation)\nnewdata &lt;- na.omit(newdata)\n\n\n补值\n插补涉及用“合理”的猜测值（假设缺失值不存在时的值）来替换缺失值。有几种方法，详见VIM、mice、Amelia和missForest等包。这里将使用VIMkNN()包中的函数，用插补值替换缺失值。\n\n\n# 用5个最近邻的值插补缺失值\nlibrary(VIM)\nnewdata &lt;- kNN(msleep, k=5)\n\n基本上，对于每个有缺失值的案例，都会选择k个最相似的、没有缺失值的案例。如果缺失值为数值型，则使用这k 个案例的中位数作为插补值。如果缺失值为类别值，则使用这k 个案例中出现频率最高的值。该过程会迭代所有观测和变量，直到结果收敛（趋于稳定）。\n重要提示：缺失值可能会对研究结果造成偏差（有时甚至非常严重）。如果有大量缺失数据，在删除观测或填补缺失值之前，要慎重考虑其合理性。"
  },
  {
    "objectID": "dataanlyr/visualization/visualization.html#ggplot2-软件包简介",
    "href": "dataanlyr/visualization/visualization.html#ggplot2-软件包简介",
    "title": "数据可视化",
    "section": "ggplot2 软件包简介",
    "text": "ggplot2 软件包简介\n本部分简要概述了ggplot2包的工作原理，该软件包具有强大的绘图功能。\n\n示例\nggplot2包中的函数可以分层分步构建图形，先从简单的图形开始，逐个添加其他元素，从而构建复杂的图形。\n在使用 ggplot2 制图时，只需使用ggplot和geom两个函数。其他函数是可选的，可以按需求添加。\n\n\nggplot\nggplot函数指定要使用的数据框、变量到图形视觉属性的映射。这些映射位于aes函数内部。\n\nlibrary(ggplot2)\n# 为了制图美观，剔除收入过高的人群\ncgss &lt;- cgss %&gt;% filter(income &lt;=200000)\n\n# 确定数据集和映射\nggplot(data = cgss,\n       mapping = aes(x = eduyear, y = income))\n\n\n\n\n\n\n\n\n图表是空的,因为只是指定映射，教育年限变量映射到x轴，收入映射到y轴，但还没有指定图上要显示的内容。\n\n\n几何对象\n几何对象 (Geoms) 是可以放置在图上的几何形状（点、线、条形等），它们是以geom_开头的函数。先看看用geom_point函数添加点，创建散点图。\n在 ggplot2 制图时，可以使用+号（容易忘记添加）将不同的函数链接在一起，一步一步构建复杂的图。\n\nggplot(data = cgss,\n       mapping = aes(x = eduyear, y = income)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n几何对象函数中可以指定参数（选项）控制其外观。下面的geom_point函数的选项包括color、size和alpha，分别控制点的颜色、大小和透明度。透明度的范围从 0（完全透明）到 1（完全不透明）。增加透明度有助于可视化重叠点密度。\n\n\nggplot(data = cgss,\n       mapping = aes(x = eduyear, y = income)) +\n  geom_point(color = \"cornflowerblue\",\n             alpha = .7,\n             size = 2)\n\n\n\n\n\n\n\n\n\n添加一条最佳拟合线，可以使用geom_smooth函数来实现。选项控制线的类型（线性、二次、非参数）、线的粗细、线的颜色以及是否包含置信区间。下面添加线性回归（method = lm）线（其中lm代表线性回归模型），默认带置信区间。\n\n\nggplot(data = cgss,\n       mapping = aes(x = eduyear, y = income)) +\n  geom_point(color = \"cornflowerblue\",\n             alpha = .5,\n             size = 2) +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n分组\n除了将变量映射到x轴和y轴（只能有两个变量维度）之外，还可以将变量映射到几何对象的颜色、形状、大小、透明度和其他视觉特征，体现多个变量维度，实现将观测更多的状态叠加到单个图形中。\n\n# 采用颜色体现性别差异\nggplot(data = cgss,\n       mapping = aes(x = eduyear, \n                     y = income,\n                     color = sex)) +\n  geom_point(alpha = .5,\n             size = 2) +\n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              linewidth = 1.5)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\ncolor = sex选项位于aes函数中，将变量sex映射到图形的视觉特征color上。\n\n\n刻度\n\n先解决中文支持的问题\n\n\nlibrary(showtext)\nlibrary(sysfonts)\n\n# 加载黑体，最好找到字体的路径再加载 mac可用fc-list :lang=zh命令查看\nfont_add(\"Heiti SC\", \"/System/Library/Fonts/STHeiti Medium.ttc\")  \n\n# 启用 showtext 自动渲染\nshowtext_auto()\n\n刻度控制变量通过以scale_开头的刻度函数来修改x轴和y轴的缩放比例和单位，以及图例所使用的颜色等刻度视觉特征。\n\nggplot(data = cgss,\n       mapping = aes(x = eduyear, \n                     y = income,\n                     color = sex)) +\n  geom_point(alpha = .5,\n             size = 2) +\n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              linewidth = 1.5) +\n  scale_x_continuous(breaks = seq(0, 20, 5)) +\n  scale_y_continuous(breaks = seq(0, 200000, 50000),\n                     label = scales::label_currency(prefix = \"￥\",suffix = \"元\")) +\n  scale_color_manual(values = c(\"indianred3\", \n                                \"cornflowerblue\"))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n切片\n切片会为给定变量（或成对的变量）的每个级别生成一个图表，方便对比。切片以facet_开头的函数创建。下面采用region变量的取值进行城乡切片。\n\nggplot(data = cgss,\n       mapping = aes(x = eduyear, \n                     y = income,\n                     color = sex)) +\n  geom_point(alpha = .5,\n             size = 2) +\n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              linewidth = 1.5) +\n  scale_x_continuous(breaks = seq(0, 20, 5)) +\n  scale_y_continuous(breaks = seq(0, 200000, 50000),\n                     label = scales::label_currency(prefix = \"￥\")) +\n  scale_color_manual(values = c(\"indianred3\", \n                                \"cornflowerblue\")) +\n  facet_wrap(~region)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n标签\n信息丰富的标签可以帮助图表的理解。labs功能为轴和图例提供自定义标签，还可以添加自定义标题、副标题和说明等。\n\nggplot(data = cgss,\n       mapping = aes(x = eduyear, \n                     y = income,\n                     color = sex)) +\n  geom_point(alpha = .5,\n             size = 2) +\n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              linewidth = 1.5) +\nscale_x_continuous(breaks = seq(0, 20, 5)) +\n  scale_y_continuous(breaks = seq(0, 200000, 50000),\n                     label = scales::label_currency(prefix = \"￥\")) +\n  scale_color_manual(values = c(\"indianred3\", \n                                \"cornflowerblue\")) +\n  facet_wrap(~region) +\n  # 增加制图标题，坐标标签等\n  labs(title = \"教育回报的影响因素\",\n       subtitle = \"CGSS2017\",\n       caption = \"来源：CGSS网站\",\n       x = \" 教育年限（年）\",\n       y = \"个人收入（元）\",\n       color = \"城乡差别\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n主题\n最后，可以使用以theme_开头的主题函数来微调图表的外观。主题函数控制图表的背景颜色、字体、网格线、图例位置以及其他与数据无关的功能。下面是简洁主题的运用。\n\nggplot(data = cgss,\n       mapping = aes(x = eduyear, \n                     y = income,\n                     color = sex)) +\n  geom_point(alpha = .5,\n             size = 2) +\n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              linewidth = 1.5) +\nscale_x_continuous(breaks = seq(0, 20, 5)) +\n  scale_y_continuous(breaks = seq(0, 200000, 50000),\n                     label = scales::label_currency(prefix = \"￥\")) +\n  scale_color_manual(values = c(\"indianred3\", \n                                \"cornflowerblue\")) +\n  facet_wrap(~region) +\n  # 增加制图标题，坐标标签等\n  labs(title = \"教育回报的影响因素\",\n       subtitle = \"CGSS2017\",\n       caption = \"来源：CGSS网站\",\n       x = \" 教育年限（年）\",\n       y = \"个人收入（元）\",\n       color = \"城乡差别\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n图表展现的结果：\n\n教育年限与个人收入之间存在正向线性关系。但是男性的教育回报更高。\n\n在较高教育水平上，城市男性的个人收入更高。\n\n性别与城乡之间可能存在交互作用。城市似乎缩小了性别之间的教育回报的差异。\n\n在较高教育水平上，两个性别的个人收入都存在一些非常高的异常值。\n\n这些仅仅是初步发现，样本量有限，并且未采用统计检验来评估差异是否由偶然变异造成。\n\n\n\n\n放置data和mapping选项\n使用 ggplot2 创建的绘图始终以ggplot函数开头。在上面的示例中，data和mapping选项位于此函数中。在这种情况下，它们适用于其geom_后的每个函数。但是，也可以将这些选项直接放在geom函数中。在此情况下，它们仅适用于该特定的几何对象。\n\n# 颜色映射放在ggplot函数中\nggplot(cgss,\n       aes(x = eduyear, \n           y = income,\n           color = sex)) +\n  geom_point(alpha = .5,\n             size = 2) +\n  geom_smooth(method = \"lm\",\n              se = FALSE, \n              linewidth = 1.5)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n颜色映射适用于点和趋势线。\n\n# 颜色映射放在geom_point函数中\nggplot(cgss,\n       aes(x = eduyear, \n           y = income)) +\n  geom_point(aes(color = sex),\n             alpha = .5,\n             size = 2) +\n  geom_smooth(method = \"lm\",\n              se = FALSE, \n              linewidth = 1.5)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n颜色映射只适用于点的分类，不适用趋势线，所以只有一条趋势线。\n\n\n图作为对象\nggplot2 图表可以保存为 R 对象（如同数据框），进一步操作，然后绘制或存取。\n\n# 创建散点图并保存为对象\nmyplot &lt;- ggplot(data = cgss,\n                  aes(x = eduyear, y = income)) +\n             geom_point()\n\n# 绘制\nmyplot\n\n\n\n\n\n\n\n# 更改点的大小和颜色并绘制\n\nmyplot &lt;- myplot + geom_point(size = 2, color = \"blue\")\nmyplot\n\n\n\n\n\n\n\n# 添加标题和趋势线后绘制，但不保存到对象\nmyplot + geom_smooth(method = \"lm\") +\n  labs(title = \"示例图\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# 采用黑白主题绘制，不保存\nmyplot + theme_bw()"
  },
  {
    "objectID": "dataanlyr/visualization/visualization.html#单变量图",
    "href": "dataanlyr/visualization/visualization.html#单变量图",
    "title": "数据可视化",
    "section": "单变量图",
    "text": "单变量图\n\n定类变量\n单个定类变量分布通常用条形图来展示。\n\n条形图\n\nggplot(cgss, aes(x = health)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n\n可以添加选项来修改条形填充和边框的颜色、绘图标签和标题等。\n\n\nggplot(cgss, aes(x=health)) + \n  geom_bar(fill = \"cornflowerblue\", \n           color=\"black\") +\n  labs(x = \"健康状况\", \n       y = \"频次\", \n       title = \"调查人群健康状况\")\n\n\n\n\n\n\n\n\n\n百分比条形图，代码aes(x=health)实际上是aes(x = health, y = after_stat(count))，其中count是一个特殊变量，表示每个类别中的频次。可以通过给y明确指定如何计算百分比。\n\n\nggplot(cgss, \n       aes(x = health, y = after_stat(count/sum(count)))) + \n  geom_bar() +\n  labs(x = \"健康状况\", \n       y = \"频率\", \n       title = \"调查人群健康状况\") +\n# 采用刻度函数添加百分比符号\n    scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\n\n\n\n排序类别\n某些情况下希望按频次对条形图进行排序，可以使用reorder函数按频次对类别进行排序。选项stat=“identity”告诉绘图函数不要计数，因为它们是直接提供的。\n\n\n# 计算健康状态每个类别的频次\nplotdata &lt;- cgss %&gt;%\n count(health)\n#使用新数据集来创建图表\nggplot(plotdata, \n       aes(x = reorder(health, n), y = n)) + \n  geom_bar(stat=\"identity\") +\n  labs(x = \"健康状况\", \n       y = \"频次\", \n       title  = \"调查人群健康状况\")\n\n\n\n\n\n\n\n\n图表条形按升序排列。使用reorder(health, -n)可按降序排列。\n\n标签，带有数字标签的条形图\n\n\nggplot(plotdata, \n       aes(x = health, y = n)) + \n  geom_bar(stat=\"identity\") +\n# 添加数值标签\n  geom_text(aes(label = n), vjust=-0.5) +\n  labs(x = \"健康状况\", \n       y = \"频次\", \n       title  = \"调查人群健康状况\")\n\n\n\n\n\n\n\n\n此处geom_text添加标签，并用vjust控制垂直对齐。\n\nplotdata &lt;- cgss %&gt;%\n  count(health) %&gt;%\n  mutate(pct = n / sum(n),\n         pctlabel = paste0(round(pct*100), \"%\"))\n\nggplot(plotdata, \n       aes(x = reorder(health, -pct), y = pct)) + \n  geom_bar(stat=\"identity\", fill=\"indianred3\", color=\"black\") +\n  geom_text(aes(label = pctlabel), vjust=-0.25) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"健康状况\", \n       y = \"频率\", \n       title  = \"调查人群健康状况\")\n\n\n\n\n\n\n\n\n\n重叠标签的调整\n如果类别过多或标签很长，类别标签可能会重叠，可以采用旋转坐标解决。\n\n\nggplot(cgss, aes(x = province)) + \n  geom_bar() +\n  labs(x = \"省份\",\n       y = \"频次\",\n       title = \"调查人群分布\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n此外，可以ggpieggpie包中的函数创建饼图；结合ggplot2，使用treemapify包创建树状图，使用waffle包创建华夫饼图。\n\nlibrary(ggpie)\nggpie(cgss, group_key = \"health\", count_type = \"full\", label_info = \"ratio\")\n\n\n\n\n\n\n\nlibrary(treemapify)\n\nplotdata &lt;- cgss %&gt;%\n  count(edu)\n\nggplot(plotdata, \n       aes(fill = edu, area = n)) +\n  geom_treemap() + \n  labs(title = \"调查人群教育程度\")\n\n\n\n\n\n\n\nlibrary(waffle)\nggplot(plotdata, aes(fill = edu, values=n)) +\n  geom_waffle(na.rm=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n尺度变量\n单个尺度变量的分布通常用直方图、核密度图或点图来绘制。\n\n直方图\n\nggplot(cgss, aes(x = age)) +\n  geom_histogram() + \n  labs(title = \"调查人群年龄分布\",\n       x = \"年龄\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n可以使用两个选项修改直方图颜色，fill为条形填充颜色，color为条形周围的边框颜色。\n\n\nggplot(cgss, aes(x = age)) +\n  geom_histogram(fill = \"cornflowerblue\", \n                 color = \"white\") + \n  labs(title = \"调查人群年龄分布\",\n       x = \"年龄\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n分组和组距\n直方图最重要的选项是分组（bins），它控制将尺度变量划分成的多个分组（即图中的条形数）。默认值为 30，但尝试较小或较大的数字有助于更好地了解分布的形状。还可以指定组距binwidth间接控制分组个数。\n\n\nggplot(cgss, aes(x = age)) +\n  geom_histogram(fill = \"cornflowerblue\", \n                 color = \"white\",\n                 bins = 20) + \n  labs(title = \"调查人群年龄分布\",\n       x = \"年龄\")\n\n\n\n\n\n\n\n\n与条形图一样，y轴可以表示计数或总数的百分比。\n\nlibrary(scales)\n\n\n载入程序包：'scales'\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nggplot(cgss, \n       aes(x = age, y= after_stat(count/sum(count)))) +\n  geom_histogram(fill = \"cornflowerblue\", \n                 color = \"white\", \n                 binwidth = 5) + \n  labs(title=\"调查人群年龄分布\", \n       y = \"比例\",\n       x = \"年龄\") +\n  scale_y_continuous(labels = percent)\n\n\n\n\n\n\n\n\n\n\n核密度图\n从技术上讲，核密度估计是一种非参数方法，用于估计连续随机变量的概率密度函数。绘制一个平滑的直方图，其中曲线下的面积等于1。\n\nggplot(cgss, aes(x = age)) +\n  geom_density() + \n  labs(title = \"调查人群年龄分布\")\n\n\n\n\n\n\n\n\n\n填充颜色\n\n\nggplot(cgss, aes(x = age)) +\n  geom_density(fill = \"indianred3\") + \n  labs(title = \"调查人群年龄分布\")\n\n\n\n\n\n\n\n\n\n平滑参数\n平滑度由带宽参数bw控制。值越大，平滑度越高，值越小，平滑度越低。\n\n\nggplot(cgss, aes(x = age)) +\n  geom_density(fill = \"deepskyblue\", \n               bw = 1) + \n  labs(title = \"调查人群年龄分布\",\n       subtitle = \"bandwidth = 1\")\n\n\n\n\n\n\n\n\n\n\n点状图\n直方图的一种替代方案是点图。同样，尺度变量被分成多个组，但每个观测值都用一个点来表示，而不是用汇总条形图。默认情况下，点的宽度与分组的宽度相对应，并且点是堆叠的，每个点代表一个观测值。当观测值数量较少（例如少于 150 个）时，这种方法效果比较好。\n\nggplot(cgss, aes(x = age)) +\n  geom_dotplot(binwidth = 1) + \n  labs(title = \"调查人群年龄分布\",\n       y = \"比例\",\n       x = \"年龄\")\n\n\n\n\n\n\n\n\nfill和选项color可分别用于指定每个点的填充色和边框色。\n\nggplot(cgss, aes(x = age)) +\n  geom_dotplot(binwidth = 1,\n               fill = \"gold\", \n               color=\"black\") + \n  labs(title = \"调查人群年龄分布\",\n       y = \"比例\",\n       x = \"年龄\")"
  },
  {
    "objectID": "dataanlyr/visualization/visualization.html#双变量图",
    "href": "dataanlyr/visualization/visualization.html#双变量图",
    "title": "数据可视化",
    "section": "双变量图",
    "text": "双变量图\n\n两个类别变量\n绘制两个类别变量之间的关系时，通常使用堆积条形图、分组条形图或分段条形图。\n\n堆积条形图\n\nlibrary(ggplot2)\n\nggplot(cgss, aes(x =region , fill = health)) + \n  geom_bar(position = \"stack\")\n\n\n\n\n\n\n\n\n\n\n分类条形图\n分类条形图将第二个类别变量的条形并排放置。\n\nggplot(cgss, aes(x = region, fill = health)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n分段条形图\n分段条形图是一种堆叠条形图，其中每个条形代表 100%。\n\nggplot(cgss, aes(x = region, fill = health)) + \n  geom_bar(position = \"fill\") +\n  labs(y = \"比例\")\n\n\n\n\n\n\n\n\n\n\n改善颜色和标签\n\nfactor修改类别变量的地区变量顺序以及健康状态变量的顺序和标签\nscale_y_continuous修改 y 轴刻度标记标签\nlabs提供标题并更改 x 轴和 y 轴的标签以及图例\nscale_fill_brewer更改填充颜色方案\ntheme_minimal删除灰色背景并改变网格颜色\n\n\nggplot(cgss, aes(x = region, fill = health)) + \n  geom_bar(position = \"fill\") +\n  scale_y_continuous(breaks = seq(0, 1, .2), \n                     label = scales::percent) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(y = \"比例\", \n       fill=\"健康状态\",\n       x = \"地区\",\n       title = \"调查人群分地区健康状态\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n两个尺度变量\n两个尺度变量之间的关系通常使用散点图和折线图来显示。\n\n散点图\n\nggplot(cgss, \n       aes(x = age, y = income)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\ngeom_point函数的选项\n\ncolor- 点颜色\n\nsize- 点大小\n\nshape- 点的形状\n\nalpha- 点透明度。透明度范围从 0（透明）到 1（不透明）。\n\n\n函数scale_x_continuous和分别控制x轴和yscale_y_continuous轴上的缩放。\n\n\nggplot(cgss, \n       aes(x = age, y = income)) +\n  geom_point(color=\"cornflowerblue\", \n             size = 2, \n             alpha=.8) +\n  scale_y_continuous(label = scales::label_currency(prefix=\"￥\",suffix = \"元\"), \n                     limits = c(0, 200000)) +\n  scale_x_continuous(breaks = seq(0, 90, 10), \n                     limits=c(15, 90)) + \n  labs(x = \"年龄\",\n       y = \"\",\n       title = \"年龄与收入的关系\",\n       subtitle = \"基于CGSS2015抽样数据\")\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n添加最佳拟合线，支持多种类型的拟合线，包括线性、多项式和非参数 (loess)。默认情况下，显示这些线的 95% 置信限度。\n\n\nggplot(cgss, aes(x = age, y = income)) +\n  geom_point(color= \"steelblue\") +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n很奇怪的，收入会随着年龄的增加而降低。然而，在30-40岁间似乎出现了一个峰值，说明35岁之前可能是上升的。直线无法反映这种非线性效应，曲线更适合。通常使用二次（一次弯曲）或三次（两次弯曲）线。很少需要使用高阶（&gt;3）多项式。\n\nggplot(cgss, aes(x = age, y = income)) +\n  geom_point(color= \"steelblue\") +\n  geom_smooth(method = \"lm\", \n              formula = y ~ poly(x, 2), \n              color = \"indianred3\")\n\n\n\n\n\n\n\n\n很可惜，二次曲线也没有证据反映这种变化，平滑的非参数拟合线也许可以更好地描述这种关系。默认的非参数拟合是Loess线，代表局部加权散点图平滑。\n\nggplot(cgss, aes(x = age, y = income)) +\n  geom_point(color= \"steelblue\") +\n  geom_smooth(color = \"tomato\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n类别与尺度变量\n针对类别变量和尺度变量之间的关系，可以使用汇总统计数据的条形图、分组核密度图、并排箱线图、并排小提琴图、均值/标准差图、脊线图和克利夫兰图。\n\n条形图（汇总统计数据）\n可以使用条形图显示尺度变量在分类变量各个水平上的汇总统计数据（例如，均值或中位数）。\n\nlibrary(dplyr)\nplotdata &lt;- cgss %&gt;%\n  group_by(health) %&gt;%\n  summarize(mean_income = mean(income))\n\nggplot(plotdata, aes(x = health, y = mean_income)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\n\nfactor函数可以修改每个健康状态的标签\n\nscale_y_continuous函数可以改进 y 轴的标签\n\ngeom_text函数可以为每个条形图添加平均值\n\n\nlibrary(scales)\nggplot(plotdata, \n       aes(x = health, y = mean_income)) +\n  geom_bar(stat = \"identity\", \n           fill = \"cornflowerblue\") +\n  geom_text(aes(label = label_currency(accuracy = 1, prefix = \"￥\")(mean_income)), \n            vjust = -0.25) +\n  scale_y_continuous(breaks = seq(0, 40000, 5000), \n                     label = label_currency(prefix = \"￥\")) +\n  labs(title = \"各健康水平的平均收入\", \n       subtitle = \"\",\n       x = \"\",\n       y = \"\")\n\n\n\n\n\n\n\n\n条形图的局限性是不显示数据的分布，仅显示每个组的汇总统计数据。分组核密度图在一定程度上纠正了这一缺陷。\n\n\n分组核密度图\n\nggplot(cgss, aes(x = income, fill = health)) +\n  geom_density(alpha = 0.4) +\n  labs(title = \"各健康水平的收入分布\")\n\n\n\n\n\n\n\n\n\n\n箱线图\n\n箱线图显示分布的第 25 个百分位数、中位数和第75 个百分位数。须线（垂直线）可捕捉正态分布的约 99%，超出此范围的观测值则绘制为表示异常值的点。\n箱线图适用于比较数值变量的组（即分类变量的级别）。\n\n\nggplot(cgss, aes(x = health, y = income)) +\n  geom_boxplot() +\n  labs(title = \"各健康水平的收入分布\")\n\n\n\n\n\n\n\n\n\n缺口箱线图提供了一种近似方法来直观地显示组间是否存在差异。虽然这不是正式的检验方法，但如果两个箱线图的缺口不重叠，则有强有力的证据（置信度为 95%）表明两组的中位数存在差异。\n\n\nggplot(cgss, aes(x = health, y = income)) +\n  geom_boxplot(notch = TRUE, \n               fill = \"cornflowerblue\", \n               alpha = .7) +\n  labs(title = \"各健康水平的收入分布\")\n\n\n\n\n\n\n\n\n\n\n小提琴图\n小提琴图与核密度图类似，但是镜像并旋转了 90度。\n\nggplot(cgss, aes(x = health, y = income)) +\n  geom_violin() +\n  labs(title = \"各健康水平的收入分布\")\n\n\n\n\n\n\n\n\n\n组合箱线图和小提琴图\n\n\nggplot(cgss, aes(x = health, y = income)) +\n  geom_violin(fill = \"cornflowerblue\") +\n  geom_boxplot(width = .15, \n               fill = \"orange\",\n               outlier.color = \"orange\",\n               outlier.size = 2) + \n  labs(title = \"各健康水平的收入分布\")\n\n\n\n\n\n\n\n\n有时候需要设置调试geom_boxplot的width参数，确保箱线图与小提琴图契合。\n\n\n山脊线图\n脊线图（也称为 joyplot）显示多个组中尺度变量的分布。它们类似于带有垂直刻面的核密度图，但占用的空间较小。脊线图使用ggridges包创建。\n\nlibrary(ggplot2)\nlibrary(ggridges)\n\nggplot(cgss, \n       aes(x = income, y = health, fill = health)) +\n  geom_density_ridges() + \n  theme_ridges() +\n  labs(\"各健康水平的收入分布\") +\n  theme(legend.position = \"none\")\n\nPicking joint bandwidth of 6990\n\n\n\n\n\n\n\n\n\n\n\n平均值/SEM 图\n\n一种常用的比较数值变量组数据的方法是绘制带误差线的均值图。误差线可以表示标准差、均值的标准误差或置信区间。\n\n\nlibrary(dplyr)\nplotdata &lt;- cgss %&gt;%\n  group_by(health) %&gt;%\n  summarize(n = n(),\n         mean = mean(income),\n         sd = sd(income),\n         se = sd / sqrt(n),\n         ci = qt(0.975, df = n - 1) * sd / sqrt(n))\nggplot(plotdata, \n       aes(x = health, \n           y = mean, \n           group = 1)) +\n  geom_point(size = 3) +\n  geom_line() +\n  geom_errorbar(aes(ymin = mean - se, \n                    ymax = mean + se), \n                width = .1)\n\n\n\n\n\n\n\n\n\n比较不性别和健康水平的收入\n\n\nplotdata &lt;- cgss %&gt;%\n  group_by(health, sex) %&gt;%\n  summarize(n = n(),\n            mean = mean(income),\n            sd = sd(income),\n            se = sd/sqrt(n))\n\n`summarise()` has grouped output by 'health'. You can override using the\n`.groups` argument.\n\n# 按性别绘制均值和标准误\nggplot(plotdata, aes(x = health,\n                     y = mean, \n                     group=sex, \n                     color=sex)) +\n  geom_point(size = 3) +\n  geom_line(linewidth = 1) +\n  geom_errorbar(aes(ymin  =mean - se, \n                    ymax = mean+se), \n                width = .1)\n\n\n\n\n\n\n\n\n\n误差线有重叠，可以稍微避开水平位置来解决。\n\n\npd &lt;- position_dodge(0.2)\nggplot(plotdata, \n       aes(x = health, \n           y = mean, \n           group=sex, \n           color=sex)) +\n  geom_point(position = pd, \n             size = 3) +\n  geom_line(position = pd,\n            linewidth = 1) +\n  geom_errorbar(aes(ymin = mean - se, \n                    ymax = mean + se), \n                width = .1, \n                position= pd)\n\n\n\n\n\n\n\n\n\n美化图表达到出版质量\n\n\npd &lt;- position_dodge(0.2)\nggplot(plotdata, \n       aes(x = health, y = mean, group=sex, color=sex)) +\n  geom_point(position=pd, \n             size=3) +\n  geom_line(position=pd, \n            linewidth = 1) +\n  geom_errorbar(aes(ymin = mean - se, \n                    ymax = mean + se), \n                width = .1, \n                position=pd, \n                size=1) +\n  scale_y_continuous(label = scales::label_currency(prefix = \"￥\")) +\n  scale_color_brewer(palette=\"Set1\") +\n  theme_minimal() +\n  labs(title = \"按性别和健康状况划分的平均收入\",\n       subtitle = \"(均值 +/- 标准误)\",\n       x = \"\", \n       y = \"\",\n       color = \"性别\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n带状图\n\n分组变量和数值变量之间的关系也可以用散点图来显示。\n\n\nggplot(cgss, aes(y = health, x = income)) +\n  geom_point() + \n  labs(title = \"各健康水平的收入分布\")\n\n\n\n\n\n\n\n\n\n点的重叠会使解释变得困难。可以适当将点错开一点距离，更容易看出关系。操作是在每个 y 坐标上添加一个小的随机数，代码是将geom_point替换为geom_jitter。\n\n\nggplot(cgss, aes(y = health, x = income)) +\n  geom_jitter() + \n  labs(title = \"各健康水平的收入分布\")\n\n\n\n\n\n\n\n\n\n使用颜色增加组间的对比\n\n\nlibrary(scales)\nggplot(cgss, \n       aes(y = health, x = income, color = health)) +\n  geom_jitter(alpha = 0.7) + \n  scale_x_continuous(label = label_currency(prefix = \"￥\")) +\n  labs(title = \"各健康水平的收入分布\", \n       subtitle = \"CGSS2015数据\",\n       x = \"\",\n       y = \"\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n可以将箱线图叠加在带状图上\n\n\nlibrary(scales)\nggplot(cgss, \n       aes(x = health, \n           y = income, color = health)) +\n  geom_boxplot(size=1,\n               outlier.shape = 1,\n               outlier.color = \"black\",\n               outlier.size  = 3) +\n  geom_jitter(alpha = 0.5, \n              width=.2) + \n  scale_y_continuous(label = label_currency(prefix = \"￥\")) +\n  labs(title = \"各健康水平的收入分布\", \n       subtitle = \"CGSS2015数据\",\n       x = \"\",\n       y = \"\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nggpol包提供geom_boxjitter函数，可以创建一个混合箱线图（箱线图组合散点图）\n\n\nlibrary(ggpol)\nlibrary(scales)\nggplot(cgss, \n       aes(x = health, \n           y = income, \n           fill=health)) +\n  geom_boxjitter(color=\"black\",\n                 jitter.color = \"darkgrey\",\n                 errorbar.draw = TRUE) +\n  scale_y_continuous(label = label_currency(prefix = \"￥\")) +\n  labs(title = \"各健康水平的收入分布\", \n       subtitle = \"CGSS2015数据\",\n       x = \"\",\n       y = \"\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\nWarning: Using the `size` aesthetic with geom_segment was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\nWarning: Using the `size` aesthetic with geom_crossbar was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n\n\n\n\n\n克利夫兰点图\n\n如果比较数值变量的各个观测值，特别是比较大量数值汇总统计数据时，可以使用克利夫兰图\n\n\nlibrary(dplyr)\nplotdata &lt;- cgss %&gt;%\n  group_by(province) %&gt;%\n  summarize(mean_income = mean(income, na.rm=T))\n\nggplot(plotdata, \n       aes(x= mean_income, y = province)) +\n  geom_point()\n\n\n\n\n\n\n\n# 对数据进行排序会更直观\nggplot(plotdata, aes(x=mean_income, \n                     y=reorder(province, mean_income))) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n美化得到棒棒糖图\n\n\n# Fancy Cleveland plot\nggplot(plotdata, aes(x=mean_income, \n                     y=reorder(province, mean_income))) +\n  geom_point(color=\"blue\", size = 2) +\n  geom_segment(aes(x = 40, \n               xend = mean_income, \n               y = reorder(province, mean_income), \n               yend = reorder(province, mean_income)),\n               color = \"lightgrey\") +\n  labs (x = \"平均收入（元）\",\n        y = \"\",\n        title = \"分地区的平均收入\",\n        subtitle = \"CGSS2015数据\") +\n  theme_minimal() + \n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())"
  },
  {
    "objectID": "dataanlyr/assignment/exercise1.html",
    "href": "dataanlyr/assignment/exercise1.html",
    "title": "课堂练习一",
    "section": "",
    "text": "在中国综合社会调查网站上下载cgss2017的数据（spss版本）、问卷和编码表。\n\n在Rstudio中将数据导入，选择编号（id）、城乡（isurban）、居住省份（s41）、个人全年总收入（A8a）、出生年份（a31）、性别（A2）、教育程度（A7a）、政治面貌（A10）、工作年限（A59c）、工作经历及状态（A58）、父亲教育程度（A89b）、母亲教育程度（A90b）、父亲的工作单位类型（A89g）、母亲的工作单位类型（A90g）等变量构成新的工作数据集cgssincome2017，再根据城乡（isurban）变量选取城市的被调查对象（取值为1）并保存。\n\n新建一个项目，载入cgssincome2017数据集，将出生年份转化为年龄（2017年），教育程度（本人、父母）重新进行编码转化为教育年限，政治面貌重新进行编码转化为是否党员。并且设定所有的缺失值为NA（CGSS往年设定为-8是无法回答；-3拒绝回答；-2是不知道；-1是不适用，请自行确认2017年数据是否依然如此！），最后成行删除掉具有缺失值的观测（建议使用cgssincome &lt;- na.omit(cgssincome)）。\n\n注意：以上步骤如果采用菜单方式载入，haven包会默认给变量添加值标签，应采用zap_labels函数去除值标签，否则有些函数无法使用，如cgssincome &lt;- zap_labels(cgssincome)\n\n\n\n\n将性别、是否党员转化为类别变量（因子factor函数），计算男女党员的交叉表; 制作收入的直方图(注意用参数breaks多分割一些区间)，收入是正态分布吗？如何让它变得更像正态分布？制作收入的箱线图，选取收入小于15万的观测再试试看；制作男女收入对比箱线图，收入存在性别差异吗？\n\n计算在置信水平为95%的条件下，居民收入的置信区间。\n\n假设有人提出2017年全国居民年平均收入为5万块，那么在0.05的显著性水平下，cgss的样本结果和这一提法一致吗？\n\n计算男性与女性的平均收入，在95%的置信水平下，收入是否存在性别差异？\n\n检验被调查者的父亲与母亲教育程度是否有差异？\n\n第一部分整理后数据，供参考，如无法独立完成数据获取与预处理，可以载入后进行初步分析"
  },
  {
    "objectID": "dataanlyr/assignment/exercise1.html#课堂练习",
    "href": "dataanlyr/assignment/exercise1.html#课堂练习",
    "title": "课堂练习一",
    "section": "",
    "text": "在中国综合社会调查网站上下载cgss2017的数据（spss版本）、问卷和编码表。\n\n在Rstudio中将数据导入，选择编号（id）、城乡（isurban）、居住省份（s41）、个人全年总收入（A8a）、出生年份（a31）、性别（A2）、教育程度（A7a）、政治面貌（A10）、工作年限（A59c）、工作经历及状态（A58）、父亲教育程度（A89b）、母亲教育程度（A90b）、父亲的工作单位类型（A89g）、母亲的工作单位类型（A90g）等变量构成新的工作数据集cgssincome2017，再根据城乡（isurban）变量选取城市的被调查对象（取值为1）并保存。\n\n新建一个项目，载入cgssincome2017数据集，将出生年份转化为年龄（2017年），教育程度（本人、父母）重新进行编码转化为教育年限，政治面貌重新进行编码转化为是否党员。并且设定所有的缺失值为NA（CGSS往年设定为-8是无法回答；-3拒绝回答；-2是不知道；-1是不适用，请自行确认2017年数据是否依然如此！），最后成行删除掉具有缺失值的观测（建议使用cgssincome &lt;- na.omit(cgssincome)）。\n\n注意：以上步骤如果采用菜单方式载入，haven包会默认给变量添加值标签，应采用zap_labels函数去除值标签，否则有些函数无法使用，如cgssincome &lt;- zap_labels(cgssincome)\n\n\n\n\n将性别、是否党员转化为类别变量（因子factor函数），计算男女党员的交叉表; 制作收入的直方图(注意用参数breaks多分割一些区间)，收入是正态分布吗？如何让它变得更像正态分布？制作收入的箱线图，选取收入小于15万的观测再试试看；制作男女收入对比箱线图，收入存在性别差异吗？\n\n计算在置信水平为95%的条件下，居民收入的置信区间。\n\n假设有人提出2017年全国居民年平均收入为5万块，那么在0.05的显著性水平下，cgss的样本结果和这一提法一致吗？\n\n计算男性与女性的平均收入，在95%的置信水平下，收入是否存在性别差异？\n\n检验被调查者的父亲与母亲教育程度是否有差异？\n\n第一部分整理后数据，供参考，如无法独立完成数据获取与预处理，可以载入后进行初步分析"
  },
  {
    "objectID": "dataanlyr/lecture1/introduction.html",
    "href": "dataanlyr/lecture1/introduction.html",
    "title": "数据分析与R语言应用课程介绍",
    "section": "",
    "text": "课程内容\n本课程是在概率统计、社会统计与定量研究方法等课程的基础上，介绍高级数据分析技术的原理，同时以R语言为分析工具，介绍相关技术的操作。通过本课程的学习帮助学生掌握R语言软件的基本操作，初步了解广义线性模型、多层线性模型、结构方程、因果推断等相关分析技术。\n课程目标\n\n熟悉R语言的软件平台，了解基本操作，包括获取导入数据、绘制统计图形。\n\n了解R语言中数据的基本管理与数据清洗操作。\n\n介绍一般统计方法的操作，包括多元线性回归、logistic回归、因子分析等。\n\n介绍高级统计方法的基本原理和操作，包括广义线性模型、多层线性模型、结构方程、因果推断等。"
  },
  {
    "objectID": "dataanlyr/lecture1/introduction.html#课程介绍",
    "href": "dataanlyr/lecture1/introduction.html#课程介绍",
    "title": "数据分析与R语言应用课程介绍",
    "section": "",
    "text": "课程内容\n本课程是在概率统计、社会统计与定量研究方法等课程的基础上，介绍高级数据分析技术的原理，同时以R语言为分析工具，介绍相关技术的操作。通过本课程的学习帮助学生掌握R语言软件的基本操作，初步了解广义线性模型、多层线性模型、结构方程、因果推断等相关分析技术。\n课程目标\n\n熟悉R语言的软件平台，了解基本操作，包括获取导入数据、绘制统计图形。\n\n了解R语言中数据的基本管理与数据清洗操作。\n\n介绍一般统计方法的操作，包括多元线性回归、logistic回归、因子分析等。\n\n介绍高级统计方法的基本原理和操作，包括广义线性模型、多层线性模型、结构方程、因果推断等。"
  },
  {
    "objectID": "dataanlyr/lecture1/introduction.html#课程计划",
    "href": "dataanlyr/lecture1/introduction.html#课程计划",
    "title": "数据分析与R语言应用课程介绍",
    "section": "课程计划",
    "text": "课程计划\n\n课程介绍\n\n统计软件介绍R\n\n多元回归模型\n\nLogistic回归\n\n广义线性模型\n\n调节效应、中介效应与结构方程\n\n多层次模型（optional）\n\n大数据收集与机器学习应用介绍（optional）\n\nNote：课程计划会根据课程进度，适度调整。"
  },
  {
    "objectID": "dataanlyr/lecture1/introduction.html#课程考核",
    "href": "dataanlyr/lecture1/introduction.html#课程考核",
    "title": "数据分析与R语言应用课程介绍",
    "section": "课程考核",
    "text": "课程考核\n\n考核登记方式：百分制\n\n成绩组成：平时考勤占30%；期末论文汇报：20%；期末论文：50%\n\n考核标准：平时考勤基础分30分，无故旷课一次扣5分，六次无故旷课记0分，课前请假不扣分。期末论文汇报20分。期末论文占50分。\n\n时间要求：最后一次课进行课程论文报告并同时提交课程论文。\n\n课外学习内容：预习复习课程内容，阅读学习材料，学习相关软件的操作，完成练习，准备数据，进行数据分析，写作课程论文。"
  },
  {
    "objectID": "dataanlyr/lecture1/introduction.html#课程论文要求",
    "href": "dataanlyr/lecture1/introduction.html#课程论文要求",
    "title": "数据分析与R语言应用课程介绍",
    "section": "课程论文要求",
    "text": "课程论文要求\n\n论文应具有实质性的研究问题，有明确的观点，观点之间有关联。\n\n可以是原创文章，也可以在复制已发表期刊论文基础上形成扩展文章。\n\n对于扩展文章，如果认为原文章结论错误，应指明错误的原因。\n\n对于文章的复制部分，应解释复制精确度，和原始文章的差距有多大？\n\n尽量不要在正文中包括过多的中间过程，简述过程，展示结果。不要把R代码和初始结果直接贴在文章中！！！\n\n复制之后，探索改进原始文章的结果呈现。在不改变原文理论假设和模型的前提下，寻找有用信息。（改善odds ratio的解释等）\n\n建模前，进行充分的探索性分析，用简单、直观的方式展示有意义的探索性分析结果（图或者表）\n\n尝试不同的定量方法，改进原文的模型，试试看能否得到相似结果或不一样的结论。（缺失数据的处理、选择性偏误、遗漏变量偏差、模型设定、添加控制变量、改换效度更高的变量、扩展时间序列、进行样本外检验、采用更合适的统计模型等）\n\n如果得到了不一样的结论，用单独一节来叙述，如果结论相似，简略介绍即可。\n\n如果你的模型比原文模型更好，那么提供验证的证据，即用原文相同的样本数据或其他的样本外数据检验。（模型的概率假定更合理；95%的因变量数据落在95%的置信区间内；模型对样本外数据的预测效果更好）"
  },
  {
    "objectID": "dataanlyr/lecture1/introduction.html#论文格式",
    "href": "dataanlyr/lecture1/introduction.html#论文格式",
    "title": "数据分析与R语言应用课程介绍",
    "section": "论文格式",
    "text": "论文格式\n\n中文宋体小四或英文 Times New Roman 12pt、双倍行距，无封面，左上角装订\n\n包括论文题目、姓名、学号\n\n参考文献建议按照《公共管理学报》要求\n\n中间过程或不重要的细节可以放在附录中\n\n明确写出所建立的模型\n\n用公式编辑器编辑用到的数学公式，如果会用LaTex排版更好\n\n数字保留两位小数或者保留到你认为足够简单清晰的位数"
  },
  {
    "objectID": "dataanlyr/lecture1/introduction.html#论文图表格式",
    "href": "dataanlyr/lecture1/introduction.html#论文图表格式",
    "title": "数据分析与R语言应用课程介绍",
    "section": "论文图表格式",
    "text": "论文图表格式\n\n图表应简明扼要，展示论点，不是展示过程，不应过度使用\n\n图表应独立设定标题，图表分开标号，下方段落应有文字介绍\n\n建议挑出图表中的具体某个数字或信息进行举例，帮助理解\n\n表格采用三线表，不要每行每列都添加分隔线\n\n表格中有百分数，应在首行标注‘%’，不要每个单元都用。\n\n表格中每列数字的小数点应对齐。\n\n图表大小应合适，信息量小的图尽量小。\n\n可以用图或者表的地方，尽量用图。"
  },
  {
    "objectID": "dataanlyr/lecture1/introduction.html#最终提交文件",
    "href": "dataanlyr/lecture1/introduction.html#最终提交文件",
    "title": "数据分析与R语言应用课程介绍",
    "section": "最终提交文件",
    "text": "最终提交文件\n\n电子版： 下列4个文件打包成一个zip或rar文件，文件名是学号+姓名，提交时间：最后一次课后第二天中午12点，成绩以电子版为准！\n\n\n\n说明文件（如果包括多个数据文件，多个R程序文件等，列明所有的文件内容）\n\n数据文件（.csv, .dta, *.xlsx等）\n\nR语言程序文件（*.R）\n\n论文（.doc，.pdf）\n\n\n\n纸版论文：最后一次课提交。"
  },
  {
    "objectID": "dataanlyr/lecture1/introduction.html#参考书目与文献",
    "href": "dataanlyr/lecture1/introduction.html#参考书目与文献",
    "title": "数据分析与R语言应用课程介绍",
    "section": "参考书目与文献",
    "text": "参考书目与文献\n\nKabacoff （王小宁等译）R语言实战（第3版），人民邮电出版社，2023\n\nGelman and Hill. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press."
  },
  {
    "objectID": "dataanlyr/lecture8/datamining.html",
    "href": "dataanlyr/lecture8/datamining.html",
    "title": "数据挖掘",
    "section": "",
    "text": "大量数据被收集和存储\n\n网站数据、电子商务\n\n超市和商店的购物信息\n\n银行账户和信用卡的交易\n\n\n计算机变得越来越便宜，越来越高效\n\n竞争压力越来越大\n\n为客户提供更优质的定制化服务（客户关系管理）\n\n\n\n\n\n\n收集存储的数据急剧增长(每秒1.7兆字节)\n\n遥感卫星\n\n天文望远镜\n\n基因数据\n\n科学模拟数据\n\n\n传统技术无法处理\n\n数据挖掘能够帮助科学家\n\n进行数据分类和分段\n建构假设\n\n2024年诺贝尔物理学奖获得者约翰·霍普菲尔德和杰弗里·欣顿是两名机器学习领域的元老级人物。 他们使用物理学工具，设计了人工神经网络，为当今强大的机器学习技术奠定了基础。\n2024年诺贝尔化学奖授予戴维·贝克、德米斯·哈萨比斯和约翰·江珀。他们采用人工智能在“计算蛋白质设计”和“蛋白质结构预测”方面成就斐然。这也是继物理学奖之后，诺贝尔奖再次被授予人工智能（AI）的相关成果及科学家。\n\n\n\n\n\n\n数据中经常有一些“隐藏”的信息，很难一目了然的发现\n\n手工分析可能需要好几个星期的时间去发现有用的信息\n\n很多数据根本没有用于分析\n\n\n\n\n\n\n从数据中提取隐含的、未知的和潜在有用的信息\n\n通过自动或半自动的探索和分析方法在大型数据中发现有意义的模式\u000b\n\n什么不是数据挖掘？\n\n服务数据中查找用户的电话号码\n\n用百度搜索“京东快递”的信息\n\n什么是数据挖掘？\n\n某些姓名在某些地区可能更普遍\n\n依据具体情形对搜索引擎返回的信息进行分组（对京东快递服务的正面和负面评价）\n\n\n\n\n\n来自机器学习、人工智能、模式识别、统计学和数据库方面的思想\n\n传统的技术无能为力\n\n数据量太大\n\n高维度数据\n\n异种数据和复杂数据\n\n\n\n\n\n\n分类 [预测]\n\n聚类 [描述]\n\n关联规则发现 [描述]\n\n序列模式发现 [描述]\n\n回归 [预测]\n\n背离检测 [预测]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n给定一组记录 (训练集)\n\n每条记录包含一组属性，其中一个属性称之为类别.\n\n\n构建一个模型使类别属性成为其他属性值的函数.\n\n目标：尽可能将未见过的记录精确归类.\n\n测试集用于决定模型的精度. 一般将数据分成训练集和测试集，采用训练集构建模型，用测试集去验证。\n\n\n分类的例子：退税、婚姻、收入？逃税\n\n\n\n\n\n\n\n\n\n\n\n\n\n目的： 锁定可能购买新产品的消费者减少邮寄成本\n\n方法:\n\n采用已有相似产品的数据.\n\n已知哪些客户决定买，哪些客户决定不买. 购买的决定就是类别属性.\n\n收集这些客户的人口学特征、生活方式等信息.\n\n工作性质、生活区域、收入情况等.\n\n采用这些信息作用输入属性来训练分类器模型.\n\n\n\n\n\n\n目的：预测信用卡交易中的欺诈.\n\n方法：\n\n采用信用卡持有人的交易信息作用属性.\n\n持卡人什么时候买、买什么、购买频率等\n\n过去的交易是否是欺诈或正常交易，这便是分类属性.\n\n训练交易分类模型.\n\n用模型对某个账户的信用卡交易进行分析检测是否存在欺诈.\n\n\n\n\n\n\n\n目的：预测天体的类别（恒星或星系），特别是针对那些在望远镜上看起来不清楚.\n\n3000 images with 23,040 x 23,040 pixels per image.\n\n\n方法:\n\n图像分段.\n\n测量图像的属性，每个天体有四十个属性.\n\n基于属性构建分类模型.\n\n成功找到16个新的高红移类星体, 难以发现的最远天体!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n有一组数据点，每个点有一组采用相似测量方式的属性，实现如下分类（分簇、分堆）\n\n簇内数据点比其他簇的数据点更相似.\n\n簇间数据点的差别要尽可能大.\n\n\n相似性的测量:\n\n欧氏距离用于连续型属性.\n\n其他基于具体问题的度量方式.\n\n\n聚类示意\n\n\n\n\n\n\n\n\n\n\n\n\n\n目的：对市场客户进行划分，利于采用不同的营销策略针对不同的客户群体.\n\n方法:\n\n基于客户的地理位置和生活习惯收集不同的信息属性.\n\n发现相似客户的组群.\n\n通过对比簇内客户和簇间客户的购买模式检验聚类的质量.\n\n\n\n\n\n\n目的: 根据文档中的主题词的出现情况对文档进行分组.\n\n方法：识别每个文档中频繁出现的主题词.利用不同主题词出现的频率测量文档的相似性，并对其聚类.\n\n应用：信息检索能够利用聚类的信息将新文档或检索词与聚类后文档联系起来.\n\n文本聚类示例\n- 聚类点：洛杉矶时报的3204篇文章.\n- 相似性测量：文档中有多少相同的词.\n\n\n\n\n\n\n\n\n\n股市数据聚类\n\n观察每天的股票动态.\n\n聚类点：股票-{涨/跌}\n\n相似性测量：如果同一天相同的涨跌频繁发生那么两点越相似.\n\n用关联性法则量化相关性.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n给定一组记录，每条记录包含给定集合中的某些项目；\n\n发现能够根据其他项目出现来预测某个项目出现的依赖规则.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n假设发现了如下规则 {可乐, … } –&gt; {薯片}\n\n薯片是后发生 =&gt; 能够用于决定如何提升其销量.\n\n可乐是先发生 =&gt; 能够用于了解如果可乐出现断货，会影响哪一类产品的销量.\n\n可乐先发生和薯片后发生 =&gt; 能够用于了解什么产品应该与可乐一起销售以提高薯片的销量！\n\n\n\n\n\n目标：通过顾客的购买行为了解会被同时购买的商品.\n\n目的：处理扫码后的销售数据发现商品之间的依赖关系.\n\n一条规则 –\n\n如果一位顾客买了尿布和牛奶，他很有可能会买啤酒.\n\n所以超市在尿布货架旁布置啤酒！\n\n\n\n\n\n\n目的：电器维修公司想预测维修消费者电器需要的原配件，那么可以事先带上，以减少往返和上门的次数.\n\n方法：对不同区域消费者之前维修所用原配件和工具的数据进行处理，发现共同出现的模式.\n\n\n\n\n\n\n在假定线性或非线性依赖关系的前提下，根据其他变量预测给定连续型变量的值.\n\n在统计学和神经网络领域应用最多.\n\n例子：\n\n基于广告费用预测新产品的销售量.\n\n利用温度、湿度和气压等构建函数预测风速.\n\n对股市指标进行时间序列预测.\n\n\n\n\n\n\n从正常行为中检测显著的异常\n\n应用：\n\n信用卡欺诈检测\n\n网络入侵检测：一般大学每日网络访问量多达1亿个连接\n\n\n\n\n\n\n尺度扩展性\n\n高维度\n\n异种数据和复杂数据\n\n数据质量\n\n数据所有权和分布\n\n隐私保护\n\n实时动态数据流"
  },
  {
    "objectID": "dataanlyr/lecture8/datamining.html#数据挖掘与机器学习",
    "href": "dataanlyr/lecture8/datamining.html#数据挖掘与机器学习",
    "title": "数据挖掘",
    "section": "",
    "text": "大量数据被收集和存储\n\n网站数据、电子商务\n\n超市和商店的购物信息\n\n银行账户和信用卡的交易\n\n\n计算机变得越来越便宜，越来越高效\n\n竞争压力越来越大\n\n为客户提供更优质的定制化服务（客户关系管理）\n\n\n\n\n\n\n收集存储的数据急剧增长(每秒1.7兆字节)\n\n遥感卫星\n\n天文望远镜\n\n基因数据\n\n科学模拟数据\n\n\n传统技术无法处理\n\n数据挖掘能够帮助科学家\n\n进行数据分类和分段\n建构假设\n\n2024年诺贝尔物理学奖获得者约翰·霍普菲尔德和杰弗里·欣顿是两名机器学习领域的元老级人物。 他们使用物理学工具，设计了人工神经网络，为当今强大的机器学习技术奠定了基础。\n2024年诺贝尔化学奖授予戴维·贝克、德米斯·哈萨比斯和约翰·江珀。他们采用人工智能在“计算蛋白质设计”和“蛋白质结构预测”方面成就斐然。这也是继物理学奖之后，诺贝尔奖再次被授予人工智能（AI）的相关成果及科学家。\n\n\n\n\n\n\n数据中经常有一些“隐藏”的信息，很难一目了然的发现\n\n手工分析可能需要好几个星期的时间去发现有用的信息\n\n很多数据根本没有用于分析\n\n\n\n\n\n\n从数据中提取隐含的、未知的和潜在有用的信息\n\n通过自动或半自动的探索和分析方法在大型数据中发现有意义的模式\u000b\n\n什么不是数据挖掘？\n\n服务数据中查找用户的电话号码\n\n用百度搜索“京东快递”的信息\n\n什么是数据挖掘？\n\n某些姓名在某些地区可能更普遍\n\n依据具体情形对搜索引擎返回的信息进行分组（对京东快递服务的正面和负面评价）\n\n\n\n\n\n来自机器学习、人工智能、模式识别、统计学和数据库方面的思想\n\n传统的技术无能为力\n\n数据量太大\n\n高维度数据\n\n异种数据和复杂数据\n\n\n\n\n\n\n分类 [预测]\n\n聚类 [描述]\n\n关联规则发现 [描述]\n\n序列模式发现 [描述]\n\n回归 [预测]\n\n背离检测 [预测]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n给定一组记录 (训练集)\n\n每条记录包含一组属性，其中一个属性称之为类别.\n\n\n构建一个模型使类别属性成为其他属性值的函数.\n\n目标：尽可能将未见过的记录精确归类.\n\n测试集用于决定模型的精度. 一般将数据分成训练集和测试集，采用训练集构建模型，用测试集去验证。\n\n\n分类的例子：退税、婚姻、收入？逃税\n\n\n\n\n\n\n\n\n\n\n\n\n\n目的： 锁定可能购买新产品的消费者减少邮寄成本\n\n方法:\n\n采用已有相似产品的数据.\n\n已知哪些客户决定买，哪些客户决定不买. 购买的决定就是类别属性.\n\n收集这些客户的人口学特征、生活方式等信息.\n\n工作性质、生活区域、收入情况等.\n\n采用这些信息作用输入属性来训练分类器模型.\n\n\n\n\n\n\n目的：预测信用卡交易中的欺诈.\n\n方法：\n\n采用信用卡持有人的交易信息作用属性.\n\n持卡人什么时候买、买什么、购买频率等\n\n过去的交易是否是欺诈或正常交易，这便是分类属性.\n\n训练交易分类模型.\n\n用模型对某个账户的信用卡交易进行分析检测是否存在欺诈.\n\n\n\n\n\n\n\n目的：预测天体的类别（恒星或星系），特别是针对那些在望远镜上看起来不清楚.\n\n3000 images with 23,040 x 23,040 pixels per image.\n\n\n方法:\n\n图像分段.\n\n测量图像的属性，每个天体有四十个属性.\n\n基于属性构建分类模型.\n\n成功找到16个新的高红移类星体, 难以发现的最远天体!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n有一组数据点，每个点有一组采用相似测量方式的属性，实现如下分类（分簇、分堆）\n\n簇内数据点比其他簇的数据点更相似.\n\n簇间数据点的差别要尽可能大.\n\n\n相似性的测量:\n\n欧氏距离用于连续型属性.\n\n其他基于具体问题的度量方式.\n\n\n聚类示意\n\n\n\n\n\n\n\n\n\n\n\n\n\n目的：对市场客户进行划分，利于采用不同的营销策略针对不同的客户群体.\n\n方法:\n\n基于客户的地理位置和生活习惯收集不同的信息属性.\n\n发现相似客户的组群.\n\n通过对比簇内客户和簇间客户的购买模式检验聚类的质量.\n\n\n\n\n\n\n目的: 根据文档中的主题词的出现情况对文档进行分组.\n\n方法：识别每个文档中频繁出现的主题词.利用不同主题词出现的频率测量文档的相似性，并对其聚类.\n\n应用：信息检索能够利用聚类的信息将新文档或检索词与聚类后文档联系起来.\n\n文本聚类示例\n- 聚类点：洛杉矶时报的3204篇文章.\n- 相似性测量：文档中有多少相同的词.\n\n\n\n\n\n\n\n\n\n股市数据聚类\n\n观察每天的股票动态.\n\n聚类点：股票-{涨/跌}\n\n相似性测量：如果同一天相同的涨跌频繁发生那么两点越相似.\n\n用关联性法则量化相关性.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n给定一组记录，每条记录包含给定集合中的某些项目；\n\n发现能够根据其他项目出现来预测某个项目出现的依赖规则.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n假设发现了如下规则 {可乐, … } –&gt; {薯片}\n\n薯片是后发生 =&gt; 能够用于决定如何提升其销量.\n\n可乐是先发生 =&gt; 能够用于了解如果可乐出现断货，会影响哪一类产品的销量.\n\n可乐先发生和薯片后发生 =&gt; 能够用于了解什么产品应该与可乐一起销售以提高薯片的销量！\n\n\n\n\n\n目标：通过顾客的购买行为了解会被同时购买的商品.\n\n目的：处理扫码后的销售数据发现商品之间的依赖关系.\n\n一条规则 –\n\n如果一位顾客买了尿布和牛奶，他很有可能会买啤酒.\n\n所以超市在尿布货架旁布置啤酒！\n\n\n\n\n\n\n目的：电器维修公司想预测维修消费者电器需要的原配件，那么可以事先带上，以减少往返和上门的次数.\n\n方法：对不同区域消费者之前维修所用原配件和工具的数据进行处理，发现共同出现的模式.\n\n\n\n\n\n\n在假定线性或非线性依赖关系的前提下，根据其他变量预测给定连续型变量的值.\n\n在统计学和神经网络领域应用最多.\n\n例子：\n\n基于广告费用预测新产品的销售量.\n\n利用温度、湿度和气压等构建函数预测风速.\n\n对股市指标进行时间序列预测.\n\n\n\n\n\n\n从正常行为中检测显著的异常\n\n应用：\n\n信用卡欺诈检测\n\n网络入侵检测：一般大学每日网络访问量多达1亿个连接\n\n\n\n\n\n\n尺度扩展性\n\n高维度\n\n异种数据和复杂数据\n\n数据质量\n\n数据所有权和分布\n\n隐私保护\n\n实时动态数据流"
  },
  {
    "objectID": "analyr/index.html#作业",
    "href": "analyr/index.html#作业",
    "title": "定量研究方法",
    "section": "作业",
    "text": "作业\n\nR语言基本设置与一般统计方法作业\n\n线性回归分析作业"
  },
  {
    "objectID": "analyr/lecture8/rapplication.html",
    "href": "analyr/lecture8/rapplication.html",
    "title": "网页爬取与机器学习",
    "section": "",
    "text": "R语言可以承担一些简单的网页爬取工作，并且可以很方便的进行分析，但是具有反爬虫机制的动态网页还需更专业的爬虫工具，例如Python等。\n\n\n以厦门市生态环境局行政执法栏目为例说明静态网页爬取过程。\n爬取网页数据，首先需要确定网页数据的位置。通常可以采用CSS selector he和Xpath。CSS selector通过数据所在对象的样式和模式指定位置，参见CSS Selector的语法；而XPath使用路径表达式来选取网页文档中的节点，可以参考XPath的语法。可以通过Chrome浏览器的开发者检查元素功能，找到数据所在的页面的对象，帮助获取样式或模式信息，确定对象的位置。示例网站处罚书来自类别（class）名为gl_list1的div的对象，并且依次向下有ul、li和a三个对象层级，a链接对象中是处罚书的名称等内容。通过rvest包中的read_html()、html_nodes()、html_text()、html_attr()函数可以实现获取html文本、网页对象节点、网页对象内容、网页对象属性内容等功能。\n\nlibrary(rvest)\n\n\nmyurl = \"https://sthjj.xm.gov.cn/zwgk/gsgk/xzcf/\"\n\nweb&lt;-read_html(myurl, encoding=\"UTF-8\") #获取html网页文本\n\npunishcompany &lt;-  web %&gt;% html_nodes(\"div.gl_list1 ul li a\") |&gt; html_text()  # 获取网页数据处罚书\n\npunishdate &lt;-  web %&gt;% html_nodes(\"div.gl_list1 ul li span\") |&gt; html_text()  # 获取处罚时间\n\npunishlink &lt;- web %&gt;% html_nodes(\"div.gl_list1 ul li a\") |&gt; html_attr(name = \"href\")  # 获取处罚书链接\n\npunish &lt;- data.frame(company=punishcompany, date=punishdate, link=punishlink) # 将数据保存到数据框\n\n可以通过循环，快速爬取多个类似结构的静态页面\n\nfor(i in 1:10){\n  url &lt;- paste(myurl, \"index_\", i, \".htm\", sep = \"\")\n  web&lt;-read_html(url,encoding=\"UTF-8\")\n  punishcompany &lt;-  web %&gt;% html_nodes(\"div.gl_list1 ul li a\") |&gt; html_text() \n  punishdate &lt;-  web %&gt;% html_nodes(\"div.gl_list1 ul li span\") |&gt; html_text() \n  punishlink &lt;- web %&gt;% html_nodes(\"div.gl_list1 ul li a\") |&gt; html_attr(name = \"href\") \n\n  punish1 &lt;- data.frame(company=punishcompany, date=punishdate, link=punishlink)\n  punish &lt;- punish %&gt;% rbind(punish1)\n}\n\nhead(punish)  # 展示获取的处罚书数据\n\n                                     company       date\n1 厦门市腾盛兴电子技术有限公司行政处罚决定书 2024-11-26\n2 厦门市玖玖机动车检测有限公司行政处罚决定书 2024-11-13\n3 解除扣押决定书厦门市腾盛兴电子技术有限公司 2024-10-16\n4                       张帅涛行政处罚决定书 2024-10-11\n5         厦门日上钢圈有限公司行政处罚决定书 2024-10-11\n6   厦门钟利机动车检测有限公司行政处罚决定书 2024-09-29\n                            link\n1 ./202411/t20241126_2903149.htm\n2 ./202411/t20241113_2900716.htm\n3 ./202411/t20241129_2903969.htm\n4 ./202410/t20241011_2894766.htm\n5 ./202410/t20241011_2894762.htm\n6 ./202409/t20240929_2892737.htm\n\ntail(punish)  # 确认是否已完整获取数据\n\n                                                                       company\n193 厦门文仪电脑材料有限公司责令改正违法行为决定书 闽厦（执法）环改〔2022〕1号\n194                                责令改正违法行为决定书 厦门日上金属有限公司\n195               厦门恒兴兴业机械有限公司行政处罚决定书 闽厦环罚〔2021〕363号\n196                  厦门海湾化工有限公司行政处罚决定书  闽厦环罚〔2021〕345号\n197                   厦门海湾化工有限公司行政处罚决定书 闽厦环罚〔2021〕344号\n198  厦门三荣陶瓷开发有限公司不予行政处罚决定书  厦环（执法）不罚决字[2021]1号\n          date                           link\n193 2022-01-10 ./202201/t20220110_2616014.htm\n194 2022-01-05 ./202305/t20230526_2761318.htm\n195 2021-12-31 ./202201/t20220110_2616044.htm\n196 2021-12-15 ./202112/t20211215_2608718.htm\n197 2021-12-15 ./202112/t20211215_2608715.htm\n198 2021-12-10 ./202112/t20211215_2608668.htm\n\n\n\n\n\n动态页面与静态页面不同，一般通过与用户建立对话（session）获取用户的数据请求，采用Javascript等程序语言建立定制页面返回数据。因此采用静态页面的方法是无法定位数据的。下面以厦门市生态环境局的咨询投诉栏目为例，介绍动态页面的爬取。\n\nmyurl = \"https://sthjj.xm.gov.cn/gzcy/wyzx/\"\n\nweb&lt;-read_html(myurl, encoding=\"UTF-8\")\n\nweb %&gt;% html_nodes(\"div.gl_list2 ul li a\") \n\n{xml_nodeset (1)}\n[1] &lt;a ms-attr-href=\"'./index_18763.htm?id='+el.letterId+'&amp;chnlId='+el.ch ...\n\nweb %&gt;% html_nodes(\"div.gl_list2 ul li a\") |&gt; html_text()\n\n[1] \"\"\n\n\n动态页面可以先通过会话请求页面，在爬取数据后，可以模拟点击下一页，此时会话会指向下一页，便可以爬取下一页的数据，依此直到爬取结束。\n\nlibrary(chromote)\n\nsess &lt;- read_html_live(myurl) # 请求动态页面\n\n#sess$view()\n\nconsultdata &lt;- NULL\ni=1\nwhile(i&lt;10){\n  consult &lt;-  sess %&gt;% html_elements(\"div.gl_list2 ul li a\") |&gt; html_text()   # 获取相关数据\n  date &lt;-  sess %&gt;% html_elements(\"div.gl_list2 ul li font\") |&gt; html_text() \n  link &lt;- sess %&gt;% html_elements(\"div.gl_list2 ul li a\") |&gt; html_attr(name = \"href\") \n  if(length(consult)==0| !(length(consult)==length(date)&length(consult)==length(link))) next \n  consulttemp &lt;- data.frame(consult=consult, date=date, link=link)\n  consultdata &lt;- consultdata %&gt;% rbind(consulttemp)     # 保存到数据框\n  \n  if(nrow(consultdata) == 200) break       # 如果完成爬取，退出\n  sess$click(\"a.next.b-free-read-leaf\")     # 模拟页面点击下一页\n  i=i+1\n}\n\nhead(consultdata)  \ntail(consultdata)  # 检查数据"
  },
  {
    "objectID": "analyr/lecture8/rapplication.html#网页爬取",
    "href": "analyr/lecture8/rapplication.html#网页爬取",
    "title": "网页爬取与机器学习",
    "section": "",
    "text": "R语言可以承担一些简单的网页爬取工作，并且可以很方便的进行分析，但是具有反爬虫机制的动态网页还需更专业的爬虫工具，例如Python等。\n\n\n以厦门市生态环境局行政执法栏目为例说明静态网页爬取过程。\n爬取网页数据，首先需要确定网页数据的位置。通常可以采用CSS selector he和Xpath。CSS selector通过数据所在对象的样式和模式指定位置，参见CSS Selector的语法；而XPath使用路径表达式来选取网页文档中的节点，可以参考XPath的语法。可以通过Chrome浏览器的开发者检查元素功能，找到数据所在的页面的对象，帮助获取样式或模式信息，确定对象的位置。示例网站处罚书来自类别（class）名为gl_list1的div的对象，并且依次向下有ul、li和a三个对象层级，a链接对象中是处罚书的名称等内容。通过rvest包中的read_html()、html_nodes()、html_text()、html_attr()函数可以实现获取html文本、网页对象节点、网页对象内容、网页对象属性内容等功能。\n\nlibrary(rvest)\n\n\nmyurl = \"https://sthjj.xm.gov.cn/zwgk/gsgk/xzcf/\"\n\nweb&lt;-read_html(myurl, encoding=\"UTF-8\") #获取html网页文本\n\npunishcompany &lt;-  web %&gt;% html_nodes(\"div.gl_list1 ul li a\") |&gt; html_text()  # 获取网页数据处罚书\n\npunishdate &lt;-  web %&gt;% html_nodes(\"div.gl_list1 ul li span\") |&gt; html_text()  # 获取处罚时间\n\npunishlink &lt;- web %&gt;% html_nodes(\"div.gl_list1 ul li a\") |&gt; html_attr(name = \"href\")  # 获取处罚书链接\n\npunish &lt;- data.frame(company=punishcompany, date=punishdate, link=punishlink) # 将数据保存到数据框\n\n可以通过循环，快速爬取多个类似结构的静态页面\n\nfor(i in 1:10){\n  url &lt;- paste(myurl, \"index_\", i, \".htm\", sep = \"\")\n  web&lt;-read_html(url,encoding=\"UTF-8\")\n  punishcompany &lt;-  web %&gt;% html_nodes(\"div.gl_list1 ul li a\") |&gt; html_text() \n  punishdate &lt;-  web %&gt;% html_nodes(\"div.gl_list1 ul li span\") |&gt; html_text() \n  punishlink &lt;- web %&gt;% html_nodes(\"div.gl_list1 ul li a\") |&gt; html_attr(name = \"href\") \n\n  punish1 &lt;- data.frame(company=punishcompany, date=punishdate, link=punishlink)\n  punish &lt;- punish %&gt;% rbind(punish1)\n}\n\nhead(punish)  # 展示获取的处罚书数据\n\n                                     company       date\n1 厦门市腾盛兴电子技术有限公司行政处罚决定书 2024-11-26\n2 厦门市玖玖机动车检测有限公司行政处罚决定书 2024-11-13\n3 解除扣押决定书厦门市腾盛兴电子技术有限公司 2024-10-16\n4                       张帅涛行政处罚决定书 2024-10-11\n5         厦门日上钢圈有限公司行政处罚决定书 2024-10-11\n6   厦门钟利机动车检测有限公司行政处罚决定书 2024-09-29\n                            link\n1 ./202411/t20241126_2903149.htm\n2 ./202411/t20241113_2900716.htm\n3 ./202411/t20241129_2903969.htm\n4 ./202410/t20241011_2894766.htm\n5 ./202410/t20241011_2894762.htm\n6 ./202409/t20240929_2892737.htm\n\ntail(punish)  # 确认是否已完整获取数据\n\n                                                                       company\n193 厦门文仪电脑材料有限公司责令改正违法行为决定书 闽厦（执法）环改〔2022〕1号\n194                                责令改正违法行为决定书 厦门日上金属有限公司\n195               厦门恒兴兴业机械有限公司行政处罚决定书 闽厦环罚〔2021〕363号\n196                  厦门海湾化工有限公司行政处罚决定书  闽厦环罚〔2021〕345号\n197                   厦门海湾化工有限公司行政处罚决定书 闽厦环罚〔2021〕344号\n198  厦门三荣陶瓷开发有限公司不予行政处罚决定书  厦环（执法）不罚决字[2021]1号\n          date                           link\n193 2022-01-10 ./202201/t20220110_2616014.htm\n194 2022-01-05 ./202305/t20230526_2761318.htm\n195 2021-12-31 ./202201/t20220110_2616044.htm\n196 2021-12-15 ./202112/t20211215_2608718.htm\n197 2021-12-15 ./202112/t20211215_2608715.htm\n198 2021-12-10 ./202112/t20211215_2608668.htm\n\n\n\n\n\n动态页面与静态页面不同，一般通过与用户建立对话（session）获取用户的数据请求，采用Javascript等程序语言建立定制页面返回数据。因此采用静态页面的方法是无法定位数据的。下面以厦门市生态环境局的咨询投诉栏目为例，介绍动态页面的爬取。\n\nmyurl = \"https://sthjj.xm.gov.cn/gzcy/wyzx/\"\n\nweb&lt;-read_html(myurl, encoding=\"UTF-8\")\n\nweb %&gt;% html_nodes(\"div.gl_list2 ul li a\") \n\n{xml_nodeset (1)}\n[1] &lt;a ms-attr-href=\"'./index_18763.htm?id='+el.letterId+'&amp;chnlId='+el.ch ...\n\nweb %&gt;% html_nodes(\"div.gl_list2 ul li a\") |&gt; html_text()\n\n[1] \"\"\n\n\n动态页面可以先通过会话请求页面，在爬取数据后，可以模拟点击下一页，此时会话会指向下一页，便可以爬取下一页的数据，依此直到爬取结束。\n\nlibrary(chromote)\n\nsess &lt;- read_html_live(myurl) # 请求动态页面\n\n#sess$view()\n\nconsultdata &lt;- NULL\ni=1\nwhile(i&lt;10){\n  consult &lt;-  sess %&gt;% html_elements(\"div.gl_list2 ul li a\") |&gt; html_text()   # 获取相关数据\n  date &lt;-  sess %&gt;% html_elements(\"div.gl_list2 ul li font\") |&gt; html_text() \n  link &lt;- sess %&gt;% html_elements(\"div.gl_list2 ul li a\") |&gt; html_attr(name = \"href\") \n  if(length(consult)==0| !(length(consult)==length(date)&length(consult)==length(link))) next \n  consulttemp &lt;- data.frame(consult=consult, date=date, link=link)\n  consultdata &lt;- consultdata %&gt;% rbind(consulttemp)     # 保存到数据框\n  \n  if(nrow(consultdata) == 200) break       # 如果完成爬取，退出\n  sess$click(\"a.next.b-free-read-leaf\")     # 模拟页面点击下一页\n  i=i+1\n}\n\nhead(consultdata)  \ntail(consultdata)  # 检查数据"
  },
  {
    "objectID": "analyr/lecture8/rapplication.html#文本挖掘",
    "href": "analyr/lecture8/rapplication.html#文本挖掘",
    "title": "网页爬取与机器学习",
    "section": "文本挖掘",
    "text": "文本挖掘\nR语言能够对已获取的文本数据进行挖掘探索，对文本进行分词后，探索主题词的频率，建立文档-词条矩阵，进行文本聚类和有目的的自动分类，进行主题建模和情感分析等。\n\n设置环境\nR语言需要Java支持才能使用文本挖掘相关软件包，首先要根据操作系统下载jdk程序进行安装，然后在rstudio中安装rjava包和Rwordseg包（install.packages(“Rwordseg”, repos=“http://R-Forge.R-project.org”)），中间可能还需要手动安装一些依赖的包。\n\nlibrary(rJava)\nlibrary(Rwordseg)\n\n\n\n文本预处理与分词\n分词是文本分析的基础，可以在分词前给出文本中常见的小地名防止错误分割词汇，还可先剔除掉数字以减少干扰。在文本预处理之后，便可以采用segmentCN函数对数据进行分词，分词器采用“jiebaR”的效果比较好。分词后可以通过设定stopwords将一些没有含义的连词、语气词等剔除。\n\ninsertWords(c(\"思明\",\"湖里\",\"同安\",\"翔安\",\"海沧\",\"集美\",\"杏林\",\"厦门\",\"新垵\"))\nconsulttemp &lt;- gsub(\"[0-9０１２３４５６７８９ &lt; &gt; ~]\",\"\",consultdata$consult)\n\nconsulttemp &lt;- segmentCN(consulttemp, analyzer = \"jiebaR\")\n\nremoveStopWords &lt;- function(x,stopwords) {\nx=x[!( x %in% stopwords)]\nreturn(x)\n}\nstopwords &lt;- stopwordsCN(c(\"了\", \"是\", \"无\", \"请问\", \"及\", \"小\", \"与\", \"如何\", \"不\", \"的\", \"可以\"))\nconsulttemp &lt;-lapply(consulttemp,removeStopWords,stopwords)\n\n\n\n词条排序与词云\n\nwords &lt;- lapply(consulttemp, strsplit, \" \")\nwordsNum &lt;- table(unlist(words))\nwordsNum &lt;- sort(wordsNum, decreasing = T) #排序\nwordsData &lt;- as.data.frame(wordsNum)\n\nlibrary(wordcloud2) #加载画词云的包\n\nconsulttop150 &lt;- head(wordsData,150) #取前150个词\nwordcloud2(consulttop150)"
  },
  {
    "objectID": "analyr/lecture8/rapplication.html#数据挖掘与机器学习简介",
    "href": "analyr/lecture8/rapplication.html#数据挖掘与机器学习简介",
    "title": "网页爬取与机器学习",
    "section": "数据挖掘与机器学习简介",
    "text": "数据挖掘与机器学习简介\n\n为什么要挖掘数据？\n\n商业角度\n\n大量数据被收集和存储\n\n网站数据、电子商务\n\n超市和商店的购物信息\n\n银行账户和信用卡的交易\n\n\n计算机变得越来越便宜，越来越高效\n\n竞争压力越来越大\n\n为客户提供更优质的定制化服务（客户关系管理）\n\n\n\n\n科学角度\n\n收集存储的数据急剧增长(每秒1.7兆字节)\n\n遥感卫星\n\n天文望远镜\n\n基因数据\n\n科学模拟数据\n\n\n传统技术无法处理\n\n数据挖掘能够帮助科学家\n\n进行数据分类和分段\n建构假设\n\n2024年诺贝尔物理学奖获得者约翰·霍普菲尔德和杰弗里·欣顿是两名机器学习领域的元老级人物。 他们使用物理学工具，设计了人工神经网络，为当今强大的机器学习技术奠定了基础。\n2024年诺贝尔化学奖授予戴维·贝克、德米斯·哈萨比斯和约翰·江珀。他们采用人工智能在“计算蛋白质设计”和“蛋白质结构预测”方面成就斐然。这也是继物理学奖之后，诺贝尔奖再次被授予人工智能（AI）的相关成果及科学家。\n\n\n\n\n发掘大数据的动机\n\n数据中经常有一些“隐藏”的信息，很难一目了然的发现\n\n手工分析可能需要好几个星期的时间去发现有用的信息\n\n很多数据根本没有用于分析\n\n\n\n\n什么是数据挖掘？\n\n从数据中提取隐含的、未知的和潜在有用的信息\n\n通过自动或半自动的探索和分析方法在大型数据中发现有意义的模式\u000b\n\n什么不是数据挖掘？\n\n服务数据中查找用户的电话号码\n\n用百度搜索“京东快递”的信息\n\n什么是数据挖掘？\n\n某些姓名在某些地区可能更普遍\n\n依据具体情形对搜索引擎返回的信息进行分组（对京东快递服务的正面和负面评价）\n\n\n\n数据挖掘的起源\n\n来自机器学习、人工智能、模式识别、统计学和数据库方面的思想\n\n传统的技术无能为力\n\n数据量太大\n\n高维度数据\n\n异种数据和复杂数据\n\n\n\n\n数据挖掘的任务\n\n分类 [预测]\n\n聚类 [描述]\n\n关联规则发现 [描述]\n\n序列模式发现 [描述]\n\n回归 [预测]\n\n背离检测 [预测]\n\n\n\n\n\n\n\n\n\n\n\n\n分类\n\n定义\n\n给定一组记录 (训练集)\n\n每条记录包含一组属性，其中一个属性称之为类别.\n\n\n构建一个模型使类别属性成为其他属性值的函数.\n\n目标：尽可能将未见过的记录精确归类.\n\n测试集用于决定模型的精度. 一般将数据分成训练集和测试集，采用训练集构建模型，用测试集去验证。\n\n\n分类的例子：退税、婚姻、收入？逃税\n\n\n\n\n\n\n\n\n\n\n\n分类的应用：直销\n\n目的： 锁定可能购买新产品的消费者减少邮寄成本\n\n方法:\n\n采用已有相似产品的数据.\n\n已知哪些客户决定买，哪些客户决定不买. 购买的决定就是类别属性.\n\n收集这些客户的人口学特征、生活方式等信息.\n\n工作性质、生活区域、收入情况等.\n\n采用这些信息作用输入属性来训练分类器模型.\n\n\n\n\n分类的应用：信用卡欺诈检测\n\n目的：预测信用卡交易中的欺诈.\n\n方法：\n\n采用信用卡持有人的交易信息作用属性.\n\n持卡人什么时候买、买什么、购买频率等\n\n过去的交易是否是欺诈或正常交易，这便是分类属性.\n\n训练交易分类模型.\n\n用模型对某个账户的信用卡交易进行分析检测是否存在欺诈.\n\n\n\n\n\n分类的应用：太空探索分类\n\n目的：预测天体的类别（恒星或星系），特别是针对那些在望远镜上看起来不清楚.\n\n3000 images with 23,040 x 23,040 pixels per image.\n\n\n方法:\n\n图像分段.\n\n测量图像的属性，每个天体有四十个属性.\n\n基于属性构建分类模型.\n\n成功找到16个新的高红移类星体, 难以发现的最远天体!\n\n\n\n\n\n\n\n\n\n\n\n\n\n聚类\n\n定义\n\n有一组数据点，每个点有一组采用相似测量方式的属性，实现如下分类（分簇、分堆）\n\n簇内数据点比其他簇的数据点更相似.\n\n簇间数据点的差别要尽可能大.\n\n\n相似性的测量:\n\n欧氏距离用于连续型属性.\n\n其他基于具体问题的度量方式.\n\n\n聚类示意\n\n\n\n\n\n\n\n\n\n\n\n聚类应用：市场划分\n\n目的：对市场客户进行划分，利于采用不同的营销策略针对不同的客户群体.\n\n方法:\n\n基于客户的地理位置和生活习惯收集不同的信息属性.\n\n发现相似客户的组群.\n\n通过对比簇内客户和簇间客户的购买模式检验聚类的质量.\n\n\n\n\n聚类应用：文档聚类\n\n目的: 根据文档中的主题词的出现情况对文档进行分组.\n\n方法：识别每个文档中频繁出现的主题词.利用不同主题词出现的频率测量文档的相似性，并对其聚类.\n\n应用：信息检索能够利用聚类的信息将新文档或检索词与聚类后文档联系起来.\n\n文本聚类示例\n- 聚类点：洛杉矶时报的3204篇文章.\n- 相似性测量：文档中有多少相同的词.\n\n\n\n\n\n\n\n\n\n股市数据聚类\n\n观察每天的股票动态.\n\n聚类点：股票-{涨/跌}\n\n相似性测量：如果同一天相同的涨跌频繁发生那么两点越相似.\n\n用关联性法则量化相关性.\n\n\n\n\n\n\n\n\n\n\n\n\n\n关联规则发现\n\n定义\n\n给定一组记录，每条记录包含给定集合中的某些项目；\n\n发现能够根据其他项目出现来预测某个项目出现的依赖规则.\n\n\n\n\n\n\n\n\n\n\n\n\n\n关联规则应用：市场营销\n\n假设发现了如下规则 {可乐, … } –&gt; {薯片}\n\n薯片是后发生 =&gt; 能够用于决定如何提升其销量.\n\n可乐是先发生 =&gt; 能够用于了解如果可乐出现断货，会影响哪一类产品的销量.\n\n可乐先发生和薯片后发生 =&gt; 能够用于了解什么产品应该与可乐一起销售以提高薯片的销量！\n\n\n\n关联规则应用：超市货架管理\n\n目标：通过顾客的购买行为了解会被同时购买的商品.\n\n目的：处理扫码后的销售数据发现商品之间的依赖关系.\n\n一条规则 –\n\n如果一位顾客买了尿布和牛奶，他很有可能会买啤酒.\n\n所以超市在尿布货架旁布置啤酒！\n\n\n\n\n关联规则应用：清单管理\n\n目的：电器维修公司想预测维修消费者电器需要的原配件，那么可以事先带上，以减少往返和上门的次数.\n\n方法：对不同区域消费者之前维修所用原配件和工具的数据进行处理，发现共同出现的模式.\n\n\n\n\n回归\n\n在假定线性或非线性依赖关系的前提下，根据其他变量预测给定连续型变量的值.\n\n在统计学和神经网络领域应用最多.\n\n例子：\n\n基于广告费用预测新产品的销售量.\n\n利用温度、湿度和气压等构建函数预测风速.\n\n对股市指标进行时间序列预测.\n\n\n\n\n背离/异常检测\n\n从正常行为中检测显著的异常\n\n应用：\n\n信用卡欺诈检测\n\n网络入侵检测：一般大学每日网络访问量多达1亿个连接\n\n\n\n\n数据挖掘面临的挑战\n\n尺度扩展性\n\n高维度\n\n异种数据和复杂数据\n\n数据质量\n\n数据所有权和分布\n\n隐私保护\n\n实时动态数据流"
  },
  {
    "objectID": "analyr/lecture7/sem.html",
    "href": "analyr/lecture7/sem.html",
    "title": "调节效应、中介效应和结构方程",
    "section": "",
    "text": "调节效应（Moderation Effect）是统计学和社会科学中的一个概念，指的是某个第三变量（称为调节变量或调节因子）能够改变两个其他变量之间的关系的强度或方向所产生的作用。通过调节效应的分析，可以更好地理解复杂的变量关系，为研究者提供更加精细化的理论指导和建议。\n当一个调节变量存在时，它会通过影响自变量与因变量之间的关系来发挥作用。例如：\n\n如果 ( X ) 是自变量，( Y ) 是因变量，( Z ) 是调节变量，那么调节效应的核心是： [ Y = _0 + _1 X + _2 Z + _3 (X Z) + ] 其中，( _3 ) 是交互项系数，表示 ( Z ) 调节 ( X ) 和 ( Y ) 关系的强弱或方向。\n\n\n\n\n\n政府透明度与公众信任\n\n\n自变量（X）：政府透明度（Transparency）。\n因变量（Y）：公众对政府的信任（Public Trust）。\n调节变量（Z）：公民政治参与水平（Political Participation）。\n\n解释：高水平的政治参与可能会增强透明度对公众信任的正面影响，因为公民更关注并感受到透明度带来的好处。\n效果：\n\n高参与：透明度增加显著提高公众信任。\n低参与：透明度的影响不明显。\n\n\n\n\n财政分权与地方经济增长\n\n\n自变量（X）：财政分权程度（Fiscal Decentralization）。\n因变量（Y）：地方经济增长（Local Economic Growth）。\n调节变量（Z）：地方政府问责制（Government Accountability）。\n\n解释：高问责制的地方，财政分权带来的灵活性能更有效地促进经济增长；而低问责制可能导致腐败和低效。\n效果：\n\n高问责制：财政分权显著促进经济增长。\n低问责制：财政分权可能削弱经济增长。\n\n\n\n\n环境政策严格性与污染治理成效\n\n\n自变量（X）：环境政策严格性（Environmental Policy Stringency）。\n因变量（Y）：污染治理效果（Pollution Mitigation Effectiveness）。\n调节变量（Z）：企业技术创新能力（Technological Innovation in Firms）。\n\n解释：具有高技术创新能力的企业能够更好地适应和满足严格的环境政策要求。\n效果：\n\n高创新能力：政策严格性对治理成效有更大影响。\n低创新能力：政策严格性可能导致高成本而无法显著改善污染。\n\n\n\n\n\n\n调节效应通常用交互图（Interaction Plot）展示。\n\n将( X ) 和 ( Y ) 的关系在不同 ( Z ) 水平下的表现。\n\n不同 ( Z ) 水平会呈现不同的斜率或曲线。\n\n\n\n\n一项关于气候变化与灾害Chapman and Lickel 2015的研究中，研究者向211名实验参与者讲述非洲发生干旱造成人道主义危机，告诉其中一半的参与者气候变化是造成干旱的原因，另一半参与者未被告知任何关于干旱的原因。接着通过一系列问题让实验参与者评价拒绝援助的正当性，以及了解他们对气候变化的怀疑程度。最后了解参与者捐款的意愿。 实验的目的是了解框架（frame），即是否告知干旱是由于气候变化产生的，对捐助意愿(donate)的影响数据。\n假定对气候变化怀疑（skeptic）程度较高的人框架对捐助意愿的效应也比较小，会存在调节效应。\n\ndisaster &lt;- read.csv(\"disaster.csv\", header = T)\n# 调节效应\nmoderafit &lt;- lm(donate ~ frame + skeptic + frame:skeptic, data = disaster)\nsummary(moderafit)\n\n\nCall:\nlm(formula = donate ~ frame + skeptic + frame:skeptic, data = disaster)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8341 -0.7077  0.1659  0.9101  2.6682 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    5.02947    0.22632  22.223   &lt;2e-16 ***\nframe          0.67930    0.33091   2.053   0.0413 *  \nskeptic       -0.13953    0.05790  -2.410   0.0168 *  \nframe:skeptic -0.17071    0.08393  -2.034   0.0432 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.234 on 207 degrees of freedom\nMultiple R-squared:  0.1343,    Adjusted R-squared:  0.1218 \nF-statistic: 10.71 on 3 and 207 DF,  p-value: 1.424e-06\n\n\n\n\n\n\n中介效应（Mediation Effect）是统计学和社会科学中的一个概念，指一个中介变量（Mediating Variable）在自变量（Independent Variable, (X)）和因变量（Dependent Variable, (Y)）之间起中介作用，即部分或全部传递自变量对因变量的影响。中介效应主要用于探讨变量之间的作用机制，揭示因果路径。中介效应的分析能够揭示隐藏的机制和作用路径，为理论研究提供深入的解释，并为实践应用提供精细化的决策依据。\n\n\n如果 (X) 通过一个中介变量 (M) 影响 (Y)，这表明 (M) 是 (X) 和 (Y) 之间的中介变量。公式化表达为： [ X M Y ] 其中：\n\n(X)：自变量，独立变量。\n(Y)：因变量，依赖变量。\n(M)：中介变量。\n\n中介效应说明 (X) 不仅直接影响 (Y)（直接效应），还通过 (M) 间接影响 (Y)（间接效应）。\n\n\n\n经典的中介效应模型可以分为三步：\n\n总效应模型：(Y = cX + _1)，其中 (c) 是 (X) 对 (Y) 的总效应。\n中介变量模型：(M = aX + _2)，其中 (a) 是 (X) 对 (M) 的效应。\n结果变量模型：(Y = c’X + bM + _3)，\n\n(c’)：控制 (M) 后，(X) 对 (Y) 的直接效应。\n(b)：(M) 对 (Y) 的效应。\n\n\n间接效应由 (a b) 表示，总效应可以分解为： [ c = c’ + (a b) ]\n\n\n\n\n总效应：自变量对因变量的总影响，包括直接效应和间接效应。\n直接效应：自变量对因变量的直接影响，不通过中介变量传递。\n间接效应：自变量通过中介变量对因变量的影响。\n\n\n\n\n\n完全中介效应（Full Mediation）\n\n(X) 对 (Y) 的影响完全通过 (M) 传递。\n控制 (M) 后，(X) 对 (Y) 的直接效应 (c’) 不显著。\n\n部分中介效应（Partial Mediation）\n\n(X) 对 (Y) 的影响部分通过 (M) 传递。\n控制 (M) 后，(X) 对 (Y) 的直接效应 (c’) 仍然显著。\n\n\n\n\n\n\n教育政策研究\n\n\n自变量：教育投资（Education Investment）。\n中介变量：师资水平（Teacher Quality）。\n因变量：学生成绩（Student Performance）。\n\n解释：教育投资可以通过提升师资水平来间接影响学生成绩。\n\n\n\n公共卫生领域\n\n\n自变量：健康教育活动（Health Education）。\n中介变量：健康知识水平（Health Knowledge）。\n因变量：健康行为（Health Behavior）。\n\n解释：健康教育活动通过提高健康知识水平，进而改变人们的健康行为。\n\n\n\n公共服务数字化与市民满意度\n\n\n自变量（X）：公共服务数字化水平（Digitalization of Public Services）。\n中介变量（M）：服务便利性（Convenience of Service）。\n因变量（Y）：市民满意度（Citizen Satisfaction）。\n\n解释：数字化的服务提高了服务的便利性，间接提升市民对公共服务的满意度。\n\n\n\n\n\n\n逐步回归分析：按照三步模型依次验证每个路径的显著性。\nBootstrap方法：通过重复抽样计算间接效应及其置信区间，常用于提高检验效力。\nSobel检验：检验间接效应是否显著，计算公式为： [ z = ] 其中 () 表示估计系数的标准误。\n\n\n\n\n\n中介效应：关注变量之间的因果路径，解释“为什么”自变量会影响因变量。\n调节效应：关注变量之间的关系强弱，解释“在什么情况下”自变量会影响因变量。\n\n\n\n\n上面气候变化与灾害的研究中，框架可能是通过影响正当性，然后正当性再影响捐助意愿的，即中介效应。\n\nlibrary(lavaan)\n\nThis is lavaan 0.6-19\nlavaan is FREE software! Please report any bugs.\n\n# 中介效应\nmediamodel &lt;- ' # 直接效应\ndonate ~ c*frame\n# 中介效应\njustify ~ a*frame\ndonate ~ b*justify\n# 间接效应 (a*b)\nab := a*b\n# 总效应\ntotal := c + (a*b)\n'\nmediafit &lt;- sem(mediamodel, data = disaster)\nsummary(mediafit)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n  Number of observations                           211\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  donate ~                                            \n    frame      (c)    0.212    0.135    1.576    0.115\n  justify ~                                           \n    frame      (a)    0.134    0.127    1.054    0.292\n  donate ~                                            \n    justify    (b)   -0.953    0.072  -13.159    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .donate            0.948    0.092   10.271    0.000\n   .justify           0.856    0.083   10.271    0.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    ab               -0.128    0.122   -1.051    0.293\n    total             0.084    0.181    0.463    0.643\n\n\n绘制图形\n\nlibrary(\"semPlot\")\nsemPaths(mediafit,whatLabels = 'est',residuals = F, nCharNodes=0, sizeMan = 12,edge.label.cex = 1.5)\n\n\n\n\n\n\n\n\n\n\n\n\n条件过程模型（Conditional Process Model）是一个综合框架，用于同时分析中介效应（Mediation Effect）和调节效应（Moderation Effect）。它探讨自变量通过中介变量影响因变量的机制，同时考虑调节变量如何影响这一机制的强度或方向。条件过程模型是一种强大的分析工具，能够揭示复杂的变量关系，为研究者提供深入的理论解释和实践指导。\n\n\n条件过程模型结合了中介效应和调节效应，回答以下关键问题：\n\n机制问题：自变量如何通过中介变量间接影响因变量？（中介效应）\n情境问题：这种中介效应在什么情况下更强或更弱？（调节效应）\n\n公式化表示为： [ Y = b_1 M + b_2 X + b_3 W + b_4 (M W) + ] 其中：\n\n(X)：自变量。\n(M)：中介变量。\n(Y)：因变量。\n(W)：调节变量。\n(b_4)：调节变量 (W) 对中介效应的调节作用。\n\n\n\n\n\n中介效应的调节：调节变量 (W) 改变了自变量 (X) 通过中介变量 (M) 对因变量 (Y) 的间接影响。\n模型交互：条件过程模型包含交互项（例如 (M W)），用来描述调节效应如何影响中介效应。\n多路径分析：条件过程模型可以同时研究直接效应、间接效应和调节效应。\n\n\n\n\n\n调节的中介效应（Moderated Mediation Effect）\n\n\n定义：调节变量影响中介效应中某一路径的强度或方向。\n解释：调节变量 ((W)) 改变了自变量 ((X)) 通过中介变量 ((M)) 对因变量 ((Y)) 的间接效应。\n(X)：自变量\n(M)：中介变量\n(W)：调节变量，对 (X M) 或 (M Y) 的路径进行调节\n(Y)：因变量\n\n示例：环保政策严格性 ((X)) 通过企业创新 ((M)) 改善环境质量 ((Y))，但监管强度 ((W)) 调节这一过程。\n\n中介的调节效应（Mediated Moderation Effect）\n\n\n定义：中介变量的某一路径影响受调节变量的影响。\n解释：中介变量 ((M)) 解释了调节变量 ((W)) 如何影响自变量 ((X)) 对因变量 ((Y)) 的关系。\n(X W)：自变量和调节变量的交互作用\n(M)：中介变量，解释交互作用如何影响 (Y)\n(Y)：因变量\n\n示例：政策执行力度 ((X)) 和地方经济发展水平 ((W)) 的交互作用通过基层治理能力 ((M)) 间接影响社会稳定 ((Y))。\n\n结合复杂模型（Combined Model）\n\n解释：同时存在调节的中介效应和中介的调节效应，展示了变量之间更复杂的关系。 - (X → M → Y)：中介效应 - (W)：调节变量同时作用于 (X → M) 和 (M → Y) 的路径。\n示例：公共政策透明度 ((X)) 通过公众信任 ((M)) 增强公众满意度 ((Y))，但这种过程因社会参与度 ((W)) 的不同而改变。\n\n\n\n\n基于回归分析的交互项建模\n\n\n\n在回归模型中添加中介变量、调节变量以及交互项（如 (M W)）。\n\n分析每个路径系数的显著性以验证条件过程关系。\n\n\nBootstrap法\n\n\n\n通过Bootstrap重复抽样方法计算间接效应的置信区间。\n\n验证不同调节水平下的中介效应是否显著。\n\n\nPROCESS工具\n\n\n\nPROCESS工具是Andrew F. Hayes开发的一个专用宏，用于SPSS和R语言，可以直接分析复杂的条件过程模型。\n\n\n结构方程的路径分析\n\n\n\n结合各类的结构方程分析软件进行路径分析。\n\n\n\n\n\n综合性强：能够同时分析机制问题（中介效应）和情境问题（调节效应）。\n解释力强：揭示复杂的因果路径和不同情境下的效果差异。\n广泛适用：适用于社会科学、心理学、公共管理等多个领域。\n\n\n\n\n\n综合调节与中介效应，框架对捐助意愿的直接和间接效应是受到怀疑程度的调节的，即被调节的中介效应。\n\n# 具有调节中介效应的条件过程模型\ncpmodel &lt;- ' # 直接效应\ndonate ~ c1*frame + c2*skeptic + c3*skeptic:frame \n# 中介效应\njustify ~ a1*frame + a2*skeptic + a3*skeptic:frame\ndonate ~ b*justify\n\n# 间接效应取决于skeptic的值，需要用平均值（或其他代表值）带入计算间接效应\n# 间接效应 (a*b) skeptic取平均值3.38\na1b := (a1+a3*3.38)*b\n# 总效应\ntotal := c1 + (a1+a3*3.38)*b\n'\ncpfit &lt;- sem(cpmodel, data = disaster)\nsummary(cpfit)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                           211\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  donate ~                                            \n    frame     (c1)    0.160    0.264    0.606    0.544\n    skeptic   (c2)   -0.043    0.046   -0.918    0.359\n    skptc:frm (c3)    0.015    0.068    0.219    0.827\n  justify ~                                           \n    frame     (a1)   -0.562    0.216   -2.606    0.009\n    skeptic   (a2)    0.105    0.038    2.782    0.005\n    skptc:frm (a3)    0.201    0.055    3.675    0.000\n  donate ~                                            \n    justify    (b)   -0.923    0.083  -11.113    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .donate            0.943    0.092   10.271    0.000\n   .justify           0.648    0.063   10.271    0.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    a1b              -0.108    0.103   -1.054    0.292\n    total             0.052    0.285    0.182    0.856\n\n\n图形\n\nsemPaths(cpfit,whatLabels = 'est', layout = \"spring\",,residuals = F, nCharNodes=0, sizeMan = 8,edge.label.cex = 1)"
  },
  {
    "objectID": "analyr/lecture7/sem.html#调节效应和中介效应",
    "href": "analyr/lecture7/sem.html#调节效应和中介效应",
    "title": "调节效应、中介效应和结构方程",
    "section": "",
    "text": "调节效应（Moderation Effect）是统计学和社会科学中的一个概念，指的是某个第三变量（称为调节变量或调节因子）能够改变两个其他变量之间的关系的强度或方向所产生的作用。通过调节效应的分析，可以更好地理解复杂的变量关系，为研究者提供更加精细化的理论指导和建议。\n当一个调节变量存在时，它会通过影响自变量与因变量之间的关系来发挥作用。例如：\n\n如果 ( X ) 是自变量，( Y ) 是因变量，( Z ) 是调节变量，那么调节效应的核心是： [ Y = _0 + _1 X + _2 Z + _3 (X Z) + ] 其中，( _3 ) 是交互项系数，表示 ( Z ) 调节 ( X ) 和 ( Y ) 关系的强弱或方向。\n\n\n\n\n\n政府透明度与公众信任\n\n\n自变量（X）：政府透明度（Transparency）。\n因变量（Y）：公众对政府的信任（Public Trust）。\n调节变量（Z）：公民政治参与水平（Political Participation）。\n\n解释：高水平的政治参与可能会增强透明度对公众信任的正面影响，因为公民更关注并感受到透明度带来的好处。\n效果：\n\n高参与：透明度增加显著提高公众信任。\n低参与：透明度的影响不明显。\n\n\n\n\n财政分权与地方经济增长\n\n\n自变量（X）：财政分权程度（Fiscal Decentralization）。\n因变量（Y）：地方经济增长（Local Economic Growth）。\n调节变量（Z）：地方政府问责制（Government Accountability）。\n\n解释：高问责制的地方，财政分权带来的灵活性能更有效地促进经济增长；而低问责制可能导致腐败和低效。\n效果：\n\n高问责制：财政分权显著促进经济增长。\n低问责制：财政分权可能削弱经济增长。\n\n\n\n\n环境政策严格性与污染治理成效\n\n\n自变量（X）：环境政策严格性（Environmental Policy Stringency）。\n因变量（Y）：污染治理效果（Pollution Mitigation Effectiveness）。\n调节变量（Z）：企业技术创新能力（Technological Innovation in Firms）。\n\n解释：具有高技术创新能力的企业能够更好地适应和满足严格的环境政策要求。\n效果：\n\n高创新能力：政策严格性对治理成效有更大影响。\n低创新能力：政策严格性可能导致高成本而无法显著改善污染。\n\n\n\n\n\n\n调节效应通常用交互图（Interaction Plot）展示。\n\n将( X ) 和 ( Y ) 的关系在不同 ( Z ) 水平下的表现。\n\n不同 ( Z ) 水平会呈现不同的斜率或曲线。\n\n\n\n\n一项关于气候变化与灾害Chapman and Lickel 2015的研究中，研究者向211名实验参与者讲述非洲发生干旱造成人道主义危机，告诉其中一半的参与者气候变化是造成干旱的原因，另一半参与者未被告知任何关于干旱的原因。接着通过一系列问题让实验参与者评价拒绝援助的正当性，以及了解他们对气候变化的怀疑程度。最后了解参与者捐款的意愿。 实验的目的是了解框架（frame），即是否告知干旱是由于气候变化产生的，对捐助意愿(donate)的影响数据。\n假定对气候变化怀疑（skeptic）程度较高的人框架对捐助意愿的效应也比较小，会存在调节效应。\n\ndisaster &lt;- read.csv(\"disaster.csv\", header = T)\n# 调节效应\nmoderafit &lt;- lm(donate ~ frame + skeptic + frame:skeptic, data = disaster)\nsummary(moderafit)\n\n\nCall:\nlm(formula = donate ~ frame + skeptic + frame:skeptic, data = disaster)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8341 -0.7077  0.1659  0.9101  2.6682 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    5.02947    0.22632  22.223   &lt;2e-16 ***\nframe          0.67930    0.33091   2.053   0.0413 *  \nskeptic       -0.13953    0.05790  -2.410   0.0168 *  \nframe:skeptic -0.17071    0.08393  -2.034   0.0432 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.234 on 207 degrees of freedom\nMultiple R-squared:  0.1343,    Adjusted R-squared:  0.1218 \nF-statistic: 10.71 on 3 and 207 DF,  p-value: 1.424e-06\n\n\n\n\n\n\n中介效应（Mediation Effect）是统计学和社会科学中的一个概念，指一个中介变量（Mediating Variable）在自变量（Independent Variable, (X)）和因变量（Dependent Variable, (Y)）之间起中介作用，即部分或全部传递自变量对因变量的影响。中介效应主要用于探讨变量之间的作用机制，揭示因果路径。中介效应的分析能够揭示隐藏的机制和作用路径，为理论研究提供深入的解释，并为实践应用提供精细化的决策依据。\n\n\n如果 (X) 通过一个中介变量 (M) 影响 (Y)，这表明 (M) 是 (X) 和 (Y) 之间的中介变量。公式化表达为： [ X M Y ] 其中：\n\n(X)：自变量，独立变量。\n(Y)：因变量，依赖变量。\n(M)：中介变量。\n\n中介效应说明 (X) 不仅直接影响 (Y)（直接效应），还通过 (M) 间接影响 (Y)（间接效应）。\n\n\n\n经典的中介效应模型可以分为三步：\n\n总效应模型：(Y = cX + _1)，其中 (c) 是 (X) 对 (Y) 的总效应。\n中介变量模型：(M = aX + _2)，其中 (a) 是 (X) 对 (M) 的效应。\n结果变量模型：(Y = c’X + bM + _3)，\n\n(c’)：控制 (M) 后，(X) 对 (Y) 的直接效应。\n(b)：(M) 对 (Y) 的效应。\n\n\n间接效应由 (a b) 表示，总效应可以分解为： [ c = c’ + (a b) ]\n\n\n\n\n总效应：自变量对因变量的总影响，包括直接效应和间接效应。\n直接效应：自变量对因变量的直接影响，不通过中介变量传递。\n间接效应：自变量通过中介变量对因变量的影响。\n\n\n\n\n\n完全中介效应（Full Mediation）\n\n(X) 对 (Y) 的影响完全通过 (M) 传递。\n控制 (M) 后，(X) 对 (Y) 的直接效应 (c’) 不显著。\n\n部分中介效应（Partial Mediation）\n\n(X) 对 (Y) 的影响部分通过 (M) 传递。\n控制 (M) 后，(X) 对 (Y) 的直接效应 (c’) 仍然显著。\n\n\n\n\n\n\n教育政策研究\n\n\n自变量：教育投资（Education Investment）。\n中介变量：师资水平（Teacher Quality）。\n因变量：学生成绩（Student Performance）。\n\n解释：教育投资可以通过提升师资水平来间接影响学生成绩。\n\n\n\n公共卫生领域\n\n\n自变量：健康教育活动（Health Education）。\n中介变量：健康知识水平（Health Knowledge）。\n因变量：健康行为（Health Behavior）。\n\n解释：健康教育活动通过提高健康知识水平，进而改变人们的健康行为。\n\n\n\n公共服务数字化与市民满意度\n\n\n自变量（X）：公共服务数字化水平（Digitalization of Public Services）。\n中介变量（M）：服务便利性（Convenience of Service）。\n因变量（Y）：市民满意度（Citizen Satisfaction）。\n\n解释：数字化的服务提高了服务的便利性，间接提升市民对公共服务的满意度。\n\n\n\n\n\n\n逐步回归分析：按照三步模型依次验证每个路径的显著性。\nBootstrap方法：通过重复抽样计算间接效应及其置信区间，常用于提高检验效力。\nSobel检验：检验间接效应是否显著，计算公式为： [ z = ] 其中 () 表示估计系数的标准误。\n\n\n\n\n\n中介效应：关注变量之间的因果路径，解释“为什么”自变量会影响因变量。\n调节效应：关注变量之间的关系强弱，解释“在什么情况下”自变量会影响因变量。\n\n\n\n\n上面气候变化与灾害的研究中，框架可能是通过影响正当性，然后正当性再影响捐助意愿的，即中介效应。\n\nlibrary(lavaan)\n\nThis is lavaan 0.6-19\nlavaan is FREE software! Please report any bugs.\n\n# 中介效应\nmediamodel &lt;- ' # 直接效应\ndonate ~ c*frame\n# 中介效应\njustify ~ a*frame\ndonate ~ b*justify\n# 间接效应 (a*b)\nab := a*b\n# 总效应\ntotal := c + (a*b)\n'\nmediafit &lt;- sem(mediamodel, data = disaster)\nsummary(mediafit)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n  Number of observations                           211\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  donate ~                                            \n    frame      (c)    0.212    0.135    1.576    0.115\n  justify ~                                           \n    frame      (a)    0.134    0.127    1.054    0.292\n  donate ~                                            \n    justify    (b)   -0.953    0.072  -13.159    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .donate            0.948    0.092   10.271    0.000\n   .justify           0.856    0.083   10.271    0.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    ab               -0.128    0.122   -1.051    0.293\n    total             0.084    0.181    0.463    0.643\n\n\n绘制图形\n\nlibrary(\"semPlot\")\nsemPaths(mediafit,whatLabels = 'est',residuals = F, nCharNodes=0, sizeMan = 12,edge.label.cex = 1.5)\n\n\n\n\n\n\n\n\n\n\n\n\n条件过程模型（Conditional Process Model）是一个综合框架，用于同时分析中介效应（Mediation Effect）和调节效应（Moderation Effect）。它探讨自变量通过中介变量影响因变量的机制，同时考虑调节变量如何影响这一机制的强度或方向。条件过程模型是一种强大的分析工具，能够揭示复杂的变量关系，为研究者提供深入的理论解释和实践指导。\n\n\n条件过程模型结合了中介效应和调节效应，回答以下关键问题：\n\n机制问题：自变量如何通过中介变量间接影响因变量？（中介效应）\n情境问题：这种中介效应在什么情况下更强或更弱？（调节效应）\n\n公式化表示为： [ Y = b_1 M + b_2 X + b_3 W + b_4 (M W) + ] 其中：\n\n(X)：自变量。\n(M)：中介变量。\n(Y)：因变量。\n(W)：调节变量。\n(b_4)：调节变量 (W) 对中介效应的调节作用。\n\n\n\n\n\n中介效应的调节：调节变量 (W) 改变了自变量 (X) 通过中介变量 (M) 对因变量 (Y) 的间接影响。\n模型交互：条件过程模型包含交互项（例如 (M W)），用来描述调节效应如何影响中介效应。\n多路径分析：条件过程模型可以同时研究直接效应、间接效应和调节效应。\n\n\n\n\n\n调节的中介效应（Moderated Mediation Effect）\n\n\n定义：调节变量影响中介效应中某一路径的强度或方向。\n解释：调节变量 ((W)) 改变了自变量 ((X)) 通过中介变量 ((M)) 对因变量 ((Y)) 的间接效应。\n(X)：自变量\n(M)：中介变量\n(W)：调节变量，对 (X M) 或 (M Y) 的路径进行调节\n(Y)：因变量\n\n示例：环保政策严格性 ((X)) 通过企业创新 ((M)) 改善环境质量 ((Y))，但监管强度 ((W)) 调节这一过程。\n\n中介的调节效应（Mediated Moderation Effect）\n\n\n定义：中介变量的某一路径影响受调节变量的影响。\n解释：中介变量 ((M)) 解释了调节变量 ((W)) 如何影响自变量 ((X)) 对因变量 ((Y)) 的关系。\n(X W)：自变量和调节变量的交互作用\n(M)：中介变量，解释交互作用如何影响 (Y)\n(Y)：因变量\n\n示例：政策执行力度 ((X)) 和地方经济发展水平 ((W)) 的交互作用通过基层治理能力 ((M)) 间接影响社会稳定 ((Y))。\n\n结合复杂模型（Combined Model）\n\n解释：同时存在调节的中介效应和中介的调节效应，展示了变量之间更复杂的关系。 - (X → M → Y)：中介效应 - (W)：调节变量同时作用于 (X → M) 和 (M → Y) 的路径。\n示例：公共政策透明度 ((X)) 通过公众信任 ((M)) 增强公众满意度 ((Y))，但这种过程因社会参与度 ((W)) 的不同而改变。\n\n\n\n\n基于回归分析的交互项建模\n\n\n\n在回归模型中添加中介变量、调节变量以及交互项（如 (M W)）。\n\n分析每个路径系数的显著性以验证条件过程关系。\n\n\nBootstrap法\n\n\n\n通过Bootstrap重复抽样方法计算间接效应的置信区间。\n\n验证不同调节水平下的中介效应是否显著。\n\n\nPROCESS工具\n\n\n\nPROCESS工具是Andrew F. Hayes开发的一个专用宏，用于SPSS和R语言，可以直接分析复杂的条件过程模型。\n\n\n结构方程的路径分析\n\n\n\n结合各类的结构方程分析软件进行路径分析。\n\n\n\n\n\n综合性强：能够同时分析机制问题（中介效应）和情境问题（调节效应）。\n解释力强：揭示复杂的因果路径和不同情境下的效果差异。\n广泛适用：适用于社会科学、心理学、公共管理等多个领域。\n\n\n\n\n\n综合调节与中介效应，框架对捐助意愿的直接和间接效应是受到怀疑程度的调节的，即被调节的中介效应。\n\n# 具有调节中介效应的条件过程模型\ncpmodel &lt;- ' # 直接效应\ndonate ~ c1*frame + c2*skeptic + c3*skeptic:frame \n# 中介效应\njustify ~ a1*frame + a2*skeptic + a3*skeptic:frame\ndonate ~ b*justify\n\n# 间接效应取决于skeptic的值，需要用平均值（或其他代表值）带入计算间接效应\n# 间接效应 (a*b) skeptic取平均值3.38\na1b := (a1+a3*3.38)*b\n# 总效应\ntotal := c1 + (a1+a3*3.38)*b\n'\ncpfit &lt;- sem(cpmodel, data = disaster)\nsummary(cpfit)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                           211\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  donate ~                                            \n    frame     (c1)    0.160    0.264    0.606    0.544\n    skeptic   (c2)   -0.043    0.046   -0.918    0.359\n    skptc:frm (c3)    0.015    0.068    0.219    0.827\n  justify ~                                           \n    frame     (a1)   -0.562    0.216   -2.606    0.009\n    skeptic   (a2)    0.105    0.038    2.782    0.005\n    skptc:frm (a3)    0.201    0.055    3.675    0.000\n  donate ~                                            \n    justify    (b)   -0.923    0.083  -11.113    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .donate            0.943    0.092   10.271    0.000\n   .justify           0.648    0.063   10.271    0.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    a1b              -0.108    0.103   -1.054    0.292\n    total             0.052    0.285    0.182    0.856\n\n\n图形\n\nsemPaths(cpfit,whatLabels = 'est', layout = \"spring\",,residuals = F, nCharNodes=0, sizeMan = 8,edge.label.cex = 1)"
  },
  {
    "objectID": "analyr/lecture7/sem.html#结构方程的特点",
    "href": "analyr/lecture7/sem.html#结构方程的特点",
    "title": "调节效应、中介效应和结构方程",
    "section": "结构方程的特点",
    "text": "结构方程的特点\n\n结构方程分析需要建立在理论基础上，从变量的测量、变量关系的假定和模型的设定都需要有清晰的理论支持或逻辑推理作为依据。\n结构方程模型同时处理潜变量的测量和变量间关系的分析，变量测量中的误差也被包含在变量关系的分析过程中。\n结构方程是基于变量间的协方差进行分析，协方差能够反映变量间的关联，也能反映理论模型生成的协方差与实际观测所得的协方差之间的差异。\n结构方程需要大样本，样本量大于200。\n结构方程的评估基于多重指标对整体模型进行比较，不依赖于单一的统计显著性。"
  },
  {
    "objectID": "analyr/lecture7/sem.html#结构方程模型分析的过程",
    "href": "analyr/lecture7/sem.html#结构方程模型分析的过程",
    "title": "调节效应、中介效应和结构方程",
    "section": "结构方程模型分析的过程",
    "text": "结构方程模型分析的过程"
  },
  {
    "objectID": "analyr/lecture7/sem.html#结构方程的组成",
    "href": "analyr/lecture7/sem.html#结构方程的组成",
    "title": "调节效应、中介效应和结构方程",
    "section": "结构方程的组成",
    "text": "结构方程的组成\n\n结构方程的变量：\n\n结构方程主要变量为尺度变量，类别变量只作为分组讨论的调节变量。\n潜变量(F)必须有两个以上（一般为3个以上，2个的情况需要模型存在多个潜变量，且之间存在关联性）的测量变量(V1, V2)，测量变量间的协方差反映潜在变量的共同影响，测量变量无法被潜变量解释的部分为测量误差(E1, E2)。\n内生潜变量所影响（对应）的为内生测量变量，外生潜变量所影响（对应）的为外生测量变量，内生潜变量的残差称为干扰项。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n结构方程的参数：\n\n测量模型参数包括潜变量与测量变量的关联强度\\(\\lambda\\)，也成为因子载荷，外生测量变量的测量误差\\(\\delta\\)，内生测量变量的测量误差\\(\\varepsilon\\)，外生变量的协方差\\(\\phi\\)。\n结构模型参数包括外生潜变量与内生潜变量之间的关系\\(\\gamma\\)，内生潜变量之间的关系\\(\\beta\\)，内生潜变量的残差或干扰项\\(\\zeta\\)。\n\n\n\n结构方程模型设定\n\n简效原则：将变量间的关系以最符合理论又最简单扼要的方式加以设定。如果一个简单模型能够解释较多实际数据的变化，那么以这个模型来说明数据的关系，比较不会得到错误的结论，结构方程可以防止弃真错误，难以防止纳伪错误。\n结构方程会遇到等值模型问题，即不同设定的模型拟合优度相等，可以通过前导理论策略（通过理论排除对等的模型）或参数竞争比较策略（比较对等模型参数估计）来解决。"
  },
  {
    "objectID": "analyr/lecture7/sem.html#结构方程模型识别",
    "href": "analyr/lecture7/sem.html#结构方程模型识别",
    "title": "调节效应、中介效应和结构方程",
    "section": "结构方程模型识别",
    "text": "结构方程模型识别\n\n结构方程的不同设定会产生模型识别问题，只有在过度识别条件下，才能对模型参数进行计算估计。通过比较待估参数数量\\(t\\)和测量数据量\\(DP\\)之间的大小，可以判断（必要不充分）模型能否参数估计。\n\n当\\(t &lt; DP\\)，为过度识别，如同有三个方程，求两个未知数的解。\n当\\(t = DP\\)，为充分识别，如同有两个个方程，求两个未知数的解。\n当\\(t &gt; DP\\)，为识别不足，如同只有一个方程，求两个未知数的解。\n其中，待估参数数量\\(t\\)根据具体的模型设定决定，测量数据量\\(DP\\)由外生测量变量的个数\\(p\\)和内生测量变量的个数\\(q\\)计算。 \\[DP=\\frac{(p+q)(p+q+1)}{2}\\]\n\n当结构方程模型设定没有结构关系的假设，模型可以顺利识别（Null Beta Rule）；当结构方程模型设定只估计结构参数，干扰项只估计方差不估计相关，模型自动识别（递归法则）。尽量保持简单的模型结构。"
  },
  {
    "objectID": "analyr/lecture7/sem.html#结构方程模型拟合评估",
    "href": "analyr/lecture7/sem.html#结构方程模型拟合评估",
    "title": "调节效应、中介效应和结构方程",
    "section": "结构方程模型拟合评估",
    "text": "结构方程模型拟合评估\n\n结构方程模型分析策略与一般统计推断有明显差异，是以支持原假设作为模型拟合度存在的证据。\n结构方程的原假设是偏好的模型（根据理论对结构系数做出了某些假定，某些系数不为零）与实际观察的数据是相符的，其检验则是通过与饱和模型（对各观察变量之间关系都做了相关假定，所有系数均不为零）相比较，如果模型之间卡方检验值比较大，则说明两个模型存在显著差异，拒绝原假设（拒绝偏好的模型），接受饱和模型。\n模型的卡方统计量、自由度和p值，卡方值越小越好，p值大于0.05。但是样本量越大，p值会减少倾向显著，作为拟合优度指标并不准确，需要考虑其他拟合指数。如果模型正确，卡方统计量会等于其自由度，但增加估计参数的数量会减小卡方值。\n近似均方根残差Root Mean Square Error of Approximation (RMSEA; Steiger, 1990) 和90%的置信区间，越小越好，小于0.08比较理想。\n比较拟合指数Comparative Fit Index (CFI; Bentler, 1990)，越大越好，应大于0.9。\n标准化均方根残差Standardized Root Mean Square Residual (SRMR)，越小越好，大于0.1说明拟合不好。"
  },
  {
    "objectID": "analyr/lecture7/sem.html#验证性因子分析",
    "href": "analyr/lecture7/sem.html#验证性因子分析",
    "title": "调节效应、中介效应和结构方程",
    "section": "验证性因子分析",
    "text": "验证性因子分析\n\n验证性因子分析的原理\n\n不同于探索性因子分析（EFA），验证性因子分析（CFA）必须有特定的理论或概念架构作为基础。CFA可以作为结构方程模型的前置步骤，也可以独立进行，只检验测量模型。\nCFA要求构念要有明确的操作化定义界定内容与范畴；测量构念的指标要能被明确指出，并且同一构念指标要具有相当的一致性，不同构念指标要具有区分度；还要经过统计验证观察数据能否支持或推翻构念是否存在的假设。\nCFA模型中，从潜变量到测量变量的箭头，代表研究者所假设的潜变量到测量变量的因果关系，统计估计量称为因子载荷，类似于回归系数，测量变量的变异可以拆解为共同变异（common variance）和独特变异（unique variance）。\n与回归模型将测量误差作为随机误差处理不同，CFA将测量变量中分离独特变异，这种变异包括了随机误差和系统误差（例如方法效益带来的误差，会导致题项相关系数偏大）。CFA可以通过共变关系分析和多维测量假设有效估计独特变异中的系统性误差。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCFA的内部拟合检验\n\nCFA除了要求整体拟合效果外，还要求针对个别因子的质量进行检验，了解个别参数是否理想（项目信效度），各潜变量的组合情形是否稳定可靠（构念的信效度）。遇到不理想的参数，可以剔除不良题项或添加参数提高测量模型的内在拟合。\n项目质量检验：题项测量误差越小则信度越高，而因子载荷越高则测量误差越小，所以可以用因子载荷来判断题项质量。一般当因子载荷\\(\\lambda\\)大于0.71时，\\(\\lambda^2\\)为50%，意味着潜变量能够解释测量变量50%的变异（即回归的R方），说明项目具有理想质量。但是对于社会科学而言，编制的量表因子载荷都不会太高，因子载荷\\(\\lambda\\)大于0.55（\\(\\lambda^2\\)为30%）即是理想的结果。\n组合信度（\\(\\rho_c\\)）：对于一组题项而言，潜变量的变异代表真实分数的变异，因此题项的组合信度可以用测量变量变异被潜变量解释的百分比来表示（类似于内部一致性系数，\\(Cronbach's \\quad \\alpha\\)）。一般量表信度需达到0.7，社会科学领域不易达到此水平，0.5以上可认为获得基本稳定性。\n\n\\[\\rho_c=\\frac{(\\Sigma\\lambda_i)^2}{((\\Sigma\\lambda_i)^2 + \\Sigma\\Theta_{ii}+2\\Sigma\\Theta_{ij})}\\]\n\n平均变异萃取量（\\(\\rho_\\nu\\)）：测量题项的因子载荷越高，表示题项能够反映潜变量的能力越高，潜变量因子能够解释各个测量变量变异的程度越大，因此可以用平均变异萃取量（即EFA中的特征值）反映潜变量被测量变量有效估计的聚敛程度。\\(\\rho_\\nu\\)大于0.5，表示潜变量聚敛能力理想。\n\n\\[\\rho_{\\nu}=\\frac{\\Sigma\\lambda^2_i}{(\\Sigma\\lambda^2_i + \\Sigma\\Theta_{ii})}=\\frac{\\Sigma\\lambda^2_i}{n}\\]\n\n因素区辩力：不同潜变量之间必须能够有效分离。\n\n相关系数区间估计：如果两个潜变量的相关系数的95%置信区间包含1，表示构念缺乏区辩力。\n\n竞争模式比较法：将设定CFA模型与完全相关模型（将潜变量的相关设定为1）相比较，如果两个模型没有区别，表示构念缺乏区辩力。\n\n平均变异萃取量比较法：比较两个潜变量的平均变异萃取量的平均值是否大于其相关系数的平方。\n\n\n\n\n验证性因素分析的步骤\n\n建立测量模型的假设\n进行模型识别，输入模型指令\n执行CFA分析\n结果分析\n模型修正\n完成分析，给出报告\n\n\n\n组织创新气氛测量模型\n案例来自《组织创新气氛量表》（邱皓政，1999），样本是384位企业员工，量表为Likert式6点度量的自陈量表，基于理论和文献先界定影响组织气氛知觉的因素包括组织价值、工作方式、团队合作、领导风格、学习成长、环境气氛等6个因素，每个因素采用3个题项测量，共有18个题项，题项描述性统计如下。\n\nlibrary(haven)\nlibrary(tidyverse)\nlibrary(modelsummary)\nlibrary(lavaan)\nlibrary(semPlot)\n\ndat &lt;- read_sav(\"ch05.sav\")\nnames(dat) &lt;- c(\"A1\",\"A2\",\"A3\",\"B1\",\"B2\",\"B3\",\"C1\",\"C2\",\"C3\",\"D1\",\"D2\",\"D3\",\"E1\",\"E2\",\"E3\",\"F1\",\"F2\",\"F3\")\ndat &lt;- zap_labels(dat)\ndim(dat) \n\n[1] 313  18\n\n\n\ndatasummary_skim(dat, fun_numeric = list('取值'=NUnique, '缺失值'=PercentMissing, '均值'=Mean, '中位数'=Median,'标准差'=SD, '最小值'=Min, '最大值'=Max), output = \"data.frame\", type = \"numeric\", fmt_sprintf(\"%.2f\")) |&gt; knitr::kable()\n\n\n\n\n\n取值\n缺失值\n均值\n中位数\n标准差\n最小值\n最大值\n\n\n\n\nA1\n6\n0\n4.42\n5.00\n0.98\n1.00\n6.00\n\n\nA2\n6\n0\n4.31\n4.00\n1.02\n1.00\n6.00\n\n\nA3\n6\n0\n4.07\n4.00\n0.97\n1.00\n6.00\n\n\nB1\n6\n0\n4.02\n4.00\n1.16\n1.00\n6.00\n\n\nB2\n6\n0\n4.25\n4.00\n1.16\n1.00\n6.00\n\n\nB3\n6\n0\n4.24\n4.00\n1.09\n1.00\n6.00\n\n\nC1\n6\n0\n4.37\n4.00\n0.98\n1.00\n6.00\n\n\nC2\n6\n0\n4.34\n4.00\n1.03\n1.00\n6.00\n\n\nC3\n6\n0\n4.31\n4.00\n1.05\n1.00\n6.00\n\n\nD1\n6\n0\n4.83\n5.00\n0.94\n1.00\n6.00\n\n\nD2\n5\n0\n4.95\n5.00\n0.84\n2.00\n6.00\n\n\nD3\n5\n0\n4.83\n5.00\n0.91\n2.00\n6.00\n\n\nE1\n6\n0\n4.63\n5.00\n0.97\n1.00\n6.00\n\n\nE2\n6\n0\n4.73\n5.00\n1.01\n1.00\n6.00\n\n\nE3\n6\n0\n4.70\n5.00\n0.98\n1.00\n6.00\n\n\nF1\n6\n0\n4.23\n4.00\n1.17\n1.00\n6.00\n\n\nF2\n6\n0\n4.63\n5.00\n1.09\n1.00\n6.00\n\n\nF3\n6\n0\n4.49\n5.00\n0.94\n1.00\n6.00\n\n\n\n\n\n\n测量模型的设定\n\n模型有18个测量变量和6个潜变量\n模型中18个测量误差\n为确定6个潜变量的度量，每个因素方差设定为1\n每个测量变量只受单一潜变量影响，因此有18个因子载荷参数\n因子共变允许自由估计，产生15个相关系数参数\n测量误差之间视为独立，没有共变关系\n\n\n\n\n\n\n\n\n\n\n\n\n测量模型的识别\n\n测量数据\\(DP=18\\times(18+1)/2=171\\)\n模型待估参数包括18个因子载荷、18个测量误差、15个潜变量协方差（潜变量方差设定为1，不需要估计），因此待估参数\\(t=18+18+15=51\\)\n因为待估参数小于测量数据，所以测量模型可以识别\n\n\n\n\n模型分析与报告\nlavaan包的模型设定默认不同潜变量之间具有相关性（即允许因子共变），如果需要不同的设定，可以在cfa函数中用\\(orthogonal=T\\)将协方差约束为0；cfa函数默认是将第1个因子载荷设定为1，如需固定潜变量方差为1，可以通过\\(std.lv=TRUE\\)来设定。\n\ncfa_model  &lt;-'\n#定义测量模型  \n  FA =~ L11*A1 + L21*A2 + L31*A3\n  FB =~ L12*B1 + L22*B2 + L32*B3\n  FC =~ L13*C1 + L23*C2 + L33*C3\n  FD =~ L14*D1 + L24*D2 + L34*D3\n  FE =~ L15*E1 + L25*E2 + L35*E3\n  FF =~ L16*F1 + L26*F2 + L36*F3'\n\ncfa_fit &lt;- cfa(model = cfa_model, \n               data = dat,\n               std.lv = TRUE) # 根据模型设定，固定潜变量方差为1\nsummary(cfa_fit, \n        fit.measures = T, # 输出拟合指标\n        standard = T)  # 输出标准化解\n\nlavaan 0.6-19 ended normally after 31 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        51\n\n  Number of observations                           313\n\nModel Test User Model:\n                                                      \n  Test statistic                               241.755\n  Degrees of freedom                               120\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              2842.819\n  Degrees of freedom                               153\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.955\n  Tucker-Lewis Index (TLI)                       0.942\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6751.785\n  Loglikelihood unrestricted model (H1)      -6630.907\n                                                      \n  Akaike (AIC)                               13605.569\n  Bayesian (BIC)                             13796.626\n  Sample-size adjusted Bayesian (SABIC)      13634.870\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.057\n  90 Percent confidence interval - lower         0.047\n  90 Percent confidence interval - upper         0.067\n  P-value H_0: RMSEA &lt;= 0.050                    0.132\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.052\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  FA =~                                                                 \n    A1       (L11)    0.815    0.051   15.967    0.000    0.815    0.830\n    A2       (L21)    0.706    0.055   12.733    0.000    0.706    0.692\n    A3       (L31)    0.614    0.054   11.430    0.000    0.614    0.634\n  FB =~                                                                 \n    B1       (L12)    0.789    0.062   12.759    0.000    0.789    0.682\n    B2       (L22)    0.961    0.058   16.591    0.000    0.961    0.833\n    B3       (L32)    0.856    0.056   15.406    0.000    0.856    0.788\n  FC =~                                                                 \n    C1       (L13)    0.699    0.053   13.178    0.000    0.699    0.717\n    C2       (L23)    0.736    0.056   13.119    0.000    0.736    0.715\n    C3       (L33)    0.696    0.058   11.950    0.000    0.696    0.663\n  FD =~                                                                 \n    D1       (L14)    0.810    0.045   18.177    0.000    0.810    0.867\n    D2       (L24)    0.741    0.040   18.750    0.000    0.741    0.886\n    D3       (L34)    0.655    0.047   14.086    0.000    0.655    0.720\n  FE =~                                                                 \n    E1       (L15)    0.806    0.046   17.392    0.000    0.806    0.830\n    E2       (L25)    0.912    0.046   19.851    0.000    0.912    0.906\n    E3       (L35)    0.791    0.047   16.810    0.000    0.791    0.811\n  FF =~                                                                 \n    F1       (L16)    0.641    0.066    9.663    0.000    0.641    0.550\n    F2       (L26)    0.828    0.058   14.296    0.000    0.828    0.758\n    F3       (L36)    0.786    0.049   16.142    0.000    0.786    0.837\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  FA ~~                                                                 \n    FB                0.542    0.054    9.989    0.000    0.542    0.542\n    FC                0.494    0.061    8.102    0.000    0.494    0.494\n    FD                0.417    0.058    7.176    0.000    0.417    0.417\n    FE                0.526    0.052   10.080    0.000    0.526    0.526\n    FF                0.695    0.046   15.148    0.000    0.695    0.695\n  FB ~~                                                                 \n    FC                0.697    0.047   14.906    0.000    0.697    0.697\n    FD                0.447    0.055    8.127    0.000    0.447    0.447\n    FE                0.575    0.048   12.091    0.000    0.575    0.575\n    FF                0.391    0.061    6.403    0.000    0.391    0.391\n  FC ~~                                                                 \n    FD                0.522    0.055    9.493    0.000    0.522    0.522\n    FE                0.603    0.050   12.127    0.000    0.603    0.603\n    FF                0.600    0.054   11.032    0.000    0.600    0.600\n  FD ~~                                                                 \n    FE                0.557    0.046   12.046    0.000    0.557    0.557\n    FF                0.316    0.062    5.137    0.000    0.316    0.316\n  FE ~~                                                                 \n    FF                0.443    0.056    7.944    0.000    0.443    0.443\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .A1                0.300    0.046    6.509    0.000    0.300    0.311\n   .A2                0.544    0.055    9.980    0.000    0.544    0.522\n   .A3                0.561    0.052   10.700    0.000    0.561    0.598\n   .B1                0.716    0.068   10.525    0.000    0.716    0.535\n   .B2                0.408    0.057    7.211    0.000    0.408    0.306\n   .B3                0.447    0.052    8.602    0.000    0.447    0.379\n   .C1                0.461    0.050    9.305    0.000    0.461    0.485\n   .C2                0.519    0.055    9.354    0.000    0.519    0.489\n   .C3                0.618    0.061   10.173    0.000    0.618    0.560\n   .D1                0.217    0.031    7.055    0.000    0.217    0.248\n   .D2                0.151    0.024    6.204    0.000    0.151    0.215\n   .D3                0.399    0.037   10.799    0.000    0.399    0.482\n   .E1                0.292    0.032    9.243    0.000    0.292    0.311\n   .E2                0.181    0.030    6.113    0.000    0.181    0.179\n   .E3                0.325    0.033    9.733    0.000    0.325    0.342\n   .F1                0.948    0.083   11.400    0.000    0.948    0.698\n   .F2                0.507    0.058    8.700    0.000    0.507    0.425\n   .F3                0.265    0.042    6.276    0.000    0.265    0.300\n    FA                1.000                               1.000    1.000\n    FB                1.000                               1.000    1.000\n    FC                1.000                               1.000    1.000\n    FD                1.000                               1.000    1.000\n    FE                1.000                               1.000    1.000\n    FF                1.000                               1.000    1.000\n\n\n\n模型结果的报告\nsummary报告第一部分为模型基本信息，估计方法为最大似然估计，优化方法为非线性最小优化，待估参数51个，样本量313。\nsummary报告第二部分为拟合指标，包括卡方值和检验结果、CFI、RMSEA、SRMR等。\n\n模型拟合度分析：对照前面的标准，卡方值显著性水平\\(0&lt;0.05\\)，表示假设模型与观察值之间有显著的差异；\\(\\chi^2/df&gt;2\\)也说明拟合度不理想；NFI、NNFI、CFI均大于标准0.9，SRMR小于标准0.08，这四个指标都表明拟合比较理想；RMSEA为0.057略微大于标准0.05，表明模型拟合不理想；综合来看，理论模型没有达到最佳的拟合度，仍有修正的空间。\n\n\nfitmeasures(cfa_fit, c(\"chisq\", \"df\", \"pvalue\", \"nfi\",\"nnfi\",\"cfi\", \"rmsea\", \"srmr\"))\n\n  chisq      df  pvalue     nfi    nnfi     cfi   rmsea    srmr \n241.755 120.000   0.000   0.915   0.942   0.955   0.057   0.052 \n\n\nsummary报告第三部分为参数估计结果，包括因子载荷、潜变量协方差、测量误差和潜变量的方差。\n\n潜变量（Latent variables）部分为因子载荷参数估计值相关结果，std.lv对应的是潜变量被标准化时的载荷，std.all对应的是所有变量都被标准化时的载荷（类似于标准化的回归系数）。\n\n协方差（Covariances）部分为潜变量协方差，由于潜变量方差设定为1，此处协方差也等于潜变量间的相关系数。\n\n方差（Variances）部分为测量误差的方差和潜变量的方差，由于潜变量方差设定为1，所以此处均为1。\n\n\n\n残差分析\n模型拟合后可以依据拟合协方差矩阵列出估计的测量变量的方差和协方差，这些模型导出数与实际观察值之间的差距即为残差。通过分析标准化残差的大小和分布，可以了解测量变量间关系的拟合程度，确定存在问题的题项。\n\nfitted(cfa_fit) # 提取拟合协方差矩阵\n\n$cov\n      A1    A2    A3    B1    B2    B3    C1    C2    C3    D1    D2    D3\nA1 0.965                                                                  \nA2 0.576 1.043                                                            \nA3 0.500 0.433 0.937                                                      \nB1 0.349 0.302 0.262 1.338                                                \nB2 0.425 0.368 0.320 0.759 1.332                                          \nB3 0.378 0.328 0.285 0.675 0.823 1.179                                    \nC1 0.282 0.244 0.212 0.385 0.469 0.417 0.950                              \nC2 0.297 0.257 0.223 0.405 0.494 0.439 0.515 1.061                        \nC3 0.280 0.243 0.211 0.383 0.467 0.415 0.487 0.512 1.102                  \nD1 0.275 0.239 0.207 0.286 0.348 0.310 0.296 0.312 0.295 0.873            \nD2 0.252 0.218 0.190 0.262 0.319 0.284 0.271 0.285 0.269 0.601 0.700      \nD3 0.223 0.193 0.168 0.231 0.282 0.251 0.239 0.252 0.238 0.531 0.486 0.829\nE1 0.345 0.299 0.260 0.365 0.445 0.396 0.340 0.358 0.338 0.363 0.332 0.294\nE2 0.391 0.339 0.294 0.413 0.504 0.448 0.385 0.405 0.383 0.411 0.376 0.333\nE3 0.339 0.294 0.255 0.359 0.437 0.389 0.334 0.352 0.332 0.357 0.326 0.289\nF1 0.363 0.315 0.274 0.198 0.241 0.214 0.269 0.283 0.268 0.164 0.150 0.133\nF2 0.469 0.407 0.353 0.255 0.311 0.277 0.347 0.366 0.346 0.212 0.194 0.171\nF3 0.446 0.386 0.335 0.243 0.296 0.263 0.330 0.347 0.328 0.201 0.184 0.163\n      E1    E2    E3    F1    F2    F3\nA1                                    \nA2                                    \nA3                                    \nB1                                    \nB2                                    \nB3                                    \nC1                                    \nC2                                    \nC3                                    \nD1                                    \nD2                                    \nD3                                    \nE1 0.942                              \nE2 0.735 1.013                        \nE3 0.638 0.721 0.951                  \nF1 0.229 0.259 0.225 1.359            \nF2 0.296 0.335 0.291 0.531 1.193      \nF3 0.281 0.318 0.276 0.504 0.651 0.883\n\ncfares &lt;- resid(cfa_fit)  # 提取残差\ncfares$cov\n\n       A1     A2     A3     B1     B2     B3     C1     C2     C3     D1     D2\nA1  0.000                                                                      \nA2 -0.003  0.000                                                               \nA3  0.022 -0.037  0.000                                                        \nB1 -0.080  0.057  0.037  0.000                                                 \nB2 -0.009  0.104 -0.057  0.001  0.000                                          \nB3 -0.021  0.053 -0.011  0.002 -0.001  0.000                                   \nC1 -0.085  0.022 -0.054  0.022  0.015  0.024  0.000                            \nC2 -0.019  0.039 -0.101  0.005 -0.020 -0.041  0.042  0.000                     \nC3  0.070  0.167  0.079 -0.004  0.002  0.006 -0.050 -0.004  0.000              \nD1  0.030  0.113 -0.048 -0.012  0.026  0.043 -0.052 -0.006  0.022  0.000       \nD2 -0.033  0.021 -0.071 -0.031 -0.041 -0.029 -0.076 -0.003  0.018  0.008  0.000\nD3 -0.010  0.075 -0.050 -0.005  0.057  0.067  0.094  0.114  0.117 -0.028  0.004\nE1 -0.017  0.015  0.087  0.021  0.073  0.063 -0.001 -0.077  0.047  0.055  0.013\nE2 -0.047  0.100  0.032 -0.029 -0.034 -0.024  0.024 -0.034  0.016 -0.021 -0.054\nE3 -0.103  0.105 -0.010 -0.031 -0.013  0.019  0.005 -0.004  0.045  0.005 -0.019\nF1 -0.018 -0.060  0.058 -0.035  0.097  0.021  0.070  0.118  0.111  0.107  0.130\nF2 -0.007 -0.043 -0.064 -0.122 -0.060  0.004 -0.080 -0.048  0.121 -0.015 -0.005\nF3  0.031 -0.007  0.018 -0.027  0.040  0.015 -0.006 -0.057  0.024 -0.033 -0.044\n       D3     E1     E2     E3     F1     F2     F3\nA1                                                 \nA2                                                 \nA3                                                 \nB1                                                 \nB2                                                 \nB3                                                 \nC1                                                 \nC2                                                 \nC3                                                 \nD1                                                 \nD2                                                 \nD3  0.000                                          \nE1  0.083  0.000                                   \nE2  0.074 -0.001  0.000                            \nE3  0.045 -0.016  0.010  0.000                     \nF1  0.187  0.076  0.109  0.097  0.000              \nF2  0.038 -0.056 -0.012 -0.019  0.019  0.000       \nF3  0.028 -0.017  0.001 -0.016 -0.032  0.009  0.000\n\nrange(cfares$cov); median(cfares$cov)  # 残差的全距和中位数\n\n[1] -0.1216818  0.1866437\n\n\n[1] 7.123659e-07\n\nresid(cfa_fit, type=\"standardized\") # 提取标准化残差，方便与标准正态分布比较\n\n$type\n[1] \"standardized\"\n\n$cov\n       A1     A2     A3     B1     B2     B3     C1     C2     C3     D1     D2\nA1  0.000                                                                      \nA2 -0.363  0.000                                                               \nA3  1.765 -1.670  0.000                                                        \nB1 -2.017  1.218  0.774  0.000                                                 \nB2 -0.326  2.626 -1.426  0.055  0.000                                          \nB3 -0.700  1.340 -0.288  0.081 -0.146  0.000                                   \nC1 -2.886  0.559 -1.395  0.552  0.522  0.784  0.000                            \nC2 -0.611  0.950 -2.455  0.117 -0.662 -1.305  2.421  0.000                     \nC3  1.876  3.832  1.833 -0.099  0.044  0.162 -2.676 -0.185  0.000              \nD1  1.262  3.253 -1.394 -0.301  0.901  1.417 -1.900 -0.216  0.653  0.000       \nD2 -1.688  0.684 -2.317 -0.888 -1.700 -1.090 -3.170 -0.104  0.599  3.454  0.000\nD3 -0.301  1.933 -1.283 -0.112  1.480  1.767  2.707  3.162  3.024 -4.148  0.748\nE1 -0.643  0.417  2.439  0.528  2.335  1.975 -0.045 -2.494  1.359  2.226  0.609\nE2 -2.361  3.078  0.956 -0.781 -1.407 -0.873  0.943 -1.297  0.479 -1.071 -3.481\nE3 -3.617  2.895 -0.260 -0.760 -0.408  0.568  0.172 -0.119  1.255  0.213 -0.859\nF1 -0.456 -1.244  1.205 -0.544  1.656  0.373  1.449  2.317  2.177  2.172  2.947\nF2 -0.295 -1.216 -1.743 -2.351 -1.483  0.104 -2.284 -1.330  2.968 -0.463 -0.189\nF3  1.823 -0.259  0.661 -0.666  1.394  0.490 -0.222 -2.153  0.745 -1.412 -2.229\n       D3     E1     E2     E3     F1     F2     F3\nA1                                                 \nA2                                                 \nA3                                                 \nB1                                                 \nB2                                                 \nB3                                                 \nC1                                                 \nC2                                                 \nC3                                                 \nD1                                                 \nD2                                                 \nD3  0.000                                          \nE1  2.629  0.000                                   \nE2  2.415 -0.342  0.000                            \nE3  1.378 -1.831  2.050  0.000                     \nF1  3.664  1.520  2.231  1.915  0.000              \nF2  0.933 -1.571 -0.403 -0.528  0.649  0.000       \nF3  0.835 -0.641  0.056 -0.579 -2.117  1.473  0.000\n\ncfastd &lt;- cfares$cov[lower.tri(unclass(cfares$cov),diag = F)]\n\nggplot(mapping = aes(sample=cfastd)) +\n  geom_qq() +\n  geom_qq_line() # 绘制QQ残差散点图分析是否服从正态分布\n\n\n\n\n\n\n\n\n\n模型修饰指数：残差分析除了初步观察测量变量间的问题，还可以将模型没有设定的关系参数加入假设，并与原设定模型相比，计算模型如果调整后这些残差变化对于模型改善的贡献，即为模型修饰指数（Modification Index）。\n当MI指数大于5时，表示该残差有修正的必要。EPC（expected parameter change）表示预期的参数改变量，一般综合考虑MI和EPC确定修正的方向。\n\n\nmodificationindices(cfa_fit, sort. = T, maximum.number = 6)\n\n    lhs op rhs     mi    epc sepc.lv sepc.all sepc.nox\n96   FC =~  D3 19.838  0.241   0.241    0.265    0.265\n63   FA =~  C3 14.826  0.289   0.289    0.276    0.276\n118  FE =~  A1 14.048 -0.258  -0.258   -0.262   -0.262\n265  D1 ~~  D2 13.890  0.192   0.192    1.061    1.061\n266  D1 ~~  D3 13.426 -0.132  -0.132   -0.448   -0.448\n175  A2 ~~  E1 12.247 -0.097  -0.097   -0.243   -0.243\n\n\n\n\n内在拟合度检验\n根据内部拟合检验部分提出的各项内部拟合指标，对测量模型进行检验。\n\n项目质量检验：因子载荷均大于0.55，大部分大于0.71，可以通过。\n\n\ncfa_lambda &lt;- inspect(cfa_fit,what=\"std\")$lambda\ncfa_lambda &lt;- cfa_lambda[cfa_lambda!=0]\nprint(cfa_lambda, digits = 2)  # 检验因子载荷\n\n [1] 0.83 0.69 0.63 0.68 0.83 0.79 0.72 0.71 0.66 0.87 0.89 0.72 0.83 0.91 0.81\n[16] 0.55 0.76 0.84\n\n\n\n组合信度CR均大于0.7，显示内在拟合较好；平均变异萃取量AVE除因素C、F外均大于0.5，说明拟合一般。\n\n\nlibrary(semTools)    \ncompRelSEM(cfa_fit) # 计算组合信度CR\n\n   FA    FB    FC    FD    FE    FF \n0.769 0.812 0.743 0.869 0.889 0.748 \n\nAVE(cfa_fit)        # 计算平均变异萃取量AVE\n\n   FA    FB    FC    FD    FE    FF \n0.523 0.592 0.487 0.681 0.725 0.499 \n\n\n\n因素区辩力：采取相关系数区间估计检验，潜变量的相关系数最大值为0.7，标准误约为0.05，95%置信区间为0.6-0.8，没有包含1，说明潜变量的区分度较好（参见summary报告第三部分的潜变量协方差，即相关系数）。\n\n\ncfa_res &lt;- summary(cfa_fit, standard=T)\ncfa_res$pe[43:57,] |&gt;\nselect(lhs, op, rhs, est, se, z, std.all, pvalue) |&gt;\nmutate(across(where(is.numeric), ~round(., 3)))\n\n   lhs op rhs   est    se      z std.all pvalue\n43  FA ~~  FB 0.542 0.054  9.989   0.542      0\n44  FA ~~  FC 0.494 0.061  8.102   0.494      0\n45  FA ~~  FD 0.417 0.058  7.176   0.417      0\n46  FA ~~  FE 0.526 0.052 10.080   0.526      0\n47  FA ~~  FF 0.695 0.046 15.148   0.695      0\n48  FB ~~  FC 0.697 0.047 14.906   0.697      0\n49  FB ~~  FD 0.447 0.055  8.127   0.447      0\n50  FB ~~  FE 0.575 0.048 12.091   0.575      0\n51  FB ~~  FF 0.391 0.061  6.403   0.391      0\n52  FC ~~  FD 0.522 0.055  9.493   0.522      0\n53  FC ~~  FE 0.603 0.050 12.127   0.603      0\n54  FC ~~  FF 0.600 0.054 11.032   0.600      0\n55  FD ~~  FE 0.557 0.046 12.046   0.557      0\n56  FD ~~  FF 0.316 0.062  5.137   0.316      0\n57  FE ~~  FF 0.443 0.056  7.944   0.443      0\n\n\n\n\n\n测量模型的调整\n\n测量模型调整的原则\n\n对于测量模型，可以对测量变量与潜变量之间的关系进行调整，删减测量变量或建立测量变量与其他潜变量之间的联系，也可以对测量变量残差的共变关系进行调整增减。\n对测量模型进行修正，应结合理论，避免过度拟合。\n\n\n\n\n\n\n\n\n\n\n\n\n假设模型的调整和设定\n根据前面CFA分析检验的结果，MI指数显示测量变量C3与潜变量A之间的关系如果纳入模型能够提升拟合度14.82，增加参数量因子载荷0.28，综合考量是贡献最大的参数改善。因此需要调整假设和设定，测量变量C3同时受到潜变量C和A的影响，其他参数设定不变。\n\n\n模型调整后的结果报告\n\n估计参数数量增加1个到52个，C3在A上的因子载荷为0.268，并且结果是显著的，并且其他变量显著性未受影响。\n\n\nnew_model &lt;-' \n#define the measurement model  \n  FA =~ L11*A1 + L21*A2 + L31*A3 + L37 * C3\n  FB =~ L12*B1 + L22*B2 + L32*B3\n  FC =~ L13*C1 + L23*C2 + L33*C3\n  FD =~ L14*D1 + L24*D2 + L34*D3\n  FE =~ L15*E1 + L25*E2 + L35*E3\n  FF =~ L16*F1 + L26*F2 + L36*F3'\n\nnew_fit &lt;- cfa(new_model,\n               data = dat,\n               std.lv = TRUE)\nsummary(new_fit)\n\nlavaan 0.6-19 ended normally after 30 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        52\n\n  Number of observations                           313\n\nModel Test User Model:\n                                                      \n  Test statistic                               226.794\n  Degrees of freedom                               119\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  FA =~                                               \n    A1       (L11)    0.805    0.051   15.803    0.000\n    A2       (L21)    0.711    0.055   12.859    0.000\n    A3       (L31)    0.616    0.054   11.510    0.000\n    C3       (L37)    0.268    0.065    4.128    0.000\n  FB =~                                               \n    B1       (L12)    0.789    0.062   12.766    0.000\n    B2       (L22)    0.961    0.058   16.591    0.000\n    B3       (L32)    0.855    0.056   15.407    0.000\n  FC =~                                               \n    C1       (L13)    0.729    0.054   13.605    0.000\n    C2       (L23)    0.755    0.057   13.284    0.000\n    C3       (L33)    0.533    0.066    8.080    0.000\n  FD =~                                               \n    D1       (L14)    0.811    0.045   18.185    0.000\n    D2       (L24)    0.741    0.040   18.743    0.000\n    D3       (L34)    0.655    0.047   14.078    0.000\n  FE =~                                               \n    E1       (L15)    0.806    0.046   17.389    0.000\n    E2       (L25)    0.912    0.046   19.857    0.000\n    E3       (L35)    0.791    0.047   16.809    0.000\n  FF =~                                               \n    F1       (L16)    0.641    0.066    9.656    0.000\n    F2       (L26)    0.828    0.058   14.305    0.000\n    F3       (L36)    0.786    0.049   16.137    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  FA ~~                                               \n    FB                0.545    0.054   10.046    0.000\n    FC                0.409    0.067    6.078    0.000\n    FD                0.424    0.058    7.319    0.000\n    FE                0.533    0.052   10.283    0.000\n    FF                0.700    0.046   15.368    0.000\n  FB ~~                                               \n    FC                0.668    0.049   13.556    0.000\n    FD                0.447    0.055    8.128    0.000\n    FE                0.574    0.048   12.089    0.000\n    FF                0.391    0.061    6.401    0.000\n  FC ~~                                               \n    FD                0.491    0.057    8.614    0.000\n    FE                0.569    0.052   10.857    0.000\n    FF                0.545    0.059    9.309    0.000\n  FD ~~                                               \n    FE                0.557    0.046   12.045    0.000\n    FF                0.316    0.062    5.137    0.000\n  FE ~~                                               \n    FF                0.443    0.056    7.943    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .A1                0.317    0.045    6.981    0.000\n   .A2                0.538    0.054    9.962    0.000\n   .A3                0.557    0.052   10.694    0.000\n   .C3                0.629    0.057   10.951    0.000\n   .B1                0.715    0.068   10.524    0.000\n   .B2                0.408    0.057    7.221    0.000\n   .B3                0.447    0.052    8.608    0.000\n   .C1                0.418    0.051    8.222    0.000\n   .C2                0.491    0.057    8.612    0.000\n   .D1                0.216    0.031    7.036    0.000\n   .D2                0.151    0.024    6.209    0.000\n   .D3                0.400    0.037   10.802    0.000\n   .E1                0.293    0.032    9.249    0.000\n   .E2                0.181    0.030    6.108    0.000\n   .E3                0.325    0.033    9.736    0.000\n   .F1                0.949    0.083   11.402    0.000\n   .F2                0.506    0.058    8.689    0.000\n   .F3                0.265    0.042    6.283    0.000\n    FA                1.000                           \n    FB                1.000                           \n    FC                1.000                           \n    FD                1.000                           \n    FE                1.000                           \n    FF                1.000                           \n\n\n\n模型拟合度的变化：卡方值依然显著未改善；卡方均值改善小于2；NFI、NNFI、CFI均有所增加；RMSEA、SRMR均有所减小，说明拟合度有改善。\n\n\nfitmeasures(cfa_fit, c(\"chisq\", \"df\", \"pvalue\", \"nfi\",\"nnfi\",\"cfi\", \"rmsea\", \"srmr\")) # 原设定模型\n\n  chisq      df  pvalue     nfi    nnfi     cfi   rmsea    srmr \n241.755 120.000   0.000   0.915   0.942   0.955   0.057   0.052 \n\nfitmeasures(new_fit, c(\"chisq\", \"df\", \"pvalue\", \"nfi\",\"nnfi\",\"cfi\", \"rmsea\", \"srmr\")) # 调整后模型\n\n  chisq      df  pvalue     nfi    nnfi     cfi   rmsea    srmr \n226.794 119.000   0.000   0.920   0.948   0.960   0.054   0.048 \n\n\n\n参数估计的比较：调整的测量变量C3与因素A的因子载荷为0.27，与MI指数增加因子载荷0.28非常接近，模型调整后，因子C的载荷有所变化，在C1、C2、C3上的载荷由0.70、0.74、0.70变化为0.73、0.76、0.53；因子A上的载荷变化不大。模型调整后，潜变量的相关系数有所变化，变化最大的是因子A和因子C之间的相关系数。\n\n\n\n\n\n\n\n\n\n\n\n\n\n模型的路径图\n\nsemPaths(cfa_fit, what = \"std\") \n\n\n\n\n\n\n\nsemPaths(new_fit, what = \"std\")"
  },
  {
    "objectID": "analyr/lecture7/sem.html#路径分析",
    "href": "analyr/lecture7/sem.html#路径分析",
    "title": "调节效应、中介效应和结构方程",
    "section": "路径分析",
    "text": "路径分析\n路径分析所包括的变量都是外显观察变量，不涉及潜变量，所用的操作可采用结构方程分析技术，相当于没有测量模型的结构方程。当模型结构过于复杂时，会出现自由度\\(df\\le0\\)的情况，无法计算拟合指数。\n\n模型设定与识别\n研究假设组织气氛知觉包括6个变量：组织价值（VALUE）、工作方式（JOBSTYLE）、团队合作（TEAMWORK）、领导风格（LEADERSH）、学习成长（LEARNING）、环境气氛（ENVIRON）。这6个变量均会影响员工的组织承诺感（commit），进而影响员工的工作绩效（out），此时，组织承诺作为中介变量，同时年资（TENURE）是控制变量，既影响组织承诺，也影响工作绩效。 整个模型有9个观察变量，因此测量数据\\(DP=(9\\times10)/2=45\\)。模型设定6个组织气氛知觉变量之间都相关，因此有15个相关系数参数，7个外生变量有7个方差参数，2个内生变量有2个解释残差（即回归方程的误差项），9个路径回归系数，所以待估参数\\(t=15+7+2+9=33\\)个。\\(t&lt;DP\\)因此模型时可识别的。\n\n\n\n\n\n\n\n\n\n\n\n参数估计\n\n变量描述\n\ndat &lt;- read_sav(\"ch07.sav\")\ndatasummary_skim(dat, fun_numeric = list('取值'=NUnique, '缺失值'=PercentMissing, '均值'=Mean, '中位数'=Median,'标准差'=SD, '最小值'=Min, '最大值'=Max), output = \"data.frame\", type = \"numeric\", fmt_sprintf(\"%.2f\")) |&gt; knitr::kable()\n\n\n\n\n\n取值\n缺失值\n均值\n中位数\n标准差\n最小值\n最大值\n\n\n\n\nout\n9\n0\n4.22\n4.33\n0.63\n2.00\n5.00\n\n\ncommit\n22\n0\n9.49\n9.67\n1.59\n4.00\n12.00\n\n\nVALUE\n15\n0\n4.27\n4.33\n0.81\n1.00\n6.00\n\n\nJOBSTYLE\n15\n0\n4.18\n4.33\n0.95\n1.00\n6.00\n\n\nTEAMWORK\n14\n0\n4.33\n4.33\n0.83\n1.33\n6.00\n\n\nLEADERSH\n12\n0\n4.87\n5.00\n0.77\n2.00\n6.00\n\n\nLEARNING\n14\n0\n4.71\n5.00\n0.88\n1.67\n6.00\n\n\nENVIRONM\n14\n0\n4.43\n4.33\n0.88\n1.00\n6.00\n\n\nTENURE\n106\n0\n9.12\n4.00\n9.23\n0.08\n32.00\n\n\n\n\n\n\n\n参数估计设定\n\n设定结构模型部分（代码part1），组织承诺commit被7个变量解释；员工绩效被2个变量解释。代码形式与回归方程类似。\n设定相关系数部分（代码part2），lavaan包会自动外生变量添加两两相关的相关系数参数，路径图里显示6个组织气氛变量与年资不相关，因此要手动设定。\n还可以定义感兴趣的间接效应和总效应（代码part3和part4）。\n\n\npath1 &lt;-'\n#part1：设定结构模型\n  commit~ a1*VALUE+a2*JOBSTYLE+a3*TEAMWORK+a4*LEADERSH\n          +a5*LEARNING+a6*ENVIRONM+a7*TENURE\n  out~c*TENURE+b*commit\n#part2：设定外生变量的相关系数\n  VALUE    ~~ JOBSTYLE \n  VALUE    ~~ TEAMWORK \n  VALUE    ~~ LEADERSH \n  VALUE    ~~ LEARNING\n  VALUE    ~~ ENVIRONM \n  JOBSTYLE ~~ TEAMWORK \n  JOBSTYLE ~~ LEADERSH \n  JOBSTYLE ~~ LEARNING \n  JOBSTYLE ~~ ENVIRONM \n  TEAMWORK ~~ LEADERSH \n  TEAMWORK ~~ LEARNING \n  TEAMWORK ~~ ENVIRONM\n  LEADERSH ~~ LEARNING \n  LEADERSH ~~ ENVIRONM \n  LEARNING ~~ ENVIRONM \n  # 设定相关系数为0\n  TENURE   ~~ 0*VALUE \n  TENURE   ~~ 0*JOBSTYLE \n  TENURE   ~~ 0*TEAMWORK \n  TENURE   ~~ 0*LEADERSH \n  TENURE   ~~ 0*LEARNING\n  TENURE   ~~ 0*ENVIRONM\n#part3：定义间接效应 (a*b)\n  a1b := a1*b\n  a2b := a2*b\n  a3b := a3*b\n  a4b := a4*b\n  a5b := a5*b\n  a6b := a6*b\n  a7b := a7*b\n#part4：定义年资TENURE的总效应\n  total := c + a7b'\n\n\n\n结果报告\nsummary报告第一部分为模型基本信息，估计方法为最大似然估计，优化方法为非线性最小优化，待估参数33个，样本量281.\nsummary报告第二部分为拟合度检验，包括卡方值和检验结果、CFI、RMSEA、SRMR等。\n\n模型拟合度分析：对照前面的标准，CFI值0.98大于标准0.9，SRMR值0.06小于标准0.08，表明拟合比较理想；RMSEA为0.067略微大于标准0.05，表明模型拟合不理想；综合来看，理论模型拟合度较好。\n\nsummary报告第三部分为参数估计结果，包括路径回归系数、外生变量协方差、外生变量方差和内生变量的解释残差、直接效应和间接效应参数估计。\n\n回归模型（Regressions）部分为路径回归系数估计值相关结果，std.all对应的是标准化的回归系数。团队合作（TEAMWORK）和领导风格（LEADERSH）对组织承诺（commit）的回归系数不显著。其余回归系数均显著。\n\n协方差（Covariances）部分为外生变量协方差，由于模型设定年资（TENURE）与其他外生变量不相关，因此固定为0，std.all是标准化后的协方差，即外生变量的相关系数。协方差估计结果均显著，说明外生变量间相关性存在。\n\n方差（Variances）部分为内生变量的解释残差（.commit和.out）和外生变量的方差。\n\n定义参数估计（Defined Parameters）部分为总效应和间接效应系数相关信息。同样，团队合作（TEAMWORK）和领导风格（LEADERSH）的间接效应系数a3b和a4b不显著，显然这与回归模型的结论一致。\n\n\npath1_fit &lt;- sem(path1, data = dat)\npath1_res &lt;- summary(path1_fit, fit.measures=T,standard = T)\npath1_res\n\nlavaan 0.6-19 ended normally after 43 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        33\n\n  Number of observations                           281\n\nModel Test User Model:\n                                                      \n  Test statistic                                27.106\n  Degrees of freedom                                12\n  P-value (Chi-square)                           0.007\n\nModel Test Baseline Model:\n\n  Test statistic                               866.406\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.982\n  Tucker-Lewis Index (TLI)                       0.945\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3522.025\n  Loglikelihood unrestricted model (H1)      -3508.473\n                                                      \n  Akaike (AIC)                                7110.051\n  Bayesian (BIC)                              7230.116\n  Sample-size adjusted Bayesian (SABIC)       7125.475\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.067\n  90 Percent confidence interval - lower         0.033\n  90 Percent confidence interval - upper         0.101\n  P-value H_0: RMSEA &lt;= 0.050                    0.181\n  P-value H_0: RMSEA &gt;= 0.080                    0.287\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.060\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  commit ~                                                              \n    VALUE     (a1)    0.428    0.116    3.687    0.000    0.428    0.218\n    JOBSTYLE  (a2)    0.242    0.100    2.409    0.016    0.242    0.146\n    TEAMWORK  (a3)    0.106    0.118    0.895    0.371    0.106    0.056\n    LEADERSH  (a4)    0.108    0.119    0.912    0.362    0.108    0.053\n    LEARNING  (a5)    0.363    0.115    3.159    0.002    0.363    0.203\n    ENVIRONM  (a6)    0.276    0.106    2.611    0.009    0.276    0.153\n    TENURE    (a7)    0.022    0.008    2.816    0.005    0.022    0.129\n  out ~                                                                 \n    TENURE     (c)    0.011    0.004    2.964    0.003    0.011    0.157\n    commit     (b)    0.172    0.021    8.135    0.000    0.172    0.430\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  VALUE ~~                                                              \n    JOBSTYLE          0.367    0.051    7.244    0.000    0.367    0.479\n    TEAMWORK          0.271    0.043    6.288    0.000    0.271    0.405\n    LEADERSH          0.206    0.039    5.274    0.000    0.206    0.331\n    LEARNING          0.344    0.047    7.328    0.000    0.344    0.486\n    ENVIRONM          0.369    0.047    7.782    0.000    0.369    0.524\n  JOBSTYLE ~~                                                           \n    TEAMWORK          0.420    0.053    7.853    0.000    0.420    0.530\n    LEADERSH          0.328    0.048    6.829    0.000    0.328    0.446\n    LEARNING          0.461    0.057    8.078    0.000    0.461    0.550\n    ENVIRONM          0.269    0.052    5.149    0.000    0.269    0.323\n  TEAMWORK ~~                                                           \n    LEADERSH          0.318    0.043    7.443    0.000    0.318    0.496\n    LEARNING          0.390    0.049    7.884    0.000    0.390    0.533\n    ENVIRONM          0.368    0.049    7.571    0.000    0.368    0.506\n  LEADERSH ~~                                                           \n    LEARNING          0.386    0.047    8.259    0.000    0.386    0.566\n    ENVIRONM          0.249    0.043    5.800    0.000    0.249    0.369\n  LEARNING ~~                                                           \n    ENVIRONM          0.332    0.050    6.626    0.000    0.332    0.430\n  VALUE ~~                                                              \n    TENURE            0.000                               0.000    0.000\n  JOBSTYLE ~~                                                           \n    TENURE            0.000                               0.000    0.000\n  TEAMWORK ~~                                                           \n    TENURE            0.000                               0.000    0.000\n  LEADERSH ~~                                                           \n    TENURE            0.000                               0.000    0.000\n  LEARNING ~~                                                           \n    TENURE            0.000                               0.000    0.000\n  ENVIRONM ~~                                                           \n    TENURE            0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .commit            1.456    0.123   11.853    0.000    1.456    0.586\n   .out               0.305    0.026   11.853    0.000    0.305    0.773\n    VALUE             0.647    0.055   11.853    0.000    0.647    1.000\n    JOBSTYLE          0.907    0.077   11.853    0.000    0.907    1.000\n    TEAMWORK          0.691    0.058   11.853    0.000    0.691    1.000\n    LEADERSH          0.598    0.050   11.853    0.000    0.598    1.000\n    LEARNING          0.776    0.065   11.853    0.000    0.776    1.000\n    ENVIRONM          0.765    0.065   11.853    0.000    0.765    1.000\n    TENURE           84.834    7.157   11.853    0.000   84.834    1.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    a1b               0.073    0.022    3.358    0.001    0.073    0.094\n    a2b               0.041    0.018    2.310    0.021    0.041    0.063\n    a3b               0.018    0.020    0.889    0.374    0.018    0.024\n    a4b               0.019    0.021    0.906    0.365    0.019    0.023\n    a5b               0.062    0.021    2.945    0.003    0.062    0.087\n    a6b               0.047    0.019    2.486    0.013    0.047    0.066\n    a7b               0.004    0.001    2.661    0.008    0.004    0.055\n    total             0.014    0.004    3.787    0.000    0.014    0.212\n\n\n\n\n\n直接与间接效应\n路径分析的目的之一是探讨内生变量被外生变量解释的总体效应、直接效应和间接效应。尽管采用分阶段回归的方式也能够估计出各个效应值，但是结构方程技术更方便对这些效应进行统计检验。\n\n直接效应：summary报告参数估计的回归模型部分的路径系数便是直接效应。例如组织价值（VALUE）对组织承诺（commit）的直接效应为a1，即0.428，而年资（TENURE）对工作绩效（out）的直接效应为c，即0.011.\n间接效应：外生变量通过中介变量对另一个内生变量产生的影响，便是间接效应。例如间接效应系数a1b代表组织价值对组织承诺的效应（a1=0.428）通过组织承诺对工作绩效的直接效应（b=0.172）间接影响到工作绩效，形成间接效应（a1b），并且数值上间接效应等于两段直接效应的乘积，即\\(a1b=a1\\times b=0.428\\times 0.172=0.073\\)，统计检验显示其p值为0.001，该间接效应具有统计上的意义。\n总效应：一个变量对结果变量直接效应和间接效应的总和便是总效应。例如，年资（TENURE）对工作绩效的直接效应（c=0.011），年资通过组织承诺对工作绩效的间接效应（a7b=0.004），那么年资对工作绩效的总效应\\(total=c+a7b=0.011+0.004\\simeq 0.015\\)，统计检验其p值为0，该总效应具有统计上的意义。\n\n\npath1_res$pe[c(1:9,40:47),] |&gt;\n  select(lhs, op, rhs, est, se, z, std.all, pvalue) |&gt;\n  mutate(across(where(is.numeric), ~round(., 3)))\n\n      lhs op      rhs   est    se     z std.all pvalue\n1  commit  ~    VALUE 0.428 0.116 3.687   0.218  0.000\n2  commit  ~ JOBSTYLE 0.242 0.100 2.409   0.146  0.016\n3  commit  ~ TEAMWORK 0.106 0.118 0.895   0.056  0.371\n4  commit  ~ LEADERSH 0.108 0.119 0.912   0.053  0.362\n5  commit  ~ LEARNING 0.363 0.115 3.159   0.203  0.002\n6  commit  ~ ENVIRONM 0.276 0.106 2.611   0.153  0.009\n7  commit  ~   TENURE 0.022 0.008 2.816   0.129  0.005\n8     out  ~   TENURE 0.011 0.004 2.964   0.157  0.003\n9     out  ~   commit 0.172 0.021 8.135   0.430  0.000\n40    a1b :=     a1*b 0.073 0.022 3.358   0.094  0.001\n41    a2b :=     a2*b 0.041 0.018 2.310   0.063  0.021\n42    a3b :=     a3*b 0.018 0.020 0.889   0.024  0.374\n43    a4b :=     a4*b 0.019 0.021 0.906   0.023  0.365\n44    a5b :=     a5*b 0.062 0.021 2.945   0.087  0.003\n45    a6b :=     a6*b 0.047 0.019 2.486   0.066  0.013\n46    a7b :=     a7*b 0.004 0.001 2.661   0.055  0.008\n47  total :=    c+a7b 0.014 0.004 3.787   0.212  0.000\n\n\n\n\n模型调整\n模型调整主要依据MI指数，考察大于5的取值，结合理论进行判断。\n\n工作绩效与组织承诺的残差间有显著的修正空间，MI值为15.2，但是预期变化量EPC为-0.24（原设定为0），表明有某种未观察到的变量，同时影响工作绩效和组织承诺，但是对它们的影响效应是相反的，尚缺乏有效的理论支撑。\n\n工作绩效对组织承诺的结构参数对应的MI值为15.2，表示该参数要纳入模型，但是如果纳入会导致回馈循环效应，并且预期改变量为-0.8，表示员工工作绩效越高，组织承诺越低，难以有理论或逻辑上的解释，缺乏合理性。\n\n学习成长、领导风格、组织价值、团队合作对工作绩效的结构参数对应的MI值均大于5，预测值也为正，表明这些变量得分越高，工作绩效越好，理论逻辑上是合理的。可以考虑增加这些变量对结果变量的直接效应。\n\n\nmodificationindices(path1_fit, sort. = T, maximum.number = 8)\n\n       lhs op      rhs     mi    epc sepc.lv sepc.all sepc.nox\n48  commit ~~      out 15.228 -0.244  -0.244   -0.366   -0.366\n63  commit  ~      out 15.228 -0.800  -0.800   -0.319   -0.319\n68     out  ~ LEARNING 13.063  0.158   0.158    0.222    0.222\n67     out  ~ LEADERSH  9.170  0.140   0.140    0.173    0.173\n64     out  ~    VALUE  7.031  0.127   0.127    0.162    0.162\n66     out  ~ TEAMWORK  6.771  0.115   0.115    0.152    0.152\n125 TENURE  ~ ENVIRONM  5.352  1.453   1.453    0.138    0.138\n120 TENURE  ~    VALUE  4.581  1.462   1.462    0.128    0.128\n\n\n先增加一条学习成长到工作绩效的直接效应，设定新的模型，试试看。\n\npath2 &lt;- '#part1：set the structure model\n  commit~ a1*VALUE+a2*JOBSTYLE+a3*TEAMWORK+a4*LEADERSH\n          +a5*LEARNING+a6*ENVIRONM+a7*TENURE\n  out~c*TENURE+b*commit + d*LEARNING # 添加直接效应d\n#part2：set the correlation\n  VALUE    ~~ JOBSTYLE \n  VALUE    ~~ TEAMWORK \n  VALUE    ~~ LEADERSH \n  VALUE    ~~ LEARNING\n  VALUE    ~~ ENVIRONM \n  JOBSTYLE ~~ TEAMWORK \n  JOBSTYLE ~~ LEADERSH \n  JOBSTYLE ~~ LEARNING \n  JOBSTYLE ~~ ENVIRONM \n  TEAMWORK ~~ LEADERSH \n  TEAMWORK ~~ LEARNING \n  TEAMWORK ~~ ENVIRONM\n  LEADERSH ~~ LEARNING \n  LEADERSH ~~ ENVIRONM \n  LEARNING ~~ ENVIRONM \n  # fix\n  TENURE   ~~ 0*VALUE \n  TENURE   ~~ 0*JOBSTYLE \n  TENURE   ~~ 0*TEAMWORK \n  TENURE   ~~ 0*LEADERSH \n  TENURE   ~~ 0*LEARNING\n  TENURE   ~~ 0*ENVIRONM\n#part3：define indirect effect (a*b)\n  a1b := a1*b\n  a2b := a2*b\n  a3b := a3*b\n  a4b := a4*b\n  a5b := a5*b\n  a6b := a6*b\n  a7b := a7*b\n#part4：define total effect of COMMIT\n  total := c + a7b'\n\npath2_fit &lt;- sem(path2, data = dat)\n\nsummary(path2_fit, fit.measures=T, standard=T)\n\nlavaan 0.6-19 ended normally after 44 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        34\n\n  Number of observations                           281\n\nModel Test User Model:\n                                                      \n  Test statistic                                13.714\n  Degrees of freedom                                11\n  P-value (Chi-square)                           0.249\n\nModel Test Baseline Model:\n\n  Test statistic                               866.406\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.997\n  Tucker-Lewis Index (TLI)                       0.989\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3515.330\n  Loglikelihood unrestricted model (H1)      -3508.473\n                                                      \n  Akaike (AIC)                                7098.659\n  Bayesian (BIC)                              7222.363\n  Sample-size adjusted Bayesian (SABIC)       7114.551\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.030\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.073\n  P-value H_0: RMSEA &lt;= 0.050                    0.736\n  P-value H_0: RMSEA &gt;= 0.080                    0.024\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.041\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  commit ~                                                              \n    VALUE     (a1)    0.428    0.116    3.687    0.000    0.428    0.218\n    JOBSTYLE  (a2)    0.242    0.100    2.409    0.016    0.242    0.146\n    TEAMWORK  (a3)    0.106    0.118    0.895    0.371    0.106    0.056\n    LEADERSH  (a4)    0.108    0.119    0.912    0.362    0.108    0.053\n    LEARNING  (a5)    0.363    0.115    3.159    0.002    0.363    0.203\n    ENVIRONM  (a6)    0.276    0.106    2.611    0.009    0.276    0.153\n    TENURE    (a7)    0.022    0.008    2.816    0.005    0.022    0.129\n  out ~                                                                 \n    TENURE     (c)    0.011    0.004    3.192    0.001    0.011    0.166\n    commit     (b)    0.125    0.024    5.201    0.000    0.125    0.315\n    LEARNING   (d)    0.158    0.043    3.706    0.000    0.158    0.222\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  VALUE ~~                                                              \n    JOBSTYLE          0.367    0.051    7.244    0.000    0.367    0.479\n    TEAMWORK          0.271    0.043    6.288    0.000    0.271    0.405\n    LEADERSH          0.206    0.039    5.274    0.000    0.206    0.331\n    LEARNING          0.344    0.047    7.328    0.000    0.344    0.486\n    ENVIRONM          0.369    0.047    7.782    0.000    0.369    0.524\n  JOBSTYLE ~~                                                           \n    TEAMWORK          0.420    0.053    7.853    0.000    0.420    0.530\n    LEADERSH          0.328    0.048    6.829    0.000    0.328    0.446\n    LEARNING          0.461    0.057    8.078    0.000    0.461    0.550\n    ENVIRONM          0.269    0.052    5.149    0.000    0.269    0.323\n  TEAMWORK ~~                                                           \n    LEADERSH          0.318    0.043    7.443    0.000    0.318    0.496\n    LEARNING          0.390    0.049    7.884    0.000    0.390    0.533\n    ENVIRONM          0.368    0.049    7.571    0.000    0.368    0.506\n  LEADERSH ~~                                                           \n    LEARNING          0.386    0.047    8.259    0.000    0.386    0.566\n    ENVIRONM          0.249    0.043    5.800    0.000    0.249    0.369\n  LEARNING ~~                                                           \n    ENVIRONM          0.332    0.050    6.626    0.000    0.332    0.430\n  VALUE ~~                                                              \n    TENURE            0.000                               0.000    0.000\n  JOBSTYLE ~~                                                           \n    TENURE            0.000                               0.000    0.000\n  TEAMWORK ~~                                                           \n    TENURE            0.000                               0.000    0.000\n  LEADERSH ~~                                                           \n    TENURE            0.000                               0.000    0.000\n  LEARNING ~~                                                           \n    TENURE            0.000                               0.000    0.000\n  ENVIRONM ~~                                                           \n    TENURE            0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .commit            1.456    0.123   11.853    0.000    1.456    0.586\n   .out               0.291    0.025   11.853    0.000    0.291    0.739\n    VALUE             0.647    0.055   11.853    0.000    0.647    1.000\n    JOBSTYLE          0.907    0.077   11.853    0.000    0.907    1.000\n    TEAMWORK          0.691    0.058   11.853    0.000    0.691    1.000\n    LEADERSH          0.598    0.050   11.853    0.000    0.598    1.000\n    LEARNING          0.776    0.065   11.853    0.000    0.776    1.000\n    ENVIRONM          0.765    0.065   11.853    0.000    0.765    1.000\n    TENURE           84.834    7.157   11.853    0.000   84.834    1.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    a1b               0.054    0.018    3.008    0.003    0.054    0.069\n    a2b               0.030    0.014    2.186    0.029    0.030    0.046\n    a3b               0.013    0.015    0.882    0.378    0.013    0.018\n    a4b               0.014    0.015    0.898    0.369    0.014    0.017\n    a5b               0.046    0.017    2.700    0.007    0.046    0.064\n    a6b               0.035    0.015    2.333    0.020    0.035    0.048\n    a7b               0.003    0.001    2.477    0.013    0.003    0.040\n    total             0.014    0.004    3.869    0.000    0.014    0.206\n\n\n学习成长到工作绩效的直接效应（d）为0.158，p值为0，说明该效应存在。比较前后模型的拟合度指标发现，卡方值明显下降，p值已大于0.05，RMSEA和SRMR都小于目标值。新模型完美拟合，无需再进行调整。\n\nfitmeasures(path1_fit, c(\"chisq\", \"df\", \"pvalue\", \"nfi\",\"nnfi\",\"cfi\", \"rmsea\", \"srmr\")) # 原设定模型\n\n chisq     df pvalue    nfi   nnfi    cfi  rmsea   srmr \n27.106 12.000  0.007  0.969  0.945  0.982  0.067  0.060 \n\nfitmeasures(path2_fit, c(\"chisq\", \"df\", \"pvalue\", \"nfi\",\"nnfi\",\"cfi\", \"rmsea\", \"srmr\")) # 调整后模型\n\n chisq     df pvalue    nfi   nnfi    cfi  rmsea   srmr \n13.714 11.000  0.249  0.984  0.989  0.997  0.030  0.041 \n\n\n前后两个模型是嵌套关系，可以进行嵌套模型的卡方差异检验，检验模型修正带来的拟合优度增幅是否具有显著意义。\n\nanova(path1_fit, path2_fit)\n\n\nChi-Squared Difference Test\n\n          Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \npath2_fit 11 7098.7 7222.4 13.714                                          \npath1_fit 12 7110.1 7230.1 27.106     13.391 0.20999       1  0.0002528 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n可以绘制两个模型的路径图进行比较。"
  },
  {
    "objectID": "analyr/lecture7/sem.html#统合模型分析",
    "href": "analyr/lecture7/sem.html#统合模型分析",
    "title": "调节效应、中介效应和结构方程",
    "section": "统合模型分析",
    "text": "统合模型分析\n\n统合模型的基础\n\n统合模型的构成：统合模型包含测量模型和结构模型。如果结构模型中的变量都是潜变量，则称为完全统合模型或完全潜在模型，如果结构模型中的某一个或某几个变量是单一指标的测量变量，则称为部分潜在模型。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n统合模型分析步骤：通常采用两阶段策略，第一个阶段是针对测量模型确定因素结构的拟合性，第二阶段则是在不改变测量模型的前提下，增加结构模型的设定，并评估结构模型的拟合性。该方法的优点在于测量模型的检测可以提供潜变量的聚敛与区分效度的信息，结构模型则可以提供预测效度的证据。\n变量组合的策略：结构方程模型分析经常会以变量组合策略简化测量模型，使结构模型得以在比较简化的情形下进行估计。\n\n例如某个潜变量有6个测量变量，可以每两题加起来求平均值，将6个变量聚合成3个变量以降低模型的复杂程度。聚合层次的变量分数称为组合分数。特别是当样本量太少，而模型估计参数很多，变通做法是通过将测量变量组合成单一变量，将潜变量转换为观察变量后进行路径分析。\n\n变量组合的优点：简化模型、提高模型拟合度、获得较理想的估计解、得到较好的测量信度、较高的变量解释力、观察变量的尺度更具有等距性与正态性、更理想的检验效力（每题的样本量与题数的比值更高）、避免特殊题项的干扰。\n\n变量组合不应为简化而简化，需要提出简化的理由与统计量上支持，必须经过验证性因子分析的检验，保证构念的单一维度，并且组合的题项要具有相同的尺度（例如都是二分变量或者都是相同点数的likert量表）。\n\n\n\n\n统合模型分析的操作\n250位教师的创意教学行为调查数据，包括教师的创造人格特质（Person）、创意教学自我效能感（Efficacy）、组织社会化（Socialized）、教学创新行为（Crea）等4个潜变量的10个测量变量。\n\n创造人格特质包括多角推理、兴趣广泛、乐在工作3个测量变量\n\n自我效能感包括自我肯定、自我防卫、社会支持寻求、外在压力抗衡4个测量变量\n\n组织社会化包括组织融入和工作熟练度2个测量变量\n\n教学创新行为已将9个量表题项加总成组合分数\n\n\n假设模型\n研究假设：\n\n教师创意教学⾃我效能越⾼，越能够表现出创意教学⾏为。\n\n教师个⼈创造性格越强、组织社会化程度越⾼，则创意教学⾃我效能越⾼。\n\n教师个⼈创造性格与组织社会化程度两项特质，会透过⾃我效能的中介作⽤，间接影响教师的创意教学⾏为。\n\n数据是以协方差矩阵的形式给出的。可以通过热力图展示测量变量间的相关系数。\n\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(modelsummary)\nlibrary(semPlot)\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\nlibrary(semptools) # 给路径图标记显著性\n\nN &lt;- 250 # 样本量\n# 协方差矩阵\nCOV &lt;- structure(c(17.711, 1.843, 0.801, 1.243, 1.692, 7.518, 7.585, \n                   6.499, 3.02, 2.663, 1.843, 0.404, 0.146, 0.227, 0.226, 1.074, \n                   1.062, 0.906, 0.369, 0.318, 0.801, 0.146, 0.374, 0.11, 0.115, \n                   0.301, 0.538, 0.388, 0.237, 0.307, 1.243, 0.227, 0.11, 0.589, \n                   0.208, 0.531, 0.609, 0.434, 0.322, 0.335, 1.692, 0.226, 0.115, \n                   0.208, 0.393, 0.741, 0.806, 0.664, 0.196, 0.326, 7.518, 1.074, \n                   0.301, 0.531, 0.741, 7.565, 5.202, 4.53, 1.947, 2.092, 7.585, \n                   1.062, 0.538, 0.609, 0.806, 5.202, 7.046, 4.626, 1.524, 1.824, \n                   6.499, 0.906, 0.388, 0.434, 0.664, 4.53, 4.626, 6.335, 1.649, \n                   1.662, 3.02, 0.369, 0.237, 0.322, 0.196, 1.947, 1.524, 1.649, \n                   4.482, 2.335, 2.663, 0.318, 0.307, 0.335, 0.326, 2.092, 1.824, \n                   1.662, 2.335, 2.982), dim = c(10L, 10L), dimnames = list(c(\"CREAT\", \n                                                                              \"SEFF1\", \"SEFF2\", \"SEFF3\", \"SEFF4\", \"PER1\", \"PER2\", \"PER3\", \"SOC1\", \n                                                                              \"SOC2\"), c(\"CREAT\", \"SEFF1\", \"SEFF2\", \"SEFF3\", \"SEFF4\", \"PER1\", \n                                                                                         \"PER2\", \"PER3\", \"SOC1\", \"SOC2\")))\n\n# X和Y相关系数，等于二者的协方差除以二者标准差的积：\nCOR &lt;- COV / (sqrt(diag(COV) %*% t(diag(COV))))\n\n# 调用corrplot命令，绘制热力图\n# Generate a lighter palette\ncol &lt;- colorRampPalette(c(\"#BB4444\", \"#EE9988\", \"#FFFFFF\", \"#77AADD\", \"#4477AA\"))\n\ncorrplot(COR, method = \"shade\", shade.col = NA, tl.col = \"black\", tl.srt = 45,\n         col = col(200), addCoef.col = \"black\", cl.pos = \"n\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n模型设定与识别\n\n由于该模型有10个测量变量，测量数据的数目为\\(DP=10\\times11/2=55\\)个。\n\n模型有5个外生测量变量，5个内生测量变量，所以有5个外生测量残差，4个内生测量残差（潜变量CREA只有1个指标，完美预测，该测量变量残差固定为0，因后面要调整测量模型，概念模型中依然采取测量变量和潜变量分开的方式，本质是1个变量），待估参数9个。\n\n模型有2个外生潜变量，2个内生潜变量，所以外生潜变量的协方差1个（概念图未标出）和方差2个（与之前的CFA部分处理不同，此处不通过限定外生变量方差为1来标准化，改为限定第一个因子载荷为1，如下），内生潜变量2个解释残差，待估参数5个。\n\n每个测量变量仅受一个潜变量影响，故产生10个因子载荷，为确立两个潜变量的尺度，潜变量的第一因子载荷固定为1，所以要扣除4个，待估参数6个。\n\n内生潜变量被外生潜变量解释，有5个待估结构参数（路径系数）。\n\n因此，待估参数\\(t=9+5+6+5=25\\)个。\n\n\n\n测量模型分析阶段\n模型拟合情况大致良好，卡方p值为0，卡方自由度比为2.6，接近2，NFI为0.94，CFI为0.96，RMSEA为0.08，SRMR为0.04，可以继续检验MI指数按照验证性因子分析介绍的方式调整测量模型，这里直接进行第二阶段的结构模型估计。\n\ncfa_mod &lt;- '\n# measurement model\n    CREA =~ 1*CREAT # 由于CREA只有一个测量变量，因此设定因子载荷为1，默认测量变量残差方差为0\n    SEFF =~ SEFF1 + SEFF2 + SEFF3 + SEFF4\n    PER  =~ PER1  + PER2  + PER3\n    SOC  =~ SOC1  + SOC2'\n\ncfa_fit &lt;- cfa(model = cfa_mod, sample.cov = COV, sample.nobs = 250)\nsummary(cfa_fit, standard=T)\n\nlavaan 0.6-19 ended normally after 63 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        25\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                                80.964\n  Degrees of freedom                                30\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  CREA =~                                                               \n    CREAT             1.000                               4.200    1.000\n  SEFF =~                                                               \n    SEFF1             1.000                               0.537    0.846\n    SEFF2             0.474    0.074    6.421    0.000    0.255    0.417\n    SEFF3             0.735    0.091    8.116    0.000    0.395    0.515\n    SEFF4             0.834    0.070   11.981    0.000    0.448    0.716\n  PER =~                                                                \n    PER1              1.000                               2.274    0.828\n    PER2              1.009    0.064   15.877    0.000    2.294    0.866\n    PER3              0.875    0.062   14.129    0.000    1.989    0.792\n  SOC =~                                                                \n    SOC1              1.000                               1.478    0.700\n    SOC2              1.064    0.149    7.148    0.000    1.573    0.913\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  CREA ~~                                                               \n    SEFF              1.861    0.205    9.093    0.000    0.826    0.826\n    PER               7.467    0.852    8.761    0.000    0.782    0.782\n    SOC               2.575    0.546    4.717    0.000    0.415    0.415\n  SEFF ~~                                                               \n    PER               0.985    0.121    8.145    0.000    0.807    0.807\n    SOC               0.342    0.076    4.485    0.000    0.432    0.432\n  PER ~~                                                                \n    SOC               1.787    0.350    5.114    0.000    0.532    0.532\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .CREAT             0.000                               0.000    0.000\n   .SEFF1             0.114    0.018    6.519    0.000    0.114    0.284\n   .SEFF2             0.308    0.028   10.834    0.000    0.308    0.826\n   .SEFF3             0.431    0.041   10.582    0.000    0.431    0.734\n   .SEFF4             0.191    0.020    9.385    0.000    0.191    0.488\n   .PER1              2.365    0.283    8.355    0.000    2.365    0.314\n   .PER2              1.758    0.241    7.302    0.000    1.758    0.250\n   .PER3              2.354    0.261    9.033    0.000    2.354    0.373\n   .SOC1              2.279    0.339    6.714    0.000    2.279    0.511\n   .SOC2              0.495    0.311    1.592    0.111    0.495    0.167\n    CREA             17.640    1.578   11.180    0.000    1.000    1.000\n    SEFF              0.288    0.037    7.721    0.000    1.000    1.000\n    PER               5.170    0.667    7.751    0.000    1.000    1.000\n    SOC               2.185    0.438    4.992    0.000    1.000    1.000\n\nfitmeasures(cfa_fit)[c('chisq', 'df', 'pvalue', 'nfi', 'nnfi', 'cfi', 'rmsea', 'srmr')] |&gt; round(3)\n\n chisq     df pvalue    nfi   nnfi    cfi  rmsea   srmr \n80.964 30.000  0.000  0.936  0.937  0.958  0.082  0.042 \n\n\n\n\n结构模型分析阶段\n将测量模型与结构模型组合在一起就构成了统合模型。测量模型不变，按照研究假设设定结构模型，进行分析并报告结果。\n\nsem_mod&lt;-'\n# 测量模型\n    CREA =~ 1*CREAT\n    SEFF =~ SEFF1 + SEFF2 + SEFF3 + SEFF4\n    PER  =~ PER1  + PER2  + PER3\n    SOC  =~ SOC1  + SOC2\n# 结构模型\n    SEFF ~ a1*PER + c1*SOC\n    CREA ~ a2*PER + b1*SEFF + c2*SOC\n# 设定外生潜变量之间的协方差\n    SOC  ~~ PER \n'\nsem_fit &lt;- sem(model = sem_mod, sample.cov = COV, sample.nobs = 250)\nsummary(sem_fit, standard=T)\n\nlavaan 0.6-19 ended normally after 81 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        25\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                                80.964\n  Degrees of freedom                                30\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  CREA =~                                                               \n    CREAT             1.000                               4.200    1.000\n  SEFF =~                                                               \n    SEFF1             1.000                               0.537    0.846\n    SEFF2             0.474    0.074    6.421    0.000    0.255    0.417\n    SEFF3             0.735    0.091    8.116    0.000    0.395    0.515\n    SEFF4             0.834    0.070   11.981    0.000    0.448    0.716\n  PER =~                                                                \n    PER1              1.000                               2.274    0.828\n    PER2              1.009    0.064   15.877    0.000    2.294    0.866\n    PER3              0.875    0.062   14.129    0.000    1.989    0.792\n  SOC =~                                                                \n    SOC1              1.000                               1.478    0.700\n    SOC2              1.064    0.149    7.148    0.000    1.573    0.913\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SEFF ~                                                                \n    PER       (a1)    0.190    0.019    9.944    0.000    0.805    0.805\n    SOC       (c1)    0.001    0.026    0.046    0.964    0.003    0.003\n  CREA ~                                                                \n    PER       (a2)    0.616    0.189    3.266    0.001    0.334    0.334\n    SEFF      (b1)    4.364    0.807    5.407    0.000    0.558    0.558\n    SOC       (c2)   -0.010    0.147   -0.065    0.948   -0.003   -0.003\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  PER ~~                                                                \n    SOC               1.787    0.350    5.114    0.000    0.532    0.532\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .CREAT             0.000                               0.000    0.000\n   .SEFF1             0.114    0.018    6.519    0.000    0.114    0.284\n   .SEFF2             0.308    0.028   10.834    0.000    0.308    0.826\n   .SEFF3             0.431    0.041   10.582    0.000    0.431    0.734\n   .SEFF4             0.191    0.020    9.385    0.000    0.191    0.488\n   .PER1              2.365    0.283    8.355    0.000    2.365    0.314\n   .PER2              1.758    0.241    7.302    0.000    1.758    0.250\n   .PER3              2.354    0.261    9.033    0.000    2.354    0.373\n   .SOC1              2.279    0.339    6.714    0.000    2.279    0.511\n   .SOC2              0.495    0.311    1.592    0.111    0.495    0.167\n   .CREA              4.941    0.600    8.238    0.000    0.280    0.280\n   .SEFF              0.100    0.020    4.954    0.000    0.349    0.349\n    PER               5.170    0.667    7.751    0.000    1.000    1.000\n    SOC               2.185    0.438    4.992    0.000    1.000    1.000\n\n\n各因子载荷均达到显著性水平，说明测量模型理想；结构模型部分，性格对自我效能感、自我效能感对创新行性格对创新行为的3个回归系数显著，社会化对自我效能感和创新行为的2个回归系数均不显著。两个外生潜变量性格和社会化的协方差和2个方差都是显著的。\n模型拟合情况如下，可以看到，模型的拟合程度还是不错的，但仍有修正空间。\n\nfitmeasures(sem_fit)[c('chisq', 'df', 'pvalue', 'nfi', 'nnfi', 'cfi', 'rmsea', 'srmr')] |&gt; round(3) # 查看拟合结果\n\n chisq     df pvalue    nfi   nnfi    cfi  rmsea   srmr \n80.964 30.000  0.000  0.936  0.937  0.958  0.082  0.042 \n\nsemPaths(sem_fit, \n         what = \"std\", \n         fade = F, \n         rotation = 2,\n         layout = \"tree2\",\n         edge.color = \"darkgrey\",\n         esize = 3,\n         edge.label.cex = 1.1,\n         ask = FALSE)\n\n\n\n\n\n\n\n\n\n\n模型调整\n\nmodificationindices(sem_fit, sort. = T, maximum.number = 5)\n\n     lhs op   rhs     mi    epc sepc.lv sepc.all sepc.nox\n47   PER =~ SEFF1 13.127  0.154   0.351    0.553    0.553\n64 CREAT ~~ SEFF4 13.032  0.329   0.329       NA       NA\n34  CREA =~ SEFF4 12.759  0.073   0.308    0.492    0.492\n72 SEFF1 ~~ SEFF4 11.869 -0.068  -0.068   -0.459   -0.459\n80 SEFF2 ~~  PER1  9.985 -0.196  -0.196   -0.230   -0.230\n\n\n模型修饰指数(MI)最高的五项推荐了5条修改路径。第一条是外生潜变量性格（PER）影响自我效能的内生测量变量自我肯定（SEFF1）的因子载荷（MI=13.13）；第二条是内生测量变量创新行为（CREAT）和压力抗衡（SEFF4）的测量残差的相关性；第三条是内生潜变量创新行为（CREA）影响自我效能的测量变量压力抗衡（SEFF4）的因子载荷（MI=13.03）；第四条是测量变量自我肯定（SEFF1）和压力抗衡（SEFF4）的测量误差的相关性；第五条是测量变量（SEFF2）与（PER1）之间的测量残差的相关性。首先模型修正以结构参数和测量因子载荷参数优先，因为这些参数涉及模型的潜变量的测量和研究假设，残差的考虑与观察不到的因素和隐含条件相关，因此不优先考虑对其调整。因此重点关注第一条和第三条建议路径。同时潜变量与测量变量之间的关系，也优先考虑内生潜变量对内生测量变量的影响或外生潜变量对外生测量变量的影响，在测量模型层面考虑内外生变量的交叉影响会使得模型过于复杂，也很难有理论的支撑。因此，选择第三条路径进行修正，这条路径也有比较合理的解释，即创新行为（CREA）影响压力抗衡（SEFF4）的因子载荷，意味当教师表现出创新行为后，可能增强其抗压的信念，因此可以将此参数纳入模型设定中。\n\nmod1_mod &lt;- '\n# 测量模型\n    CREA =~ CREAT + SEFF4 # 修饰模型，添加CREA影响SEFF4的路径\n    SEFF =~ SEFF1 + SEFF2 + SEFF3 + SEFF4\n    PER  =~ PER1  + PER2  + PER3\n    SOC  =~ SOC1  + SOC2\n    CREAT ~~ 0 * CREAT  # 固定内生测量变量CREAT的残差为0\n# 结构模型\n    SEFF ~ a1*PER + c1*SOC\n    CREA ~ a2*PER + b1*SEFF + c2*SOC\n# 设定外源潜在变量间的协方差\n    SOC  ~~ PER \n'\n\nmod1_fit &lt;- sem(model = mod1_mod, sample.cov = COV, sample.nobs = 250)\nsummary(mod1_fit, standard=T)\n\nlavaan 0.6-19 ended normally after 94 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        26\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                                67.863\n  Degrees of freedom                                29\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  CREA =~                                                               \n    CREAT             1.000                               4.200    1.000\n    SEFF4             0.058    0.013    4.614    0.000    0.243    0.389\n  SEFF =~                                                               \n    SEFF1             1.000                               0.584    0.921\n    SEFF2             0.432    0.069    6.290    0.000    0.253    0.414\n    SEFF3             0.665    0.086    7.773    0.000    0.389    0.507\n    SEFF4             0.361    0.101    3.589    0.000    0.211    0.337\n  PER =~                                                                \n    PER1              1.000                               2.280    0.831\n    PER2              1.003    0.063   15.965    0.000    2.288    0.864\n    PER3              0.872    0.061   14.206    0.000    1.988    0.791\n  SOC =~                                                                \n    SOC1              1.000                               1.484    0.702\n    SOC2              1.056    0.147    7.173    0.000    1.567    0.909\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SEFF ~                                                                \n    PER       (a1)    0.205    0.020   10.479    0.000    0.801    0.801\n    SOC       (c1)   -0.017    0.027   -0.618    0.537   -0.043   -0.043\n  CREA ~                                                                \n    PER       (a2)    0.916    0.190    4.827    0.000    0.497    0.497\n    SEFF      (b1)    2.561    0.713    3.590    0.000    0.356    0.356\n    SOC       (c2)    0.039    0.154    0.255    0.798    0.014    0.014\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  PER ~~                                                                \n    SOC               1.807    0.351    5.145    0.000    0.534    0.534\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .CREAT             0.000                               0.000    0.000\n   .SEFF4             0.211    0.020   10.758    0.000    0.211    0.539\n   .SEFF1             0.061    0.025    2.483    0.013    0.061    0.151\n   .SEFF2             0.309    0.028   10.864    0.000    0.309    0.829\n   .SEFF3             0.436    0.041   10.613    0.000    0.436    0.743\n   .PER1              2.334    0.279    8.355    0.000    2.334    0.310\n   .PER2              1.783    0.240    7.443    0.000    1.783    0.254\n   .PER3              2.357    0.260    9.079    0.000    2.357    0.374\n   .SOC1              2.261    0.340    6.657    0.000    2.261    0.507\n   .SOC2              0.515    0.308    1.672    0.094    0.515    0.173\n   .CREA              5.970    0.640    9.330    0.000    0.338    0.338\n   .SEFF              0.134    0.029    4.631    0.000    0.393    0.393\n    PER               5.200    0.667    7.795    0.000    1.000    1.000\n    SOC               2.203    0.439    5.014    0.000    1.000    1.000\n\nfitmeasures(mod1_fit)[c('chisq', 'df', 'pvalue', 'nfi', 'nnfi', 'cfi', 'rmsea', 'srmr')] |&gt; round(3) # 查看拟合结果\n\n chisq     df pvalue    nfi   nnfi    cfi  rmsea   srmr \n67.863 29.000  0.000  0.946  0.950  0.968  0.073  0.041 \n\n\n新添加的标准化因子载荷为0.389，达到显著水平。整体模型拟合度也有改善，卡方值为67.86，RMSEA为0.073。再检查MI指数，新的路径建议第四项SEFF3与SEFF4的残差相关还需要添加。其余建议路径都是内生测量变量与外生测量变量残差之间的相关性，建议路径太复杂不考虑。\n\nmodificationindices(mod1_fit, sort. = T, maximum.number = 5)\n\n     lhs op   rhs     mi    epc sepc.lv sepc.all sepc.nox\n84 SEFF1 ~~  SOC2 10.929 -0.128  -0.128   -0.723   -0.723\n86 SEFF2 ~~  PER1 10.650 -0.201  -0.201   -0.237   -0.237\n76 SEFF4 ~~  SOC1  9.449 -0.146  -0.146   -0.212   -0.212\n72 SEFF4 ~~ SEFF3  8.778  0.060   0.060    0.199    0.199\n90 SEFF2 ~~  SOC2  8.058  0.127   0.127    0.319    0.319\n\n\n添加残差相关路径后，设定模型进行分析。\n\nmod2_mod &lt;- '\n# 测量模型\n    CREA =~ CREAT + SEFF4 # 修饰模型，添加CREA影响SEFF4的路径\n    SEFF =~ SEFF1 + SEFF2 + SEFF3 + SEFF4\n    PER  =~ PER1  + PER2  + PER3\n    SOC  =~ SOC1  + SOC2\n    CREAT ~~ 0 * CREAT \n# 结构模型\n    SEFF ~ a1*PER + c1*SOC\n    CREA ~ a2*PER + b1*SEFF + c2*SOC\n# 设定外源潜在变量间的协方差\n    SOC  ~~ PER \n    SEFF3 ~~ SEFF4 # 添加内生测量变量的相关性\n# 定义间接效应 (a*b)\n    PERindiCREA := a1*b1\n    SOCindiCREA := c1*b1\n# 定义总效应\n    PERtotCREA := PERindiCREA + a2\n    SOCtotCREA := SOCindiCREA + c2\n'\n\nmod2_fit &lt;- sem(model = mod2_mod, sample.cov = COV, sample.nobs = 250)\n\nfitmeasures(mod2_fit)[c('chisq', 'df', 'pvalue', 'nfi', 'nnfi', 'cfi', 'rmsea', 'srmr')] |&gt; round(3) # 查看拟合结果\n\n chisq     df pvalue    nfi   nnfi    cfi  rmsea   srmr \n59.089 28.000  0.001  0.953  0.959  0.974  0.067  0.039 \n\nmodificationindices(mod2_fit, sort. = T, maximum.number = 5)\n\n     lhs op   rhs     mi    epc sepc.lv sepc.all sepc.nox\n90 SEFF2 ~~  PER1 10.595 -0.201  -0.201   -0.236   -0.236\n80 SEFF4 ~~  SOC1  9.903 -0.147  -0.147   -0.211   -0.211\n88 SEFF1 ~~  SOC2  9.604 -0.123  -0.123   -0.807   -0.807\n60   SOC =~ SEFF1  9.189 -0.129  -0.191   -0.301   -0.301\n94 SEFF2 ~~  SOC2  8.452  0.130   0.130    0.327    0.327\n\n\n模型拟合优度有所提升，卡方值减少到59.09，RMSEA下降到0.067。MI指数检验，只剩内生与外生测量变量的相关性路径，不考虑复杂关系，没有需要调整的路径。同时，由于最终模型和原模型有嵌套关系，因此可以进行卡方检验，显示模型具有显著改进，因此模型调整结束。\n\nanova(mod2_fit, sem_fit)\n\n\nChi-Squared Difference Test\n\n         Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nmod2_fit 28 7919.1 8014.2 59.089                                          \nsem_fit  30 7937.0 8025.0 80.964     21.875 0.19937       2  1.778e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n报告分析结果\n\n报告标准化系数：包括标准化的因子载荷、结构模型的标准化系数等。\n\n\nsummary(mod2_fit, standard=T)\n\nlavaan 0.6-19 ended normally after 235 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        27\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                                59.089\n  Degrees of freedom                                28\n  P-value (Chi-square)                           0.001\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  CREA =~                                                               \n    CREAT             1.000                               4.200    1.000\n    SEFF4             0.062    0.011    5.502    0.000    0.262    0.420\n  SEFF =~                                                               \n    SEFF1             1.000                               0.597    0.942\n    SEFF2             0.414    0.068    6.122    0.000    0.247    0.405\n    SEFF3             0.631    0.085    7.403    0.000    0.377    0.492\n    SEFF4             0.311    0.090    3.470    0.001    0.186    0.297\n  PER =~                                                                \n    PER1              1.000                               2.283    0.832\n    PER2              1.001    0.063   15.987    0.000    2.286    0.863\n    PER3              0.871    0.061   14.233    0.000    1.988    0.791\n  SOC =~                                                                \n    SOC1              1.000                               1.482    0.701\n    SOC2              1.059    0.148    7.166    0.000    1.569    0.911\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SEFF ~                                                                \n    PER       (a1)    0.209    0.020   10.678    0.000    0.799    0.799\n    SOC       (c1)   -0.023    0.027   -0.847    0.397   -0.057   -0.057\n  CREA ~                                                                \n    PER       (a2)    0.960    0.186    5.148    0.000    0.522    0.522\n    SEFF      (b1)    2.301    0.672    3.423    0.001    0.327    0.327\n    SOC       (c2)    0.047    0.155    0.306    0.759    0.017    0.017\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  PER ~~                                                                \n    SOC               1.805    0.351    5.139    0.000    0.534    0.534\n .SEFF4 ~~                                                              \n   .SEFF3             0.060    0.021    2.864    0.004    0.060    0.193\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .CREAT             0.000                               0.000    0.000\n   .SEFF4             0.215    0.020   10.911    0.000    0.215    0.552\n   .SEFF1             0.046    0.027    1.676    0.094    0.046    0.113\n   .SEFF2             0.311    0.029   10.905    0.000    0.311    0.836\n   .SEFF3             0.445    0.042   10.669    0.000    0.445    0.758\n   .PER1              2.323    0.278    8.348    0.000    2.323    0.308\n   .PER2              1.794    0.240    7.490    0.000    1.794    0.256\n   .PER3              2.357    0.259    9.089    0.000    2.357    0.374\n   .SOC1              2.268    0.340    6.679    0.000    2.268    0.508\n   .SOC2              0.508    0.309    1.644    0.100    0.508    0.171\n   .CREA              6.082    0.642    9.477    0.000    0.345    0.345\n   .SEFF              0.145    0.032    4.555    0.000    0.407    0.407\n    PER               5.212    0.667    7.811    0.000    1.000    1.000\n    SOC               2.196    0.439    5.007    0.000    1.000    1.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    PERindiCREA       0.481    0.142    3.396    0.001    0.262    0.262\n    SOCindiCREA      -0.053    0.064   -0.835    0.404   -0.019   -0.019\n    PERtotCREA        1.441    0.123   11.681    0.000    0.783    0.783\n    SOCtotCREA       -0.006    0.168   -0.034    0.973   -0.002   -0.002\n\n\n\n报告直接效应和间接效应\n\n展示创造性格（PER）和组织社会化（SOC），对于创意教学行为（CREA）的间接效应和总效应量及其统计检验。\n\nsummary(mod2_fit, standard = T)$pe |&gt;\n  filter(op == \":=\") |&gt;\n  select(label, est, se, z, pvalue, std.all) |&gt;\n  mutate(across(where(is.numeric), ~round(., 3))) |&gt;\n  arrange(label) |&gt;\n  knitr::kable(caption = \"潜在变量路径分析的部分复杂效应量\")\n\n\n潜在变量路径分析的部分复杂效应量\n\n\nlabel\nest\nse\nz\npvalue\nstd.all\n\n\n\n\nPERindiCREA\n0.481\n0.142\n3.396\n0.001\n0.262\n\n\nPERtotCREA\n1.441\n0.123\n11.681\n0.000\n0.783\n\n\nSOCindiCREA\n-0.053\n0.064\n-0.835\n0.404\n-0.019\n\n\nSOCtotCREA\n-0.006\n0.168\n-0.034\n0.973\n-0.002\n\n\n\n\n\n\n报告路径图\n\n\nsemPaths(mod2_fit, \n         what = \"std\", \n         fade = F, # 关闭路径颜色渐变\n         rotation = 2,\n         layout = \"tree2\",\n         edge.color = \"darkgrey\",\n         esize = 3,\n         edge.label.cex = 1.1,\n         ask = FALSE)\n\n\n\n\n\n\n\n\n参考文献：《结构方程与建模的原理与应用》，邱皓政、林碧芳著，中国轻工业出版社，2009."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "enp/introduction.html#课程简介",
    "href": "enp/introduction.html#课程简介",
    "title": "环境政策分析课程介绍",
    "section": "课程简介",
    "text": "课程简介\n本课程以中国环境政策为切入点，系统揭示环境治理背后那些不常被看见、却对理解政策形成至关重要的制度机制和政治逻辑。课程不仅关注空气、水、土地利用、农业、能源、废物处理等领域的政策，还力求让学生在此过程中全面理解中国环境政策的形成与实施。\n环境政策的悖论 所谓环境政策悖论，是指我们往往清楚环境问题的最佳解决方案，但这些方案在实践中却常常迟迟无法落实，或者落实不足。中国在环境治理中同样面临这一悖论。例如：\n\n农业与粮食安全：中国农业发展中存在耕地退化、农药和化肥过度使用等问题。尽管有机农业和绿色农业能够改善土壤质量、提高农产品附加值、保障长期粮食安全，但由于农业补贴体系、地方财政压力以及农资企业的市场影响力，政策转型仍存在阻力。\n能源转型：尽管“碳达峰、碳中和”目标已经确立，并且可再生能源发展迅速，但以煤炭为主的能源结构短期内难以彻底改变。能源转型所带来的经济、社会与就业挑战，使得政策落实进展缓慢。\n\n政策形成的制度环境 理解中国的环境政策，必须把握其政策制定的背景。中国环境政策形成受到多重因素制约：\n\n信息不完全与复杂性：环境问题跨部门、跨区域，信息不对称常导致政策滞后或执行困难。\n制度与激励：中央政府在制定目标时具有较强的政策推动力，但地方政府在执行时往往受到经济发展目标、财政收入和官员考核的多重激励制约。\n社会参与：公众舆论、媒体监督、环保组织的行动，在推动环境问题进入政府议程中日益重要，但其作用仍存在边界。\n本跨学科课程旨在介绍环境经济学与环境政治学的理论，重点关注政策分析与评估。\n我们将介绍外部性理论、政策过程模型、政策网络以及环境政策的分析框架。\n我们将介绍公众对环境问题的不同观点及其哲学根源，并介绍不同主体在规制环境中的互动所产生的后果。\n我们将运用环境经济学比较和分析环境政策工具，如规制型、自愿型与市场型方法，并进一步介绍风险评估与环境正义问题。\n环境问题正日益受到公众关注，环境政策已成为公共政策领域的重要组成部分。\n本课程面向大二以上本科生，作为一门专业课程，提供经济学与政策科学的视角与工具，以解决环境问题。"
  },
  {
    "objectID": "enp/introduction.html#学习目标",
    "href": "enp/introduction.html#学习目标",
    "title": "环境政策分析课程介绍",
    "section": "学习目标",
    "text": "学习目标\n\n识别并有效应对环境问题。\n熟悉环境经济学与政策的理论和模型，掌握理论与实践的结合。\n理解环境政策过程及其影响因素，包括：\n\n集体行动问题\n议题关注周期\n有限理性\n权力运用\n\n了解环境政策工具（命令与控制、市场型、信息型机制）的优缺点。\n掌握环境价值评估的概念与方法，能运用经济学框架评估政策影响。\n理解产权与外部性的相关知识，分析市场与政府在资源配置中的失灵。\n掌握自然资源与环境经济学的模型与方法，并应用于生物多样性、空气与水污染等议题。\n获得环境政策分析的基本能力，能理解政策过程并进行实际评估。"
  },
  {
    "objectID": "enp/introduction.html#教学方法",
    "href": "enp/introduction.html#教学方法",
    "title": "环境政策分析课程介绍",
    "section": "教学方法",
    "text": "教学方法\n\n教学形式：讲授、讨论、辅导\n讲授：30 学时\n专题讨论：2 学时\n要求学生课前阅读相关资料\n案例教学：运用经济学与政策理论分析环境政策，结合中国环境政策改革实践"
  },
  {
    "objectID": "enp/introduction.html#课程大纲",
    "href": "enp/introduction.html#课程大纲",
    "title": "环境政策分析课程介绍",
    "section": "课程大纲",
    "text": "课程大纲\n\n第1周（9/13）\n环境政策分析导论\n\n课程简介\n学习目标\n教学方法\n课程安排\n评估方式\n教材与参考书目\n答疑安排\n期末政策备忘录说明\n\n\n\n第2周（9/20）\n环境政策与管理\n\n环境政策\n环境管理\n中国环境政策与管理实践\n\n\n\n第3周（9/27）\n环境政策分析的理论与框架\n\n外部性\n政策过程模型\n政策网络与网络化环境治理\nCohen 框架\n\n\n\n第5周（10/11）\n经济分析工具\n\n收益与成本、供需、支付意愿、机会成本、均衡边际原则\n市场、经济效率、市场失灵、外部性、公共品、公地与共有资源\n损害函数、治理成本函数、社会最优排放水平、执法成本\n\n\n\n第6周（10/18）\n成本收益分析\n\n收益：直接损害成本、显示性偏好、陈述性偏好方法\n成本：有/无原则、社会成本、设施成本、地方规制成本、行业与国家层面成本\n\n\n\n第7周（10/25）\n规制型政策分析\n\n命令与控制方法\n标准类型\n效率与成本效益评估\n\n\n\n第8周（11/1）\n激励型政策分析\n\n排放税与补贴\n市场化制度\n总量控制与交易制度\n成本效益比较\n\n\n\n第9周（11/8）\n公众舆论与环境规制\n\n环境话语：生存主义、普罗米修斯观、怀疑主义、行政理性、民主实用主义、经济理性、可持续发展、生态现代化、生态文明、绿色政治思想\n规制环境：科学与风险分析、成本收益分析、政府角色、规制方法、环境法\n\n\n\n第10周（11/15）\n风险评估与环境正义\n\n风险评估、科学与政治关系、风险管理、风险标准与沟通、预防原则\n环境正义：风险与歧视、环境不平等、治理挑战\n\n\n\n第11周（11/22）\n环境与社会影响评估\n\n环境影响评价\n社会影响评价\n\n\n\n第12周（11/29）\n环境政策的监测与评估\n\n环境监测\n主要评估方法：形成性评估、环境影响评价、目标无关评估、实验与准实验方法、经济评估、理论导向评估\n\n\n\n第13周（12/6）\n资源政策与管理\n\n自然资源的经济评价\n将自然资源视为资产管理\n自然资源政策与管理\n\n\n\n第14周（12/13）\n社会网络分析在资源管理中的应用\n\n网络的结构特征\n自然资源利用与开采中的社会网络模型\n\n\n\n第15周（12/20）\nREDD：减少毁林与森林退化导致的排放\n\n案例比较\n全球环境政治\n\n\n\n第16周（12/27）\n课程总结与口头报告"
  },
  {
    "objectID": "enp/introduction.html#考核方式",
    "href": "enp/introduction.html#考核方式",
    "title": "环境政策分析课程介绍",
    "section": "考核方式",
    "text": "考核方式\n\n出勤：10%\n作业：60%\n\n政策权衡矩阵（1页，10/11截止）\n经济分析工具习题（10/18截止）\n成本收益分析习题（10/25截止）\n环境政策经济分析习题（11/8截止）\n公众意见调查设计（2页，11/22截止）\n环境影响评价设计（2页，11/29截止）\n\n期末政策备忘录：20%（5页，英文，12/27截止）\n口头报告：10%（12/27课堂展示，3-5分钟）"
  },
  {
    "objectID": "enp/introduction.html#教材与参考书",
    "href": "enp/introduction.html#教材与参考书",
    "title": "环境政策分析课程介绍",
    "section": "教材与参考书",
    "text": "教材与参考书\n环境经济学\n\n马中、刘学敏：《人口资源与环境经济学》，高等教育出版社，2019\nField, Barry & Field, Martha：《环境经济学导论》，第7版，McGraw Hill，2017 （中文版：东北财经大学出版社）\n\n环境政治与政策\n\n宋国军：《环境政策分析（第二版）》，化学工业出版社，2019\nNeil Carter：《The Politics of the Environment》，第2版，剑桥大学出版社，2007\nSteven Cohen：《Understanding Environmental Policy》，第2版，哥伦比亚大学出版社"
  },
  {
    "objectID": "enp/introduction.html#答疑与办公时间",
    "href": "enp/introduction.html#答疑与办公时间",
    "title": "环境政策分析课程介绍",
    "section": "答疑与办公时间",
    "text": "答疑与办公时间\n\nQQ群：593521662（请假、答疑）\n助教：吴雨霜（作业收集与批改、答疑）\n时间：每周一下午 2:30-5:30 或预约\n地点：成智二号楼 221 室\n\n提供帮助：\n\n课程内容解答\n论文写作指导\n作业准备与反馈\n成绩咨询与学术建议"
  },
  {
    "objectID": "enp/introduction.html#政策备忘录要求",
    "href": "enp/introduction.html#政策备忘录要求",
    "title": "环境政策分析课程介绍",
    "section": "政策备忘录要求",
    "text": "政策备忘录要求\n目标：将课堂与阅读中的概念应用于案例研究。\n要求：\n\n概念部分：解释一个概念，结合理论与政策，并联系案例。\n案例部分：聚焦课堂涉及的议题，可涉及问题本身、政策制定、经济分析等。\n理论与政策结合：讨论理论在实际政策中的适用性，并提出修正建议。"
  },
  {
    "objectID": "enp/introduction.html#论文写作指南",
    "href": "enp/introduction.html#论文写作指南",
    "title": "环境政策分析课程介绍",
    "section": "论文写作指南",
    "text": "论文写作指南\n\n研究问题与假设：明确研究问题，区分问题与假设，并分解为子问题。\n研究范围：限定地理、时间、行业等范围。\n理论框架：说明采用的理论与原因，并对比不同理论。\n论文结构：摘要、引言、主体、讨论、结论与总结、参考文献、附录。\n文献规范：遵循 APA 格式，至少引用 10 篇学术文献。"
  },
  {
    "objectID": "enp/introduction.html#学术规范",
    "href": "enp/introduction.html#学术规范",
    "title": "环境政策分析课程介绍",
    "section": "学术规范",
    "text": "学术规范\n\n抄袭属严重学术不端，将直接判定课程成绩为 0 分。\n\n\n课程特色与新增内容 本课程将重点讨论以下议题：\n\n“双碳”战略与能源转型：分析中国应对气候变化的政策逻辑，以及能源结构调整中的博弈与困境。\n环境法治与治理工具：介绍中国《环境保护法》《大气污染防治法》《长江保护法》等法律制度的实施情况，以及环境影响评价、排污许可、碳交易等治理工具。\n重大环境事件与政策转折：探讨如松花江水污染、雾霾治理、“长江十年禁渔”等案例如何推动政策变革。\n国际比较与全球责任：结合《巴黎协定》等国际议程，分析中国在全球环境治理中的角色与责任，尤其是中美关系对气候政策的影响。\n\n课程结构\n\n第一部分：政策制定过程\n\n中国环境政策的制度框架与非正式影响因素\n地方政府、企业、公众和环保组织在政策过程中的作用\n环境诉讼与司法救济的发展\n\n第二部分：环境政策专题\n\n大气污染治理与碳排放政策\n能源政策与可再生能源发展\n固体废物与危险废物管理\n水环境保护与流域治理\n土地利用与生态修复\n国际环境问题与中国的全球治理参与\n\n\n课程目标 通过本课程的学习，学生将：\n\n理解中国环境问题的复杂性与政策制定的现实逻辑；\n掌握环境政策背后的制度激励与执行困境；\n具备对比中外环境政策、评估政策成效的能力；\n在未来从事公共管理、环境治理或相关研究时，能够提出具有现实针对性的改进建议。"
  },
  {
    "objectID": "enp/introduction.html#什么是生态系统",
    "href": "enp/introduction.html#什么是生态系统",
    "title": "环境政策分析课程介绍",
    "section": "什么是生态系统？",
    "text": "什么是生态系统？\n生态系统是指一群植物、动物或非生物要素在外部环境中相互作用所构成的整体。通常，生态学家会研究：\n\n个体生物（其生命周期、环境需求及其功能）；\n生物种群（包括稳定性、增长或衰退等问题）；\n生物群落；\n整个生态系统（涉及碳、氧、氢、土壤矿物及能量的生物地球化学循环）。\n\n表面上看，生态系统似乎是相对独立的单元，与外部环境互动很少。然而，生态系统实际上是开放系统，会与外部环境的各种要素互动。事实上，整个地球本身就是一个生态系统，通常被称为生态圈或生物圈。\n地球与所有生态系统一样，从太阳获得能量。除此之外，地球与外部环境几乎没有其他互动。因此，除了来自太阳的能量外，地球上的资源自形成以来至今始终是有限的（虽然其形态不断发生变化）。这就是“宇宙飞船地球”一词的由来——地球如同飞船，必须依靠已有的资源生存。当某些资源耗尽，或被转化为人类无法利用的形式时，新的供应并不能“从外界进口”，人类对这些资源的依赖功能便永远丧失。"
  },
  {
    "objectID": "enp/introduction.html#生态循环的简化例子",
    "href": "enp/introduction.html#生态循环的简化例子",
    "title": "环境政策分析课程介绍",
    "section": "生态循环的简化例子",
    "text": "生态循环的简化例子\n为了理解生态系统的运作，我们来看一个简化的淡水循环例子：河流或湖泊中的鱼会产生有机废物，这些废物沉入水底，为细菌提供养分，并生成无机物，藻类再以其为食，而鱼又以藻类为食。这个循环看似简单，但正如生态学家 Edward Kormondy 指出，研究自然系统时会面对几乎无限复杂的变量。\n生态学家巴里·科蒙纳（Barry Commoner）在《闭合的圈》中提出了生态学的四条法则，这些法则帮助我们理解生态系统的运作规律，以及人类操控自然的局限性。"
  },
  {
    "objectID": "enp/introduction.html#生态学的四条法则",
    "href": "enp/introduction.html#生态学的四条法则",
    "title": "环境政策分析课程介绍",
    "section": "生态学的四条法则",
    "text": "生态学的四条法则\n\n一切事物彼此相连 生态系统由多个相互联系的部分组成。操纵系统中的某一部分，往往会对其他部分产生意想不到的影响。生态系统像一副纸牌屋，任何一个环节的变化都会影响整体的稳定性。\n例如，草原干旱会导致老鼠缺乏食物而进入休眠，进而减少捕食风险，同时让草恢复生长。\n一切事物必须有去处 任何物质不会凭空消失。比如，一节含汞电池最终可能通过垃圾焚烧、雨水沉降进入湖泊，被细菌转化为甲基汞，随后在鱼体内累积，最终进入人体，造成危害。\n类似地，化石燃料燃烧释放的二氧化硫、氮氧化物最终会转化为酸雨或温室气体，对环境和气候产生深远影响。\n自然最懂得 人类自以为能够征服自然，但对自然系统的干预往往适得其反。科蒙纳用“钟表”的比喻说明：随意用铅笔戳钟表机芯，几乎必然导致损坏。同样，人类的随意干预会破坏自然系统中经过数十亿年演化形成的微妙平衡。\n例如，20世纪70年代推广的汽油添加剂 MTBE 原本被认为能减少污染，但由于其高溶解性和难以分解的特性，最终导致严重的地下水污染，并带来巨额治理成本。\n没有免费的午餐 人类对自然的任何利用都会有代价，可能表现为资源不可逆的转化，或生态系统的失衡。生态学的整体性决定了任何“收益”都必须伴随某种“支付”，只是支付可能被延迟，但无法避免。"
  },
  {
    "objectID": "enp/introduction.html#稳态与可持续性",
    "href": "enp/introduction.html#稳态与可持续性",
    "title": "环境政策分析课程介绍",
    "section": "稳态与可持续性",
    "text": "稳态与可持续性\n在四条法则之外，还需理解两个概念：稳态（steady state）与可持续性（sustainability）。\n\n稳态：生态系统能长期维持的活动水平。例如，有限的耕地和水资源决定了人类人口的上限。生态系统通常经历适应性循环：生长、积累、崩溃、重组。如果效率过高（如单一化林木种植），其抗扰动能力反而降低，更容易在火灾、病虫害等干扰下崩溃。\n可持续性：资源的使用和环境的污染必须保持在自然系统的再生能力之内，否则就是不可持续的。例如，伐木量不能超过森林的再生量，污染物的排放也不能超过自然的吸纳能力。所谓“可持续增长”本身就是自相矛盾的说法。"
  },
  {
    "objectID": "enp/introduction.html#公地悲剧与共享资源",
    "href": "enp/introduction.html#公地悲剧与共享资源",
    "title": "环境政策分析课程介绍",
    "section": "公地悲剧与共享资源",
    "text": "公地悲剧与共享资源\n生态学家加勒特·哈丁在《公地的悲剧》中指出，共享资源常常因个体逐利而被过度利用。\n\n地下水：农户为避免邻居先抽水，倾向于尽快开采，导致水源枯竭。\n空气与水体：个人或企业缺乏控制排放的动力，污染成本由全体社会承担。\n渔业与海洋：各国为竞争渔获，缺乏限制捕捞的动力，最终导致渔业资源衰竭。\n\n缺乏强有力的国际治理机构，使得公地型资源在全球层面更难管理。"
  },
  {
    "objectID": "enp/introduction.html#小结",
    "href": "enp/introduction.html#小结",
    "title": "环境政策分析课程介绍",
    "section": "小结",
    "text": "小结\n我们生活在一个资源有限的世界。每开一次车，就减少了不可再生的石油储量，同时增加了温室气体排放；每用一张纸，就减少了森林和栖息地。自然具有一定的恢复力，但当人类活动越过临界点时，生态系统可能无法再维持生命所需的功能。\n因此，环境政策的制定者必须理解生态规律、稳态与可持续性、公地资源问题。这不仅是科学问题，更涉及价值观、文化与政治——这将是下一章的主题。\n\n\n思考题\n\n为什么在制定环境政策时必须考虑生态学的法则？\n什么是适应性循环？它为何存在？\n为什么在制定与实施环境政策时需要考虑适应性循环？\n什么是生态系统？结合你熟悉的环境，哪些因素可能破坏它？\n第三条生态学法则是“自然最懂得”。请举例说明人类破坏自然系统的案例。\n为什么地球常被称为“宇宙飞船”？\n如何定义“可持续性”？你生活中有哪些行为在长期看来是不可持续的？\n什么是“公地资源”？\n公地资源是否应该允许个人拥有或享有权利？\n你所在社区是否存在类似渔业资源的公地资源？它们面临哪些威胁？"
  },
  {
    "objectID": "bigdata/basics/rbasics.html",
    "href": "bigdata/basics/rbasics.html",
    "title": "R语言基础",
    "section": "",
    "text": "install.packages(\"foreign\")\nlibrary(foreign)\n\n\n\n\nR 有一个工作目录概念，会在工作目录下查找你要求它加载的文件，也会将你要求它保存的文件存放在这里。RStudio 会在控制台顶部显示你当前的工作目录。\n如果只是单个简单代码脚本，可以在 R 内部设置工作目录。如果有多个脚本和各种格式的文件，建议建立项目文件。\n\nsetwd(\"~/Downloads/rproject/presention\")\ngetwd()\n\n\n\n\n尽可能的用脚本记录所有的操作，即从原始数据到最终的结果。避免保存中间结果，或者将环境中的变量保存下来，而是用脚本去还原。\nRStudio 会在退出时自动保存脚本编辑器的内容，并在重新打开时自动重新加载。建议在退出前将脚本保存在合适的文件夹，避免使用“Untitled1”、“Untitled2”、“Untitled3”之类的名称，而是保存为可理解的文件名。\n文件名命名原则如下：\n\n文件名应易于机器识别：避免使用空格、符号和特殊字符。不要依赖大小写来区分文件。\n文件名应该是人类可读的：使用文件名来描述文件中的内容。\n文件名应该与默认运行顺序一致：文件名以数字开头，以便按字母顺序排列使用顺序。\n\n\n01-load-data.R\n02-exploratory-analysis.R\n03-model-approach-1.R\n04-model-approach-2.R\nfig-01.png\nfig-02.png\nreport-2022-03-20.qmd\nreport-2022-04-02.qmd\nreport-draft-notes.txt\n\n将与给定项目相关的所有文件（输入数据、R 脚本、分析结果和图表）放在一个目录中更为方便。RStudio 通过项目文件进行支持。单击“文件”&gt;“新建项目”，然后按照提示步骤操作。\n新建项目后，在脚本编辑器中输入命令保存文件，运行完整的脚本，RStudio会自动将新生成的 文件保存到项目文件夹。关闭编辑器后，可以通过项目文件夹中.Rproj文件重新打开项目，继续上次的工作。"
  },
  {
    "objectID": "bigdata/basics/rbasics.html#安装和使用包",
    "href": "bigdata/basics/rbasics.html#安装和使用包",
    "title": "R语言基础",
    "section": "",
    "text": "install.packages(\"foreign\")\nlibrary(foreign)"
  },
  {
    "objectID": "bigdata/basics/rbasics.html#设置工作目录",
    "href": "bigdata/basics/rbasics.html#设置工作目录",
    "title": "R语言基础",
    "section": "",
    "text": "R 有一个工作目录概念，会在工作目录下查找你要求它加载的文件，也会将你要求它保存的文件存放在这里。RStudio 会在控制台顶部显示你当前的工作目录。\n如果只是单个简单代码脚本，可以在 R 内部设置工作目录。如果有多个脚本和各种格式的文件，建议建立项目文件。\n\nsetwd(\"~/Downloads/rproject/presention\")\ngetwd()"
  },
  {
    "objectID": "bigdata/basics/rbasics.html#脚本和项目",
    "href": "bigdata/basics/rbasics.html#脚本和项目",
    "title": "R语言基础",
    "section": "",
    "text": "尽可能的用脚本记录所有的操作，即从原始数据到最终的结果。避免保存中间结果，或者将环境中的变量保存下来，而是用脚本去还原。\nRStudio 会在退出时自动保存脚本编辑器的内容，并在重新打开时自动重新加载。建议在退出前将脚本保存在合适的文件夹，避免使用“Untitled1”、“Untitled2”、“Untitled3”之类的名称，而是保存为可理解的文件名。\n文件名命名原则如下：\n\n文件名应易于机器识别：避免使用空格、符号和特殊字符。不要依赖大小写来区分文件。\n文件名应该是人类可读的：使用文件名来描述文件中的内容。\n文件名应该与默认运行顺序一致：文件名以数字开头，以便按字母顺序排列使用顺序。\n\n\n01-load-data.R\n02-exploratory-analysis.R\n03-model-approach-1.R\n04-model-approach-2.R\nfig-01.png\nfig-02.png\nreport-2022-03-20.qmd\nreport-2022-04-02.qmd\nreport-draft-notes.txt\n\n将与给定项目相关的所有文件（输入数据、R 脚本、分析结果和图表）放在一个目录中更为方便。RStudio 通过项目文件进行支持。单击“文件”&gt;“新建项目”，然后按照提示步骤操作。\n新建项目后，在脚本编辑器中输入命令保存文件，运行完整的脚本，RStudio会自动将新生成的 文件保存到项目文件夹。关闭编辑器后，可以通过项目文件夹中.Rproj文件重新打开项目，继续上次的工作。"
  },
  {
    "objectID": "bigdata/basics/rbasics.html#单个元素的对象整数小数字符串逻辑符号",
    "href": "bigdata/basics/rbasics.html#单个元素的对象整数小数字符串逻辑符号",
    "title": "R语言基础",
    "section": "3.1 单个元素的对象（整数、小数、字符串、逻辑符号）",
    "text": "3.1 单个元素的对象（整数、小数、字符串、逻辑符号）\n\ninteger_obj &lt;- 4 ## 整数\nnumeric_obj &lt;- 4.39053 ## 实数\nstring_obj &lt;- \"This is a string\" ## 字符串 \nlogical_obj &lt;- TRUE ## 逻辑符号"
  },
  {
    "objectID": "bigdata/basics/rbasics.html#多个元素组成的对象",
    "href": "bigdata/basics/rbasics.html#多个元素组成的对象",
    "title": "R语言基础",
    "section": "3.2 多个元素组成的对象",
    "text": "3.2 多个元素组成的对象\n\n向量\n\nvector_of_integers &lt;- c(3, 9, 10, 4) \nvector_of_integers\n\n[1]  3  9 10  4\n\nvector_of_numerics &lt;- rep(4.39024, 10) # rep函数赋值\nvector_of_numerics\n\n [1] 4.39024 4.39024 4.39024 4.39024 4.39024 4.39024 4.39024 4.39024 4.39024\n[10] 4.39024\n\nsequentialvec &lt;- 1:10 #冒号赋值\nbackwardssequence &lt;- 10:1\nseqvec2 &lt;- seq(0, 50, 10) #seq函数赋值\nsequentialvec  \n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nbackwardssequence\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\nseqvec2\n\n[1]  0 10 20 30 40 50\n\n\n\n\n向量的运算\n\nvector_of_integers - 2 * vector_of_integers \n\n[1]  -3  -9 -10  -4\n\nvector_of_integers\n\n[1]  3  9 10  4\n\n\n\n\n向量中元素的选择\n\nvector_of_integers\n\n[1]  3  9 10  4\n\nvector_of_integers[1] \n\n[1] 3\n\nvector_of_integers[2:4] \n\n[1]  9 10  4\n\n# 求平均值\nmean_of_vector_of_integers &lt;- mean(vector_of_integers)\nmean_of_vector_of_integers\n\n[1] 6.5\n\n\n练习：创建一个包含100到200之间所有整数的向量seq12，求其平均值，并赋值给mean12\n\nseq12 &lt;- seq(100,200,1)\nmean12 &lt;- mean(seq12)\nmean12\n\n[1] 150\n\n\n\n\n矩阵\n\n采用向量创建矩阵\n\n\nvector_of_integers\n\n[1]  3  9 10  4\n\ntwo_by_two_matrix &lt;- matrix(vector_of_integers, nrow=2, ncol=2) \ntwo_by_two_matrix\n\n     [,1] [,2]\n[1,]    3   10\n[2,]    9    4\n\n\n\n选择特定的矩阵元素\n\n\ntwo_by_two_matrix[1,2] \n\n[1] 10\n\ntwo_by_two_matrix[,2] \n\n[1] 10  4\n\ntwo_by_two_matrix[,1]\n\n[1] 3 9\n\n\n\n矩阵加法、矩阵按元素相乘、矩阵乘法\n\n\ntwo_by_two_matrix + two_by_two_matrix \n\n     [,1] [,2]\n[1,]    6   20\n[2,]   18    8\n\ntwo_by_two_matrix * two_by_two_matrix\n\n     [,1] [,2]\n[1,]    9  100\n[2,]   81   16\n\ntwo_by_two_matrix %*% two_by_two_matrix \n\n     [,1] [,2]\n[1,]   99   70\n[2,]   63  106"
  },
  {
    "objectID": "bigdata/basics/rbasics.html#读入数据生成数据框",
    "href": "bigdata/basics/rbasics.html#读入数据生成数据框",
    "title": "R语言基础",
    "section": "8.1 读入数据生成数据框",
    "text": "8.1 读入数据生成数据框\n\nhouses_data &lt;- read.table(file = \"houses.txt\", header = T, sep = \"\\t\")"
  },
  {
    "objectID": "bigdata/basics/rbasics.html#数据框的基础操作",
    "href": "bigdata/basics/rbasics.html#数据框的基础操作",
    "title": "R语言基础",
    "section": "8.2 数据框的基础操作",
    "text": "8.2 数据框的基础操作\n\nhead(houses_data)\n\n  PRICE SQFT AGE FEATS NE CUST COR  TAX\n1  2050 2650  13     7  1    1   0 1639\n2  2080 2600   *     4  1    1   0 1088\n3  2150 2664   6     5  1    1   0 1193\n4  2150 2921   3     6  1    1   0 1635\n5  1999 2580   4     4  1    1   0 1732\n6  1900 2580   4     4  1    0   0 1534\n\nhouses_data$PRICE[1:20]\n\n [1] 2050 2080 2150 2150 1999 1900 1800 1560 1450 1449 1375 1270 1250 1235 1170\n[16] 1180 1155 1110 1139  995\n\nhouses_data[houses_data$SQFT &gt; 2500,][1:5,]\n\n  PRICE SQFT AGE FEATS NE CUST COR  TAX\n1  2050 2650  13     7  1    1   0 1639\n2  2080 2600   *     4  1    1   0 1088\n3  2150 2664   6     5  1    1   0 1193\n4  2150 2921   3     6  1    1   0 1635\n5  1999 2580   4     4  1    1   0 1732\n\nnames(houses_data)\n\n[1] \"PRICE\" \"SQFT\"  \"AGE\"   \"FEATS\" \"NE\"    \"CUST\"  \"COR\"   \"TAX\"  \n\nhouses_data[1:10,1]\n\n [1] 2050 2080 2150 2150 1999 1900 1800 1560 1450 1449\n\nsq.ft.price &lt;- NULL # 创建一个对象\nfor(i in 1:10000){\n    sample.data &lt;- houses_data[sample(1:nrow(houses_data), 100),]\n    price.persqft &lt;- sample.data$PRICE / sample.data$SQFT # 建立代表单位面积价格的变量\n    sq.ft.price[i] &lt;- mean(price.persqft) # 将每个样本均值存入向量\n}\n\nhist(sq.ft.price)"
  },
  {
    "objectID": "bigdata/basics/rbasics.html#r-语言自带帮助系统",
    "href": "bigdata/basics/rbasics.html#r-语言自带帮助系统",
    "title": "R语言基础",
    "section": "10.1 R 语言自带帮助系统",
    "text": "10.1 R 语言自带帮助系统\n\n??mean\n?mean"
  },
  {
    "objectID": "bigdata/basics/rbasics.html#ai工具",
    "href": "bigdata/basics/rbasics.html#ai工具",
    "title": "R语言基础",
    "section": "10.2 AI工具",
    "text": "10.2 AI工具\n把想要实现的操作描述清楚或者直接把控制台反馈的错误信息贴入deepseek，一般能给出比较好的解决方案。此外，Rstudio的插件Copilot也能提供代码自动补齐和自动编写等功能。"
  },
  {
    "objectID": "bigdata/basics/rbasics.html#网上搜索",
    "href": "bigdata/basics/rbasics.html#网上搜索",
    "title": "R语言基础",
    "section": "10.3 网上搜索",
    "text": "10.3 网上搜索\n\nStack Overflow http://stackoverflow.com/questions/tagged/r\n拷贝报错信息，Google，一般都能查到解决问题的办法。"
  },
  {
    "objectID": "bigdata/basics/rbasics.html#命名",
    "href": "bigdata/basics/rbasics.html#命名",
    "title": "R语言基础",
    "section": "11.1 命名",
    "text": "11.1 命名\n对象名称必须以字母开头，并且只能包含字母、数字、_ 和.。对象名称应具有描述性，可以采用多词命名规范，推荐使用snake_case 格式，即用_ 分隔小写单词。在编写代码时，简短名称（a1，b2等）节省的时间相对较少（尤其是自动完成功能可以帮助完成输入），但调试较长的代码时，会难以理解和回忆起缩写代表的变量，需要反复确认，可能会更耗时。"
  },
  {
    "objectID": "bigdata/basics/rbasics.html#空格",
    "href": "bigdata/basics/rbasics.html#空格",
    "title": "R语言基础",
    "section": "11.2 空格",
    "text": "11.2 空格\n除幂符号^外，数学运算符两边（即 +，-，*，/，==，&lt;等）以及赋值运算符（&lt;-）周围都应加上空格。\n\n# 好的代码风格\nz &lt;- (a + b)^2 / d\n\n# 不好的代码风格\nz&lt;-( a + b ) ^ 2/d\n\n对于常规函数调用，不要在括号内外添加空格。逗号后始终添加空格，就像标准英语一样。\n\n# 好的代码风格\nmean(x, na.rm = TRUE)\n\n# 不好的代码风格\nmean (x ,na.rm=TRUE)\n\n如果能提高对齐效果，方便浏览代码，添加额外的空格让代码看起来更齐整也是可以的。\n\nflights |&gt; \n  mutate(\n    speed      = distance / air_time,\n    dep_hour   = dep_time %/% 100,\n    dep_minute = dep_time %%  100\n  )"
  },
  {
    "objectID": "bigdata/basics/rbasics.html#注释",
    "href": "bigdata/basics/rbasics.html#注释",
    "title": "R语言基础",
    "section": "11.3 注释",
    "text": "11.3 注释\n# 号后的内容会被R语言看做是文本注释，不会运行。添加注释可以帮助其他人理解代码，也帮助自己以后调试理解代码。注释的内容可以重点解释为什么这么做（例如，为什么要更改默认的参数等）。\n如果代码脚本很长，可以使用分段注释将文件分解为易于管理的部分。\n\n# 数据获取 --------------------------------------\n\n# 数据绘图 --------------------------------------\n\nRStudio 提供了键盘快捷键来创建这些标题（Cmd/Ctrl + Shift + R），并将它们显示在编辑器左下方的代码导航下拉菜单中。在脚本中添加分段注释后，可以使用脚本编辑器左下角的代码导航工具轻松导航到它们。"
  },
  {
    "objectID": "bigdata/basics/rbasics.html#从文件中读取数据",
    "href": "bigdata/basics/rbasics.html#从文件中读取数据",
    "title": "R语言基础",
    "section": "12.1 从文件中读取数据",
    "text": "12.1 从文件中读取数据\n常见的CSV逗号分隔值文件，第一行通常为标题行，包含列名，列之间使用逗号分隔。\nStudent ID,Full Name,favourite.food,mealPlan,AGE\n1,Sunil Huffmann,Strawberry yoghurt,Lunch only,4\n2,Barclay Lynn,French fries,Lunch only,5\n3,Jayendra Lyne,N/A,Breakfast and lunch,7\n4,Leon Rossini,Anchovies,Lunch only,\n5,Chidiegwu Dunkel,Pizza,Breakfast and lunch,five\n6,Güvenç Attila,Ice cream,Lunch only,6\n\n\n\n\n\n\n\n\n\n\nStudent ID\nFull Name\nfavourite.food\nmealPlan\nAGE\n\n\n\n\n1\nSunil Huffmann\nStrawberry yoghurt\nLunch only\n4\n\n\n2\nBarclay Lynn\nFrench fries\nLunch only\n5\n\n\n3\nJayendra Lyne\nN/A\nBreakfast and lunch\n7\n\n\n4\nLeon Rossini\nAnchovies\nLunch only\nNA\n\n\n5\nChidiegwu Dunkel\nPizza\nBreakfast and lunch\nfive\n\n\n6\nGüvenç Attila\nIce cream\nLunch only\n6\n\n\n\n使用 read_csv() 函数读入，参数是文件的路径。\n\nstudents &lt;- read_csv(\"students.csv\")\n\n运行后，会显示数据的行数和列数、使用的分隔符以及列规格（按列的数据类型组织的列名）。 读取数据后，第一步通常是进行一些转换，使其更易于在后续分析中使用。\n\nstudents\n\n# A tibble: 6 × 5\n  `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n2            2 Barclay Lynn     French fries       Lunch only          5    \n3            3 Jayendra Lyne    N/A                Breakfast and lunch 7    \n4            4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6            6 Güvenç Attila    Ice cream          Lunch only          6    \n\n\n检查数据，发现需要处理缺失值、重新命名变量、定义分类变量、修正变量值等。\n\n# 将字符字符串 N/A 识别为缺失值 NA\nstudents &lt;- read_csv(\"students.csv\", na = c(\"N/A\", \"\"))\n\n# 对不符合命名规范的变量名进行重命名\nstudents |&gt; \n  rename(\n    student_id = `Student ID`,\n    full_name = `Full Name`\n  )\n\n# A tibble: 6 × 5\n  student_id full_name        favourite.food     mealPlan            AGE  \n       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n2          2 Barclay Lynn     French fries       Lunch only          5    \n3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch 7    \n4          4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6          6 Güvenç Attila    Ice cream          Lunch only          6    \n\n# janitor包能将不规范的命名改变成规范命名\nstudents &lt;- students |&gt; janitor::clean_names()\n\n# 将变量转化为因子\nstudents &lt;- students |&gt;\n  mutate(meal_plan = factor(meal_plan))\n\n# 修正变量值\nstudents &lt;- students |&gt;\n  mutate(age = parse_number(if_else(age == \"five\", \"5\", age)))\n\nstudents\n\n# A tibble: 6 × 5\n  student_id full_name        favourite_food     meal_plan             age\n       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;fct&gt;               &lt;dbl&gt;\n1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n2          2 Barclay Lynn     French fries       Lunch only              5\n3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch     7\n4          4 Leon Rossini     Anchovies          Lunch only             NA\n5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n6          6 Güvenç Attila    Ice cream          Lunch only              6"
  },
  {
    "objectID": "bigdata/basics/rbasics.html#控制列类型",
    "href": "bigdata/basics/rbasics.html#控制列类型",
    "title": "R语言基础",
    "section": "12.2 控制列类型",
    "text": "12.2 控制列类型\nCSV文件不包含变量类型（例如逻辑型、数字型、字符串等）的信息，因此 readr 会尝试猜测类型。每一列抽取1000行的值，依次判断是否为逻辑性、数字型、日期型变量，如果都是不是，便为字符串。但是现实中，常常由于数据集不采用NA标注缺失值，采用其他字符标注（例如，.或者-等），导致数值型变量被判断为字符串。\n\nsimple_csv &lt;- \"\n  x\n  10\n  .\n  20\n  30\"\nread_csv(simple_csv)\n\n# A tibble: 4 × 1\n  x    \n  &lt;chr&gt;\n1 10   \n2 .    \n3 20   \n4 30   \n\n\n可以使用 col_types 参数来测试，如果报错，则采用problems() 了解更多信息。\n\ndf &lt;- read_csv(\n  simple_csv, \n  col_types = list(x = col_double())\n)\nproblems(df)\n\n# A tibble: 1 × 5\n    row   col expected actual file                                              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;                                             \n1     3     1 a double .      /private/var/folders/3r/j3j4w0015hd4fxnyycp4nf6m0…\n\n\n再设置新的缺失值 na = \".\"，得到符合要求的数字列。\n\nread_csv(simple_csv, na = \".\")\n\n# A tibble: 4 × 1\n      x\n  &lt;dbl&gt;\n1    10\n2    NA\n3    20\n4    30\n\n\nreadr 提供了九种列类型，包括 col_logical() 、col_double()、col_integer()、 col_character() 、 col_factor()、col_date() 和 col_datetime()，分别创建逻辑值、小数、整数、字符串、因子、日期和日期时间等列类型；col_number() 是宽松的数字解析器，会忽略非数字组件，可以表示货币，col_skip() 可以跳过某些数据列不读取。"
  },
  {
    "objectID": "bigdata/basics/rbasics.html#从多个文件中读取数据",
    "href": "bigdata/basics/rbasics.html#从多个文件中读取数据",
    "title": "R语言基础",
    "section": "12.3 从多个文件中读取数据",
    "text": "12.3 从多个文件中读取数据\n\nsales_files &lt;- c(\"data/01-sales.csv\", \"data/02-sales.csv\", \"data/03-sales.csv\")\n# `id` 参数会在结果数据框中添加一个名为 `file` 的新列，用于标识数据来自哪个文件。\nread_csv(sales_files, id = \"file\")\n\n# 可以使用基础函数 `list.files()` 通过匹配文件名中的模式来查找文件，避免逐一写文件名\nsales_files &lt;- list.files(\"data\", pattern = \"sales\\\\.csv$\", full.names = TRUE)\nsales_files"
  },
  {
    "objectID": "bigdata/basics/rbasics.html#写入文件",
    "href": "bigdata/basics/rbasics.html#写入文件",
    "title": "R语言基础",
    "section": "12.4 写入文件",
    "text": "12.4 写入文件\n\nwrite_csv(students, \"students.csv\")\n\n如果不希望列的数据结构信息消失，可以采用write_rds() 和 read_rds() 替代。\n\nwrite_rds(students, \"students.rds\")\nread_rds(\"students.rds\")"
  },
  {
    "objectID": "bigdata/basics/rbasics.html#监测代码的表现",
    "href": "bigdata/basics/rbasics.html#监测代码的表现",
    "title": "R语言基础",
    "section": "13.1 监测代码的表现",
    "text": "13.1 监测代码的表现\n在 R 中编写用于处理大数据的数据分析脚本时，建议先用少量子样本测试脚本的关键代码。为了快速识别潜在的瓶颈，可以使用一些 R 包来精确跟踪脚本各部分的处理时间以及内存占用。\n\n处理时间\n\n# 获取循环代码运行的时间（每次依照计算机的进程会略有不同）\nsystem.time(for (i in 1:100) {i + 5})\n\n 用户  系统  流逝 \n0.001 0.000 0.001 \n\nlibrary(microbenchmark)\n# 获取代码运行时间的统计信息\nmicrobenchmark(for (i in 1:100) {i + 5})\n\nUnit: microseconds\n                           expr     min       lq     mean  median       uq\n for (i in 1:100) {     i + 5 } 608.112 626.2955 698.4723 651.613 693.5765\n      max neval\n 4426.483   100\n\n\n\n\n内存占用\n\nhello &lt;- \"Hello, World!\"\n# 获取对象大小\nobject.size(hello)\n\n120 bytes\n\n# 创建字符串大向量\nlarge_string &lt;- rep(LETTERS[1:20], 1000^2)\nhead(large_string)\n\n[1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\"\n\n# 转换成因子\nlarge_factor &lt;- as.factor(large_string)\n\n# 比较字符串对象与因子对象内存占用的大小\nobject.size(large_string) - object.size(large_factor)\n\n79999456 bytes\n\n\n\n\n快速检测代码瓶颈\n\nlibrary(profvis)\n\n# 分析代码的表现\nprofvis({\n        x &lt;- 1:10000\n        z &lt;- 1.5\n\n# 方法一：采用循环向量元素依次相乘\nmultiplication &lt;- \n        function(x,z) {\n                result &lt;- c()\n                for (i in 1:length(x)) {result &lt;- c(result, x[i]*z)}\n                return(result)\n        }\nresult &lt;- multiplication(x,z)\n\n# 方法二：直接向量乘法\nresult2 &lt;- x * z \nhead(result2) \n})\n\n\n\n\n\n运行后，会用可视化的方式显示每行代码占用内存和运行时间的信息。"
  },
  {
    "objectID": "bigdata/basics/rbasics.html#预先分配足够内存",
    "href": "bigdata/basics/rbasics.html#预先分配足够内存",
    "title": "R语言基础",
    "section": "13.2 预先分配足够内存",
    "text": "13.2 预先分配足够内存\n对象初始化时，会在内存中的某个位置默认占用少量内存。一旦对象增长，如果没有足够的空间，需要将其移动到空间更大的位置。这种重新分配内存的操作，需要时间，会减慢整个进程的速度。因此，更有效的代码应该预先分配足够的内存。\n\n# 未预先分配足够内存\nsqrt_vector &lt;- \n     function(x) {\n          output &lt;- c()\n          for (i in 1:length(x)) {\n               output &lt;- c(output, x[i]^(1/2))\n          }\n          \n          return(output)\n     }\n\n# 预先分配足够内存\nsqrt_vector_faster &lt;- \n     function(x) {\n# 根据参数x的长度，预先分配output占用的内存\n          output &lt;- rep(NA, length(x))\n          for (i in 1:length(x)) {\n               output[i] &lt;-  x[i]^(1/2)\n          }\n          \n          return(output)\n     }\n\n两种代码的运行时间对比如图。"
  },
  {
    "objectID": "bigdata/basics/rbasics.html#使用向量化的r函数",
    "href": "bigdata/basics/rbasics.html#使用向量化的r函数",
    "title": "R语言基础",
    "section": "13.3 使用向量化的R函数",
    "text": "13.3 使用向量化的R函数\n许多 R 函数（例如数学运算符）都是向量化的，即操作直接作用于向量，从而利用向量中每个元素的相似性，从而可以增强运行效率。相对地，如果采用循环，R 必须在每次迭代中一遍又一遍地重复相同的准备步骤，会增加运行时间。\n\n# 向量化的运行，不使用循环迭代\nsqrt_vector_fastest &lt;- \n     function(x) {\n               output &lt;-  x^(1/2)\n          return(output)\n     }\n\n比较运行时间\n\n\n\n\n\n\n\n\n\n即使没有可用的向量化 R 基础函数，也建议不要编写简单的循环，尽量使用 apply 型的函数来避免内存分配问题。"
  },
  {
    "objectID": "bigdata/basics/rbasics.html#避免不必要的复制对象",
    "href": "bigdata/basics/rbasics.html#避免不必要的复制对象",
    "title": "R语言基础",
    "section": "13.4 避免不必要的复制对象",
    "text": "13.4 避免不必要的复制对象\nR 脚本对于复制对象（b &lt;- a）是将两个不同的名称（a和b）指向同一个存储空间（例如，一个非常大的文本向量）。但是，如果对某个名称的元素进行了改变（a[1] &lt;- 0），那么R会复制一个存储空间来区分两个名称所指的对象，相当于占用空间扩大一倍。\n\na &lt;- runif(10000)\nb &lt;- a\n\nlibrary(lobstr)\n# 检查两个名称对象的地址\nobj_addr(a)\n\n[1] \"0x122ef8000\"\n\nobj_addr(b)\n\n[1] \"0x122ef8000\"\n\na[1] &lt;- 0\n\nobj_addr(a)\n\n[1] \"0x3100d8000\"\n\nobj_addr(b)\n\n[1] \"0x122ef8000\""
  },
  {
    "objectID": "bigdata/basics/rbasics.html#及时释放内存",
    "href": "bigdata/basics/rbasics.html#及时释放内存",
    "title": "R语言基础",
    "section": "13.5 及时释放内存",
    "text": "13.5 及时释放内存\n如果在程序运行过程中，不再需要某个较大的中间数据对象，可以通过 rm() 函数及时删除，释放紧张的内存。\n\nlibrary(pryr)\nmem_change(large_vector &lt;- runif(10^8))\n\n800 MB\n\nmem_change(rm(large_vector))\n\n-800 MB"
  },
  {
    "objectID": "bigdata/basics/rbasics.html#必要时使用低级语言",
    "href": "bigdata/basics/rbasics.html#必要时使用低级语言",
    "title": "R语言基础",
    "section": "13.5 必要时使用低级语言",
    "text": "13.5 必要时使用低级语言\nR 是一种解释型语言，运行代码时由解释器逐条将代码翻译成机器码运行。而低级的编译型语言是将代码整体编译成机器码，然后运行。运行已编译的代码比运行必须先逐条解释的代码快得多。事实上，R 的一些核心函数都是用低级编程语言实现的，调用的 R 函数只是与这些函数进行交互，例如 sum 函数等。必要时可以通过R语言调用低级语言编写的代码（例如data.table包），增加运行的速度。\n\n参考书籍\n\nHadley Wickham, Mine Cetinkaya-Rundel, Garrett Grolemund. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data, (2nd edition)，O’Reilly Media, 2023.\n\nKabacoff （王小宁等译） R语言实战（第3版），人民邮电出版社，2023.\n\nUlrich Matter Big Data Analytics: A Guide to Data Science Practitioners Making the Transition to Big Data, CRC Press, 2024."
  },
  {
    "objectID": "bigdata/machlearn/mlbasics.html",
    "href": "bigdata/machlearn/mlbasics.html",
    "title": "机器学习基础",
    "section": "",
    "text": "机器学习（ML）在公共管理和公共政策领域的重要性日益凸显，在治理实践中也有越来越多的应用场景。以下是几个具体的例子：\n\n社会救助：利用机器学习预测某些低保或临时救助家庭在未来30天内再次申请救助的可能性，从而帮助民政部门优化资源配置，提前介入。\n医疗保障：通过历史医保数据预测患者在出院后30天内再次入院的风险，以便医保局和社区卫生服务中心提前安排随访和健康管理。\n环境治理：根据垃圾投放频率、社区特征等数据，预测垃圾分类政策的居民参与率，帮助城市管理部门有针对性地开展宣传和监管。\n基层治理：将居民按照人口学特征、公共服务需求和社区参与行为进行分群，辅助街道和社区制定更精准的公共服务供给方案。\n公共服务流失预警：预测老年人在养老机构或社区养老服务中的“流失率”，便于民政和养老机构提前采取关怀或服务改进措施，减少老年人脱离公共支持体系。\n\n本质上，这些任务都在于从数据中学习。针对不同问题，我们都可以用一组特征数据来训练算法，进而提取出有价值的治理建议。\n机器学习算法根据训练过程中所需的监督类型分为两类：\n\n监督学习：构建预测模型（如预测再申请救助、预测再入院风险）。\n无监督学习：构建描述模型（如对居民群体进行分群分析）。\n\n至于选择哪种类型的学习方法，则取决于你想要解决的治理任务。\n\n\n预测模型用于那些需要利用数据集中的其他变量（或特征）来预测某个给定输出（或目标）的任务。预测模型中的学习算法试图发现并建模目标变量（即被预测的变量）与其他特征（即预测变量）之间的关系。\n在公共管理和公共政策中的预测建模包括：\n\n根据家庭收入、就业状况等特征，预测某个家庭未来6周内申请最低生活保障的可能性；\n根据社区人口结构、基础设施条件等特征，预测该社区房价或公共住房需求；\n根据公务员个人背景、工作压力和岗位信息，预测其离职或流动的可能性；\n根据居民健康档案和症状数据，预测其在社区医疗机构中再次就诊或住院的风险；\n根据地方财政支出结构和政策实施条件，预测某项公共政策（如农村教育项目）的实施周期或效果达成时间。\n\n这些例子都有明确的学习任务：它们都试图利用特征变量（X）来预测一个结果指标（Y）。\n在机器学习领域经常交替使用以下术语：\n\nX：预测变量、自变量、属性、特征、预测因子\nY：目标变量、因变量、响应、结果指标\n\n上述预测建模的例子属于监督学习。监督的含义在于：目标值在训练过程中起到“监督”作用，指示学习算法需要学习的任务。具体来说，给定一组数据，学习算法会尝试优化一个函数（即算法步骤），以找到特征值的组合，使预测值尽可能接近真实的目标输出。\n在监督学习中，训练数据包含了目标值，因此这些目标值可以帮助“监督”训练过程，找到最优的算法参数。\n大多数监督学习问题可归为两类：回归问题或分类问题。\n\n\n当监督学习目标是预测一个数值型结果时，这类任务被称为回归问题（不要与线性回归建模混淆）。回归问题主要关注预测一个落在连续区间上的输出值。\n在公共管理的情境下，典型的回归问题包括：\n\n根据人口规模、财政投入和教师数量，预测某地区的教育资源人均供给水平；\n根据历史财政支出和经济发展水平，预测某城市的下一年度公共预算支出总额；\n根据居民收入、医保报销比例和就医习惯，预测一个地区的医疗费用平均水平；\n根据城市基础设施条件与人口密度，预测公共交通客流量。\n\n这意味着，在给定一组预测变量的情况下，响应值可能落在某个连续范围内。比如，预测某市的公共预算可能在 100亿元至300亿元 之间。\n\n\n\n当监督学习目标是预测一个类别型结果时，这类任务被称为分类问题。分类问题通常涉及预测一个二分类或多分类的响应变量，例如：\n\n预测某家庭是否会再次申请低保（是/否）；\n预测某居民是否会参加垃圾分类活动（参与/不参与）；\n预测某地区某项公共政策是否能够如期完成（完成/未完成）；\n对市民的政策满意度进行分类：\n\n二分类：满意 vs. 不满意；\n多分类：非常不满意、不满意、一般、满意、非常满意。\n\n\n在机器学习模型中，分类问题不仅仅是预测一个固定类别（如“是”或“否”），我们往往希望预测的是某个类别的概率（例如：居民参加垃圾分类的概率为 0.65，不参加的概率为 0.35）。通常情况下，概率最高的类别会被作为最终预测结果。因此，即便分类问题的本质是预测类别，但其输出仍然是一个数值（概率）。\n虽然确实存在一些只能用于回归问题或只能用于分类问题的机器学习算法，但大多数常用的监督学习算法都可以应用于两者。这些算法也正是近年来公共管理和公共政策分析中最常见的机器学习方法。\n\n\n\n\n与监督学习相对，无监督学习是指一套理解和描述数据的统计工具，其分析过程没有目标变量。本质上，无监督学习关注的是在数据集中识别群体。这些群体可能是基于行（即聚类）或列（即降维）而定义的，但两者的动机有所不同。\n聚类的目标是基于观测到的变量，将观测对象划分为相似的群体。例如，在公共管理中，可以将城市居民划分为不同的同质群体，以便政府制定差异化的公共服务供给方案。\n在降维中，更关注于减少数据集中的变量数量。例如，在社会治理数据中，不同指标可能高度相关（如收入水平、消费能力与住房条件），经典的回归模型在这种情况下会出现问题。一些降维技术可以将变量集合简化为一个更小的不相关变量集，这个新的变量集常常可以作为后续监督学习模型的输入（如主成分回归）。\n无监督学习常常作为 探索性数据分析（EDA）的一部分使用。然而，这种分析更具主观性，因为它不像监督学习那样有一个明确的目标（如预测一个结果）。同时，也很难评估无监督学习结果的质量。原因很简单：在监督学习中（比如线性回归），可以通过检验模型在未使用过的数据上的预测效果来验证结果。但在无监督学习中，并不知道“正确答案”为何，因此也就无法直接验证结果的优劣。\n尽管具有主观性，但无监督学习的重要性不容忽视，它常被用于政府治理和政策分析，例如：\n\n社会服务分群：将居民划分为不同群体（如低收入群体、独居老人群体、新就业人群），以便制定更精准的社会救助或养老服务政策。\n公共健康管理：识别具有相似健康状况和就诊行为的居民群体，从而为特定人群制定差异化的健康干预措施（如糖尿病高风险人群的专项随访）。\n基层治理分类：根据居民的参与行为和诉求模式，将社区居民分为积极参与群体、被动接受群体和边缘群体，以帮助街道和社区设计更有效的治理方式。\n政策指标降维：在涉及大量经济、社会、环境指标的可持续发展评价中，利用降维方法提取几个核心综合指标，为后续政策评估和建模提供更简化的数据输入。\n\n这些问题，以及更多类似的场景，都可以通过无监督学习来解决。此外，无监督学习模型的输出结果往往还能作为输入，供后续的监督学习模型使用。"
  },
  {
    "objectID": "bigdata/machlearn/mlbasics.html#监督学习",
    "href": "bigdata/machlearn/mlbasics.html#监督学习",
    "title": "机器学习基础",
    "section": "",
    "text": "预测模型用于那些需要利用数据集中的其他变量（或特征）来预测某个给定输出（或目标）的任务。预测模型中的学习算法试图发现并建模目标变量（即被预测的变量）与其他特征（即预测变量）之间的关系。\n在公共管理和公共政策中的预测建模包括：\n\n根据家庭收入、就业状况等特征，预测某个家庭未来6周内申请最低生活保障的可能性；\n根据社区人口结构、基础设施条件等特征，预测该社区房价或公共住房需求；\n根据公务员个人背景、工作压力和岗位信息，预测其离职或流动的可能性；\n根据居民健康档案和症状数据，预测其在社区医疗机构中再次就诊或住院的风险；\n根据地方财政支出结构和政策实施条件，预测某项公共政策（如农村教育项目）的实施周期或效果达成时间。\n\n这些例子都有明确的学习任务：它们都试图利用特征变量（X）来预测一个结果指标（Y）。\n在机器学习领域经常交替使用以下术语：\n\nX：预测变量、自变量、属性、特征、预测因子\nY：目标变量、因变量、响应、结果指标\n\n上述预测建模的例子属于监督学习。监督的含义在于：目标值在训练过程中起到“监督”作用，指示学习算法需要学习的任务。具体来说，给定一组数据，学习算法会尝试优化一个函数（即算法步骤），以找到特征值的组合，使预测值尽可能接近真实的目标输出。\n在监督学习中，训练数据包含了目标值，因此这些目标值可以帮助“监督”训练过程，找到最优的算法参数。\n大多数监督学习问题可归为两类：回归问题或分类问题。\n\n\n当监督学习目标是预测一个数值型结果时，这类任务被称为回归问题（不要与线性回归建模混淆）。回归问题主要关注预测一个落在连续区间上的输出值。\n在公共管理的情境下，典型的回归问题包括：\n\n根据人口规模、财政投入和教师数量，预测某地区的教育资源人均供给水平；\n根据历史财政支出和经济发展水平，预测某城市的下一年度公共预算支出总额；\n根据居民收入、医保报销比例和就医习惯，预测一个地区的医疗费用平均水平；\n根据城市基础设施条件与人口密度，预测公共交通客流量。\n\n这意味着，在给定一组预测变量的情况下，响应值可能落在某个连续范围内。比如，预测某市的公共预算可能在 100亿元至300亿元 之间。\n\n\n\n当监督学习目标是预测一个类别型结果时，这类任务被称为分类问题。分类问题通常涉及预测一个二分类或多分类的响应变量，例如：\n\n预测某家庭是否会再次申请低保（是/否）；\n预测某居民是否会参加垃圾分类活动（参与/不参与）；\n预测某地区某项公共政策是否能够如期完成（完成/未完成）；\n对市民的政策满意度进行分类：\n\n二分类：满意 vs. 不满意；\n多分类：非常不满意、不满意、一般、满意、非常满意。\n\n\n在机器学习模型中，分类问题不仅仅是预测一个固定类别（如“是”或“否”），我们往往希望预测的是某个类别的概率（例如：居民参加垃圾分类的概率为 0.65，不参加的概率为 0.35）。通常情况下，概率最高的类别会被作为最终预测结果。因此，即便分类问题的本质是预测类别，但其输出仍然是一个数值（概率）。\n虽然确实存在一些只能用于回归问题或只能用于分类问题的机器学习算法，但大多数常用的监督学习算法都可以应用于两者。这些算法也正是近年来公共管理和公共政策分析中最常见的机器学习方法。"
  },
  {
    "objectID": "bigdata/machlearn/mlbasics.html#无监督学习",
    "href": "bigdata/machlearn/mlbasics.html#无监督学习",
    "title": "机器学习基础",
    "section": "",
    "text": "与监督学习相对，无监督学习是指一套理解和描述数据的统计工具，其分析过程没有目标变量。本质上，无监督学习关注的是在数据集中识别群体。这些群体可能是基于行（即聚类）或列（即降维）而定义的，但两者的动机有所不同。\n聚类的目标是基于观测到的变量，将观测对象划分为相似的群体。例如，在公共管理中，可以将城市居民划分为不同的同质群体，以便政府制定差异化的公共服务供给方案。\n在降维中，更关注于减少数据集中的变量数量。例如，在社会治理数据中，不同指标可能高度相关（如收入水平、消费能力与住房条件），经典的回归模型在这种情况下会出现问题。一些降维技术可以将变量集合简化为一个更小的不相关变量集，这个新的变量集常常可以作为后续监督学习模型的输入（如主成分回归）。\n无监督学习常常作为 探索性数据分析（EDA）的一部分使用。然而，这种分析更具主观性，因为它不像监督学习那样有一个明确的目标（如预测一个结果）。同时，也很难评估无监督学习结果的质量。原因很简单：在监督学习中（比如线性回归），可以通过检验模型在未使用过的数据上的预测效果来验证结果。但在无监督学习中，并不知道“正确答案”为何，因此也就无法直接验证结果的优劣。\n尽管具有主观性，但无监督学习的重要性不容忽视，它常被用于政府治理和政策分析，例如：\n\n社会服务分群：将居民划分为不同群体（如低收入群体、独居老人群体、新就业人群），以便制定更精准的社会救助或养老服务政策。\n公共健康管理：识别具有相似健康状况和就诊行为的居民群体，从而为特定人群制定差异化的健康干预措施（如糖尿病高风险人群的专项随访）。\n基层治理分类：根据居民的参与行为和诉求模式，将社区居民分为积极参与群体、被动接受群体和边缘群体，以帮助街道和社区设计更有效的治理方式。\n政策指标降维：在涉及大量经济、社会、环境指标的可持续发展评价中，利用降维方法提取几个核心综合指标，为后续政策评估和建模提供更简化的数据输入。\n\n这些问题，以及更多类似的场景，都可以通过无监督学习来解决。此外，无监督学习模型的输出结果往往还能作为输入，供后续的监督学习模型使用。"
  },
  {
    "objectID": "bigdata/machlearn/mlbasics.html#工具与数据",
    "href": "bigdata/machlearn/mlbasics.html#工具与数据",
    "title": "机器学习基础",
    "section": "2.1 工具与数据",
    "text": "2.1 工具与数据\n本部分需使用以下R语言包：\n\n# 数据来源\nlibrary(modeldata)\n# 数据处理\nlibrary(dplyr)     \nlibrary(ggplot2)   # 可视化\n\n# 建模过程\nlibrary(rsample)   # 数据重采样\nlibrary(caret)     # 重采样和模型训练\nlibrary(h2o)       # 分布式建模工具\n\n# h2o 设置\nh2o.no_progress()\nh2o.init()\n##  Connection successful!\n## \n## R is connected to the H2O cluster: \n##     H2O cluster uptime:         17 days 6 hours \n##     H2O cluster timezone:       Asia/Shanghai \n##     H2O data parsing timezone:  UTC \n##     H2O cluster version:        3.44.0.3 \n##     H2O cluster version age:    1 year, 8 months and 19 days \n##     H2O cluster name:           H2O_started_from_R_liangdan_iua658 \n##     H2O cluster total nodes:    1 \n##     H2O cluster total memory:   3.58 GB \n##     H2O cluster total cores:    10 \n##     H2O cluster allowed cores:  10 \n##     H2O cluster healthy:        TRUE \n##     H2O Connection ip:          localhost \n##     H2O Connection port:        54321 \n##     H2O Connection proxy:       NA \n##     H2O Internal Security:      FALSE \n##     R Version:                  R version 4.4.2 (2024-10-31)\n\n本部分使用的示例数据如下：\n房产销售信息\n\n问题类型：有监督回归\n响应变量：Sale_Price（如：$195,000，$215,000）\n特征数量：80\n观测值数量：2,930\n研究目标：利用房产属性预测房屋的销售价格\n数据获取：由 AmesHousing 包提供\n\n员工离职信息（最初由 IBM Watson Analytics Lab 提供）\n\n问题类型：有监督二分类\n响应变量：Attrition（即：“Yes”，“No”）\n特征数量：30\n观测值数量：1,470\n研究目标：利用员工属性预测其是否会离职\n数据获取：由 rsample 包提供\n\nH2O对象能够调用 H2O 提供的分布式机器学习平台，以便更高效地进行建模与分析。尽管示例的数据量并不大，但是实践中如果针对大数据进行建模和分析，往往需要更有效率的方案。\n\n# 房屋销售数据\names &lt;- AmesHousing::make_ames()\names.h2o &lt;- as.h2o(ames)\n\n# 员工离职数据\nchurn &lt;- modeldata::attrition %&gt;% \n# 因为h2o对象不能处理有序变量，要先进行转化成无序的因子\n    mutate_if(is.ordered, .funs = factor, ordered = FALSE)\nchurn.h2o &lt;- as.h2o(churn)"
  },
  {
    "objectID": "bigdata/machlearn/mlbasics.html#数据划分",
    "href": "bigdata/machlearn/mlbasics.html#数据划分",
    "title": "机器学习基础",
    "section": "2.2 数据划分",
    "text": "2.2 数据划分\n机器学习的主要目标是找到一个算法 \\(f(X)\\) 能够基于输入特征 \\(X\\) 准确预测未来的目标变量 \\(\\hat{Y}\\)。这不仅要求模型在历史数据上拟合良好，更重要的是在未来数据上的预测能力，即模型的泛化能力。\n为了客观评估模型的泛化性能，我们通常将数据集划分为训练集与测试集：\n\n训练集：用于特征选择、模型训练、超参数调优和模型比较。\n测试集：在最终确定模型后，用于评估模型的泛化误差。\n\n注意：在最终模型确定之前，不能提前使用测试集，否则会导致结果偏倚。\n划分比例常见有 60%–40%，70%–30%，或 80%–20%。一般规律是：\n\n训练数据过多（如 &gt;80%）可能导致过拟合。\n测试数据过多（如 &gt;40%）可能影响参数稳定性。\n如果样本量极大（如 &gt;100,000），则可以适当减少训练样本，以提高计算效率。\n在特征数量接近或超过样本数量时（p ≥ n），需要更大的训练样本以识别稳定信号。\n\n\n简单随机抽样\n最常见的方法是简单随机抽样。例如，若要对房产销售数据进行 70%–30% 的训练/测试划分，可以使用以下几种方式：\n\n# 采用 base R\nset.seed(123)  \nindex_1 &lt;- sample(1:nrow(ames), round(nrow(ames) * 0.7))\ntrain_1 &lt;- ames[index_1, ]\ntest_1  &lt;- ames[-index_1, ]\n\n# 采用 caret 包\nset.seed(123)  \nindex_2 &lt;- createDataPartition(ames$Sale_Price, p = 0.7, \n                               list = FALSE)\ntrain_2 &lt;- ames[index_2, ]\ntest_2  &lt;- ames[-index_2, ]\n\n# 采用 rsample 包\nset.seed(123)  \nsplit_1  &lt;- initial_split(ames, prop = 0.7)\ntrain_3  &lt;- training(split_1)\ntest_3   &lt;- testing(split_1)\n\n# 采用 h2o 包\nsplit_2 &lt;- h2o.splitFrame(ames.h2o, ratios = 0.7, \n                          seed = 123)\ntrain_4 &lt;- split_2[[1]]\ntest_4  &lt;- split_2[[2]]\n\n\n\n\n\n\n训练数据（黑）vs.测试数据（红）中响应变量的分布\n\n\n\n\n\n\n分层抽样\n如果希望明确控制抽样，使训练集和测试集的Y的分布保持相似，就可以使用分层抽样（stratified sampling）。这在分类问题中更为常见，尤其是当响应变量严重不平衡时（例如：90% 的观测值响应为 “Yes”，10% 的观测值响应为 “No”）。类似地，对于一些样本量较小、且响应变量明显偏离正态分布（如 Sale_Price 呈正偏分布）的回归问题，也可以使用分层抽样。\n在公共政策场景中，很多目标变量是不平衡的。例如：\n\n社会救助申请数据中，可能约80%的家庭“不符合条件”，只有20%“符合条件”。\n公共安全风险预测中，恶性事件比例往往远小于一般事件。\n\n在连续型响应变量的情境下，分层抽样会将Y划分为若干分位区间（quantiles），并在每个区间内进行随机抽样。这样能够保证训练集和测试集中响应变量分布的代表性更加均衡。\n执行分层抽样的最简单方法是使用rsample包，在其中指定要进行分层的响应变量。例如，在原始的员工离职数据中，响应分布存在不平衡（No：84%，Yes：16%）。通过强制使用分层抽样，训练集和测试集都能保持近似相同的响应分布。\n\n# 原始响应分布\ntable(churn$Attrition) %&gt;% prop.table()\n## \n##        No       Yes \n## 0.8387755 0.1612245\n## \n##        No       Yes \n## 0.8387755 0.1612245\n\n# 使用 rsample 包进行分层抽样，指定分层变量Attrition\nset.seed(123)\nsplit_strat  &lt;- initial_split(churn, prop = 0.7, \n                              strata = \"Attrition\")\ntrain_strat  &lt;- training(split_strat)\ntest_strat   &lt;- testing(split_strat)\n\n# 训练集与测试集中的响应比例保持一致\ntable(train_strat$Attrition) %&gt;% prop.table()\n## \n##        No       Yes \n## 0.8394942 0.1605058\n## \n##       No      Yes \n## 0.838835 0.161165\ntable(test_strat$Attrition) %&gt;% prop.table()\n## \n##        No       Yes \n## 0.8371041 0.1628959\n## \n##        No       Yes \n## 0.8386364 0.1613636\n\n\n\n类别不平衡\n类别不平衡是公共管理领域的常见问题，例如：\n\n城市中极少数家庭享受住房困难补贴（&lt;10%）。\n公共安全事件（如火灾、事故）的发生率远低于“无事故”的样本。\n\n类别不平衡的数据会显著影响模型的预测和性能。可以采用欠采样（down-sampling）和过采样（up-sampling）解决。\n欠采样（down-sampling）通过减少多数类样本数量，使其与少数类样本数量相当。适用于数据量充足的情况，方法是保留所有少数类样本，并在多数类样本中随机抽取等量样本，形成一个新的平衡数据集。优点是数据规模缩小，后续建模的计算负担减少。\n过采样（up-sampling）适用于数据量不足的情况。方法是增加少数类样本的数量，而不是丢弃多数类样本。具体做法包括重复采样或自助法（bootstrap）。\n两种方法并无绝对优劣，应用哪种方法取决于具体研究场景和数据集特征。在实践中，过采样与欠采样结合使用往往效果更佳。一种常用的混合方法是 SMOTE（Synthetic Minority Over-Sampling Technique，合成少数类过采样技术）。在 R语言中，可以通过参数实现这些方法（例如：caret::trainControl() 中的 sampling 参数）。此外，许多 R 中的机器学习算法也内置了类别加权机制以应对类别不平衡（例如，大多数 h2o 算法都支持 weights_column 和 balance_classes 参数）。"
  },
  {
    "objectID": "bigdata/machlearn/mlbasics.html#创建模型",
    "href": "bigdata/machlearn/mlbasics.html#创建模型",
    "title": "机器学习基础",
    "section": "2.3 创建模型",
    "text": "2.3 创建模型\nR 生态系统提供了各种各样的机器学习（ML）算法实现，不同算法在公式定义方式上不一致，输出结果和预测值的获取方式也存在差异。\n\n公式接口\n\n公式接口（formula interface） 使用 R 的公式规则来指定符号化的模型表示。例如：Y ~ X，表示“Y 是 X 的函数”。\n\n\n# 销售价格作为社区和出售年份的函数\nmodel_fn(Sale_Price ~ Neighborhood + Year_Sold, data = ames)\n\n# 加入变量交互项\nmodel_fn(Sale_Price ~ Neighborhood + Year_Sold + \n           Neighborhood:Year_Sold, data = ames)\n\n# 使用所有预测变量的简写方式\nmodel_fn(Sale_Price ~ ., data = ames)\n\n# 行内函数和变量转换\nmodel_fn(log10(Sale_Price) ~ ns(Longitude, df = 3) + \n           ns(Latitude, df = 3), data = ames)\n\n\nXY 接口（non-formula interface） 通过分别指定自变量（X）和因变量（Y）来建模：\n\n\n# 分开提供 X 和 Y\nfeatures &lt;- c(\"Year_Sold\", \"Longitude\", \"Latitude\")\nmodel_fn(x = ames[, features], y = ames$Sale_Price)\n\n这种方式在计算效率上更高，但如果需要在建模前进行转换、哑变量处理或交互项操作，就显得不太方便。\n\n变量名接口（variable name specification） 提供一个训练数据框，然后通过字符串指定响应变量和特征变量。这是 h2o 包所采用的方式：\n\n\nmodel_fn(\n  x = c(\"Year_Sold\", \"Longitude\", \"Latitude\"),\n  y = \"Sale_Price\",\n  data = ames.h2o\n)\n\n\n\n引擎\n虽然 R 中有许多独立的机器学习包，但也存在一些 元引擎（meta engines），它们能够在接口上提供一致性。\n例如，以下三种方法都能得到相同的线性回归模型结果：\n\nlm_lm    &lt;- lm(Sale_Price ~ ., data = ames)\nlm_glm   &lt;- glm(Sale_Price ~ ., data = ames, family = gaussian)\nlm_caret &lt;- train(Sale_Price ~ ., data = ames, method = \"lm\")\n\n\nlm() 和 glm() 是两种不同的算法引擎，用于拟合线性模型；\ncaret::train() 是一种 元引擎，它可以通过 method = \"&lt;方法名&gt;\" 来调用几乎任何直接算法引擎。\n\n选择 直接引擎 还是 元引擎 有取舍：\n\n直接引擎 灵活度高，但需要熟悉每个实现的语法差异；\n元引擎 提供一致的接口和输出提取方式，但灵活性可能略差。"
  },
  {
    "objectID": "bigdata/machlearn/mlbasics.html#重采样方法",
    "href": "bigdata/machlearn/mlbasics.html#重采样方法",
    "title": "机器学习基础",
    "section": "2.4 重采样方法",
    "text": "2.4 重采样方法\n基于训练数据计算误差指标评价模型表现（例如通过R方评价线性回归的拟合优度），会出现过度拟合。使用验证方法，即将训练集再次分为一个训练集和一个验证集（或留出集），在验证集上估计模型性能，会由于单一验证集导致高度不稳定且不可靠。\n注意区分 测试集、验证集和留出集。测试集是直到最终模型被选定才使用的数据，验证集是在模型训练过程中使用的，但是，不同文献叫法不同，非常容易混淆。\n重采样方法 提供了一种替代方法，它允许反复在训练数据的不同部分上拟合模型，并在其他部分上测试其性能。最常用的两种重采样方法是 k 折交叉验证 和 自助法（bootstrapping）。\n\nk 折交叉验证\nk 折交叉验证（k-fold CV）是一种重采样方法，它将训练数据随机划分为 k 组（即“折”），每组大小大致相等。模型在k-1个折上进行训练，并在剩余的折上计算模型性能。这个过程重复 k 次，每次使用不同的折作为验证集。最终得到 k 个泛化误差估计（例如 的平均值，即 k 折交叉验证的误差估计，近似表示了模型在未知数据上的表现。\n实践中，通常使用 k=5 或 k=10 。k 的大小没有规定，一般 k 越大，估计的性能与测试集获得的真实性能之间的差异会越减。然而，k 过大会带来计算负担。Molinaro等 (2005) 发现 k=10 的表现与 留一交叉验证（LOOCV, k=n） 类似。\n\n\n\n\n\n\n\n\n\nk折交叉验证依然有波动性问题，对于较小的数据集（例如n &lt; 10,000），使用 10 折交叉验证重复 5 或 10 次 能够提高估计的准确性，并提供误差波动性的估计。\n\n# 使用 h2o 的示例\nh2o.cv &lt;- h2o.glm(\n  x = x, \n  y = y, \n  training_frame = ames.h2o,\n  nfolds = 10  # 执行 10 折交叉验证\n)\n\n\nvfold_cv(ames, v = 10)\n## #  10-fold cross-validation \n## # A tibble: 10 × 2\n##    splits             id    \n##    &lt;list&gt;             &lt;chr&gt; \n##  1 &lt;split [2637/293]&gt; Fold01\n##  2 &lt;split [2637/293]&gt; Fold02\n##  3 &lt;split [2637/293]&gt; Fold03\n##  4 &lt;split [2637/293]&gt; Fold04\n##  5 &lt;split [2637/293]&gt; Fold05\n##  6 &lt;split [2637/293]&gt; Fold06\n##  7 &lt;split [2637/293]&gt; Fold07\n##  8 &lt;split [2637/293]&gt; Fold08\n##  9 &lt;split [2637/293]&gt; Fold09\n## 10 &lt;split [2637/293]&gt; Fold10\n\n\n\n自助法（Bootstrapping）\n自助采样（Bootstrap sample） 是从原始数据集中 有放回地 随机抽样得到的子集。自助样本的大小与原始数据集相同。由于采样是有放回的，每个自助样本中可能包含重复的值。未被包含的观测点称为 袋外数据（OOB, out-of-bag）。在自助法中，用采样子集训练模型，再用袋外数据验证模型性能（例如，随机森林（random forest）。\n\nbootstraps(ames, times = 10)\n## # Bootstrap sampling \n## # A tibble: 10 × 2\n##    splits              id         \n##    &lt;list&gt;              &lt;chr&gt;      \n##  1 &lt;split [2930/1056]&gt; Bootstrap01\n##  2 &lt;split [2930/1066]&gt; Bootstrap02\n##  3 &lt;split [2930/1062]&gt; Bootstrap03\n##  4 &lt;split [2930/1048]&gt; Bootstrap04\n##  5 &lt;split [2930/1088]&gt; Bootstrap05\n##  6 &lt;split [2930/1108]&gt; Bootstrap06\n##  7 &lt;split [2930/1075]&gt; Bootstrap07\n##  8 &lt;split [2930/1083]&gt; Bootstrap08\n##  9 &lt;split [2930/1049]&gt; Bootstrap09\n## 10 &lt;split [2930/1064]&gt; Bootstrap10"
  },
  {
    "objectID": "bigdata/machlearn/mlbasics.html#偏差方差权衡",
    "href": "bigdata/machlearn/mlbasics.html#偏差方差权衡",
    "title": "机器学习基础",
    "section": "2.5 偏差方差权衡",
    "text": "2.5 偏差方差权衡\n预测误差可以分解为偏差和方差两部分。模型在最小化偏差与方差的能力之间往往存在权衡。\n\n偏差（Bias）\n偏差是指模型的期望（或平均）预测值与我们要预测的真实值之间的差异。它衡量的是一个模型的预测在整体上距离真实值有多远，反映了模型对数据潜在结构的刻画能力。下图展示了一个多项式模型无法很好捕捉潜在结构的（高偏差），但是高偏差模型通常不会受到重采样所引入噪声的明显影响，生成了25个相似但依旧有偏差的预测。\n\n\n\n\n\n\n\n\n\n\n\n方差（Variance）\n由方差引起的误差是指某个模型对同一数据点预测的波动程度。许多模型（例如 k 最近邻、决策树、梯度提升机）都具有很强的适应性和灵活性，能够拟合复杂的模式，但这种灵活性也带来了过拟合训练数据的风险。下图展示高方差的k最近邻模型在单一数据集上能够较好捕捉潜在的非线性、非单调数据结构，但在 25 个自助法样本上拟合的模型受到噪声影响，预测表现高度不稳定。\n\n\n\n\n\n\n\n\n\n\n\n超参数调优（Hyperparameter Tuning）\n超参数（又称调优参数）是机器学习算法中用来控制复杂度、从而影响偏差-方差权衡的“旋钮”。并不是所有算法都有超参数（例如普通最小二乘回归），但大多数算法至少有一个或多个。\n例如，高方差的 k 最近邻（kNN）模型中，模型只有一个超参数k。它决定了预测时，使用训练集中与目标观测点最近的 k 个观测值来计算预测结果。如果 k 很小，模型预测时依赖的观测值很少，容易受到噪声影响，导致预测值高度波动；而当 k 变大时，预测会基于更大子集的均值，从而减少预测的方差，但是又会导致高偏差。\n\n\n\n\n\n\n\n\n\n可以手动调整超参数，直到找到能带来高预测精度（例如通过 k 折交叉验证评估）的最佳组合，非常耗时。另一种方法是网格搜索（grid search）。在网格搜索中，预先定义一组候选的超参数值（例如 k = 1, 2, …, j），并通过重采样方法（如 k 折交叉验证）评估哪个 k 在新数据上的泛化性能最好。\n下图展示了一个重复 10 折交叉验证的网格搜索结果，考察了 k 从 2 到 150 的不同取值。结果表明，平均来看，k = 46 的误差最小。"
  },
  {
    "objectID": "bigdata/machlearn/mlbasics.html#模型评估",
    "href": "bigdata/machlearn/mlbasics.html#模型评估",
    "title": "机器学习基础",
    "section": "2.6 模型评估",
    "text": "2.6 模型评估\n模型评估方法是通过 损失函数（loss function） 来衡量预测精度。损失函数是一种度量，用于比较预测值和真实值之间的差异。在评估预测模型性能时，可以选择多种损失函数，每种函数提供对预测精度的独特理解，并且在回归和分类模型之间有所不同。在确定首选的性能指标时，需要结合 问题的具体情境。在比较多个模型时，则必须使用 相同的指标。\n\n回归模型常见指标\n\nMSE（均方误差）\n\\[\nMSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\]\n将误差平方后再求平均，大误差会受到更大惩罚。MSE（以及 RMSE）是最常见的误差度量。 目标：最小化\nRMSE（均方根误差）\n\\[\nRMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n\\]\nRMSE 让误差与响应变量保持相同的单位。 目标：最小化\nDeviance（平均残差偏差） 衡量模型通过极大似然估计解释数据变异程度。实质上，它比较“饱和模型”（完全拟合模型）与“非饱和模型”（仅含截距或均值模型）的差异。如果响应变量服从高斯分布，Deviance 大约等于 MSE，否则 Deviance 通常能给出更有用的误差估计，更常用于分类模型。 目标：最小化\nMAE（平均绝对误差）\n\\[\nMAE = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|\n\\]\n与 MSE 相比，MAE 不平方误差，因此对大误差的惩罚较小。 目标：最小化\nRMSLE（均方根对数误差）\n\\[\nRMSLE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\left( \\log(y_i+1) - \\log(\\hat{y}_i+1) \\right)^2}\n\\]\n先对真实值和预测值取对数再计算误差。当响应变量数值跨度大时，较大响应值及其误差可能主导 MSE/RMSE，而 RMSLE 可以减弱这种影响，使小数值的预测误差也能得到合理体现。 目标：最小化\nR²（决定系数） 表示因变量方差中能被自变量解释的比例。 但它有不少局限性：例如，两个模型的 RMSE 相同，但如果其中一个数据集的响应变量变异性更小，那么它的 R² 会更低。因此，R² 不应被过度依赖。 目标：最大化\n\n\n\n分类模型常见指标\n\n误分类率（Misclassification） 即整体错误率。例如预测 3 个类别（高、中、低），真实样本量分别为 25、30、45，总计 100。如果高类错分 3 个，中类错分 6 个，低类错分 4 个，则总共错分 13 个样本，误分类率为 13%。 目标：最小化\n各类别平均误差（Mean per class error） 每一类的平均误差率再取平均。例如上例中，分别为 3/25、6/30、4/45，平均误差率约为 13.6%。如果类别均衡，则该值等于误分类率。 目标：最小化\nMSE（均方误差，分类版） 计算预测概率与真实类别（1.0）之间的距离。预测错误时，差距越大惩罚越大。 目标：最小化\n交叉熵（Cross-entropy，又称对数损失或偏差） 与 MSE 类似，但引入对预测概率的对数运算。对于真实类别预测概率过低时，惩罚极大（即对错误答案充满信心时最糟糕）。 目标：最小化\n基尼指数（Gini Index） 主要用于树模型，衡量节点“纯度”。数值越小表示节点中大多数样本来自同一类别。 目标：最小化\n\n\n\n\n混淆矩阵与分类指标\n在分类模型中，我们通常使用 混淆矩阵（Confusion Matrix） 来评估性能。它比较实际类别与预测类别：\n\n真阳性（TP）：预测为正，实际也为正\n假阳性（FP）：预测为正，实际为负\n假阴性（FN）：预测为负，实际为正\n真阴性（TN）：预测为负，实际为负\n\n由此可得多种性能指标：\n\n准确率（Accuracy）\n\\[\nAccuracy = \\frac{TP+TN}{Total}\n\\]\n衡量总体预测正确率。 目标：最大化\n精确率（Precision）\n\\[\nPrecision = \\frac{TP}{TP+FP}\n\\]\n衡量预测为正的样本中有多少是真的正例。 目标：最大化\n召回率（Sensitivity/Recall）\n\\[\nRecall = \\frac{TP}{TP+FN}\n\\]\n衡量所有真实正例中有多少被预测出来。 目标：最大化\n特异度（Specificity）\n\\[\nSpecificity = \\frac{TN}{TN+FP}\n\\]\n衡量所有真实负例中有多少被正确识别。 目标：最大化\nAUC（ROC 曲线下面积） ROC 曲线以 假阳性率 为横轴，真阳性率 为纵轴。随机猜测对应对角线，曲线越靠近左上角，模型越好。AUC 衡量曲线下面积，反映分类器整体性能。 目标：最大化\n\n\nlibrary(plotROC)\n\n# Generate data\nset.seed(123)\nresponse &lt;- rbinom(200, size = 1, prob = .5)\n\nset.seed(123)\ncurve1   &lt;- rnorm(200, mean = response, sd = .40)\n\nset.seed(123)\ncurve2   &lt;- rnorm(200, mean = response, sd = .75)\n\nset.seed(123)\ncurve3   &lt;- rnorm(200, mean = response, sd = 2.0)\n\ndf &lt;- tibble(response, curve1, curve2, curve3)\n\nggplot(df) + \n  geom_roc(aes(d = response, m = curve1), n.cuts = 0, size = .5, color = \"#1E56F9\") + \n  geom_roc(aes(d = response, m = curve2), n.cuts = 0, size = .5, color = \"#7194F9\") + \n  geom_roc(aes(d = response, m = curve3), n.cuts = 0, size = .5, color = \"#B6C7F9\") +\n  geom_abline(lty = 'dashed') +\n  annotate(\"text\", x = .48, y = .46, label = c(\"与猜测无异\"), \n           vjust = 1, angle = 34) +\n  annotate(\"text\", x = .3, y = .6, label = c(\"一般\"), \n           vjust = 1, angle = 33, color = \"#B6C7F9\") +\n  annotate(\"text\", x = .20, y = .75, label = c(\"较好\"), \n           vjust = 1, angle = 33, color = \"#7194F9\") +\n  annotate(\"text\", x = .10, y = .96, label = c(\"最佳\"), \n           vjust = 1, angle = 33, color = \"#1E56F9\") +\n  xlab(\"假阳性率\") +\n  ylab(\"真阳性率\")\n\n\n\n\n\n\n\n\n\n\n\n2.7 建模示例\n在 Ames 房价数据 上做一个简单评估。首先，进行 分层抽样（stratified sampling），将数据划分为训练集和测试集，同时确保训练集和测试集之间的响应变量分布一致。\n\n# 使用 rsample 包进行分层抽样\nset.seed(123)\nsplit &lt;- initial_split(ames, prop = 0.7, strata = \"Sale_Price\")\names_train  &lt;- training(split)\names_test   &lt;- testing(split)\n\n接下来，对数据应用 k 近邻回归器（k-nearest neighbor regressor, KNN）。使用 caret 包的元框架简化 重采样、网格搜索和模型应用 的过程。\n\n重采样方法：10 折交叉验证（10-fold CV），并重复 5 次。\n网格搜索：指定要评估的超参数值（k = 2, 3, 4, …, 25）。\n模型训练与验证：使用 KNN 模型（method = “knn”），采用预先设定的重采样过程（trControl = cv）、网格搜索（tuneGrid = hyper_grid）、以及损失函数（metric = “RMSE”）。\n\n该网格搜索大约需要 3.5 分钟。\n\n# 指定重采样策略\ncv &lt;- trainControl(\n  method = \"repeatedcv\", \n  number = 10, \n  repeats = 5\n)\n\n# 创建超参数值的网格\nhyper_grid &lt;- expand.grid(k = seq(2, 25, by = 1))\n\n# 使用网格搜索调参 KNN 模型\nknn_fit &lt;- train(\n  Sale_Price ~ ., \n  data = ames_train, \n  method = \"knn\", \n  trControl = cv, \n  tuneGrid = hyper_grid,\n  metric = \"RMSE\"\n)\n\n从结果可以看到，最佳模型对应于 k = 6，其 RMSE 为 43846.05。这意味着，平均而言，我们的模型对房价的预测误差约为 43846美元。\n\n# 打印和绘制交叉验证结果\nknn_fit\n## k-Nearest Neighbors \n## \n## 2049 samples\n##   80 predictor\n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold, repeated 5 times) \n## Summary of sample sizes: 1844, 1844, 1843, 1844, 1844, 1845, ... \n## Resampling results across tuning parameters:\n## \n##   k   RMSE      Rsquared   MAE     \n##    2  47206.74  0.6596344  31133.25\n##    3  45684.91  0.6773635  30007.71\n##    4  44771.97  0.6901038  29276.09\n##    5  44064.54  0.7005790  28996.93\n##    6  43846.05  0.7045166  28895.48\n##    7  43858.13  0.7059401  28883.74\n##    8  44181.13  0.7033657  29055.05\n##    9  44352.03  0.7028597  29109.00\n##   10  44332.46  0.7053884  29129.62\n##   11  44282.81  0.7083442  29081.39\n##   12  44486.34  0.7075253  29155.41\n##   13  44647.15  0.7076206  29256.68\n##   14  44790.79  0.7077073  29307.11\n##   15  45041.02  0.7063767  29423.86\n##   16  45119.37  0.7073844  29484.00\n##   17  45264.01  0.7070891  29586.22\n##   18  45366.02  0.7072968  29641.92\n##   19  45537.84  0.7066304  29766.52\n##   20  45746.63  0.7052851  29907.89\n##   21  45983.35  0.7031524  30058.55\n##   22  46187.92  0.7017539  30192.39\n##   23  46406.95  0.7001361  30329.98\n##   24  46605.01  0.6986611  30483.94\n##   25  46824.70  0.6971044  30617.33\n## \n## RMSE was used to select the optimal model using the smallest value.\n## The final value used for the model was k = 6.\nggplot(knn_fit)"
  },
  {
    "objectID": "bigdata/machlearn/mlbasics.html#相关r包",
    "href": "bigdata/machlearn/mlbasics.html#相关r包",
    "title": "机器学习基础",
    "section": "3.1 相关R包",
    "text": "3.1 相关R包\n\n# 辅助包\nlibrary(dplyr)    # 数据操作\nlibrary(ggplot2)  # 可视化\nlibrary(visdat)   # 数据可视化增强\n\n# 特征工程相关包\nlibrary(caret)    # 各种机器学习任务\nlibrary(recipes)  # 特征工程任务"
  },
  {
    "objectID": "bigdata/machlearn/mlbasics.html#目标变量的转换",
    "href": "bigdata/machlearn/mlbasics.html#目标变量的转换",
    "title": "机器学习基础",
    "section": "3.2 目标变量的转换",
    "text": "3.2 目标变量的转换\n对响应变量进行转换常常能提升预测效果，尤其是要求满足某些假设的参数模型。例如，线性回归模型假设误差服从正态分布，进而要求响应变量服从条件正态分布。当预测目标变量有长尾（即存在异常值）或偏态时，假设往往不成立。\n例如，Ames 房价数据集（Sale_Price）呈现右偏（即正偏）。而简单的线性模型，例如：\n\\[\nSale\\_Price = \\beta_0 + \\beta_1 \\cdot Year\\_Built + \\epsilon\n\\]\n往往假设误差项 \\(\\epsilon\\) （进而响应变量）服从正态分布，进行对数（或类似）变换通常能缓解违背正态性问题。\n\nmodels &lt;- c(\"原始模型残差分布\", \n            \"对数变换后模型残差分布\")\nlist(\n  m1 = lm(Sale_Price ~ Year_Built, data = ames_train),\n  m2 = lm(log(Sale_Price) ~ Year_Built, data = ames_train)\n) %&gt;%\n  map2_dfr(models, ~ broom::augment(.x) %&gt;% mutate(model = .y)) %&gt;%\n  ggplot(aes(.resid)) +\n    geom_histogram(bins = 75) +\n    facet_wrap(~ model, scales = \"free_x\") +\n    ylab(NULL) +\n    xlab(\"残差\")\n\n\n\n\n\n\n\n\n修正目标变量正偏态的两种方式：\n方式1：对数变换（Log Transformation） 大多数右偏分布通过对数变换后可近似正态分布。简单做法是直接对训练集和测试集进行对数变换。更好的方式是使用 recipe 包（或类似的caret::preProcess()）生成一个可重复应用的预处理模板。\n\n# 对数变换\names_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;%\n  step_log(all_outcomes())\n\n如果响应变量有负数或零，对数变换会产生 NaN 或 -Inf。此时可以使用 log1p()（相当于 log(1+x)），或在 step_log() 中使用 offset 参数（默认值为0）。若响应变量值小于或等于-1，则可通过 step_YeoJohnson() 进行Yeo-Johnson变换，不要求输入变量为正。\n\nlog(-0.5)\n## [1] NaN\n## [1] NaN\nlog1p(-0.5)\n## [1] -0.6931472\n## [1] -0.6931472\n\n\n方式2：Box-Cox 变换 Box-Cox 变换比对数更灵活（对数是其特例），它会在 \\(-5 \\leq \\lambda \\leq 5\\) 范围内搜索最佳参数 \\(\\lambda\\)，以找到能让变量尽量接近正态分布的变换：\n\\[\ny(\\lambda) =\n\\begin{cases}\n\\dfrac{Y^\\lambda - 1}{\\lambda}, & \\lambda \\neq 0 \\\\\n\\log(Y), & \\lambda = 0\n\\end{cases}\n\\]\n注意应当在训练集上计算最佳 λ，并在训练集与测试集上使用相同 λ，以避免数据泄露。recipes 包能自动完成这一过程。\n此外，如果在建模时对目标变量进行了变换，模型预测的结果也是在变换后的尺度上。通常需要将预测值逆变换回原始尺度，以便决策者更容易理解。\n\n# 对数变换与逆变换\ny &lt;- log(10)\nexp(y) \n## [1] 10\n## [1] 10\n\n# Box-Cox 变换与逆变换\nlambda  &lt;- forecast::BoxCox.lambda(ames_train$Sale_Price)\ny &lt;- forecast::BoxCox(10, lambda)\n\ninv_box_cox &lt;- function(x, lambda) {\n  if (lambda == 0) exp(x) else (lambda*x + 1)^(1/lambda)\n}\n\ninv_box_cox(y, lambda)\n## [1] 10\n## attr(,\"lambda\")\n## [1] 0.138742\n## [1] 10"
  },
  {
    "objectID": "bigdata/machlearn/mlbasics.html#处理缺失值",
    "href": "bigdata/machlearn/mlbasics.html#处理缺失值",
    "title": "机器学习基础",
    "section": "3.3 处理缺失值",
    "text": "3.3 处理缺失值\n数据缺失通常被归为两类：信息性缺失（informative missingness）和随机缺失（missingness at random）。信息性缺失意味着缺失值背后存在某种结构性原因，可能是数据收集方式上的缺陷，或是观测环境的异常。随机缺失则意味着缺失值的出现与数据收集过程无关。\n\n如果缺失是 信息性的，可以为其设定一个新的类别（例如”None”），因为其独特性可能会影响预测性能。\n如果缺失是 随机的，则可能需要删除（deletion）或填补（imputation）。\n\n不同的机器学习模型对缺失值的处理方式不同。大多数算法无法直接处理缺失值（例如广义线性模型、神经网络、支持向量机），因此必须在建模前处理好缺失值。少数模型（主要是基于树的方法）内置了处理缺失值的机制。由于建模通常需要比较多个模型以寻找最佳方案，因此一般都需要在建模前处理好缺失值，以保证各个算法基于相同的数据质量。\n\n缺失值可视化\n理解缺失值（即 NA）的分布非常重要。\n\nAmesHousing::ames_raw %&gt;%\n  is.na() |&gt;\n  reshape2::melt() |&gt;\n  ggplot(aes(Var1, Var2, fill=value)) + \n  geom_raster() + \n  scale_x_continuous(NULL, expand = c(0, 0)) +\n  scale_fill_grey(name = \"\", \n                  labels = c(\"存在\", \n                             \"缺失\")) +\n  xlab(\"观测\") +\n  ylab(\"变量\") +\n  theme(axis.text.y  = element_text(size = 4))\n\n\n\n\n\n\n\n\n通过热图，可以清楚地看到大部分缺失值集中在哪些变量上（如 Alley、Fireplace Qual、Pool QC、Fence 和 Misc Feature）。这些变量由于缺失比例过高，往往需要在统计分析前被删除或填补。此外，有一些明显的缺失模式，例如所有与garage相关的变量往往在相同的观测中缺失。\n\n\n缺失值填补（Imputation）\n缺失值填补 是用替代的“最佳猜测值”来代替缺失值的过程。这通常是特征工程中的首要步骤之一，因为它会影响后续的预处理和建模。\n\n基于统计量的填补\n\n最简单的方式是用描述性统计量（均值、中位数、众数）替换缺失值。这种方式计算高效，但不考虑其他特征对缺失值的影响。改进方法是使用分组统计量，但在大数据集上很难操作。更可行的办法是使用 建模填补，如 K 最近邻（KNN）或基于树的方法。\n\nk最近邻（KNN）填补\n\n找出缺失值所在的观测；\n计算它与其他观测的相似度（基于其他特征）；\n使用最相似的 k 个邻居的值来填补缺失（计算平均值或者取众数）。\n\n\n如果所有特征都是数值型，则常用欧氏距离。如果是数值与类别混合，则常用 Gower 距离。建议 k 的取值为 5–10。KNN 填补适用于中小规模数据集，数据越大，计算成本越高。\n\names_recipe %&gt;%\n  step_impute_knn(all_predictors(), neighbors = 6)\n## \n## ── Recipe ──────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:    1\n## predictor: 80\n## \n## ── Operations\n## • Log transformation on: all_outcomes()\n## • K-nearest neighbor imputation for: all_predictors()\n\n\n基于决策树补值\n\n某些决策树及其扩展模型可以直接处理缺失值，因此也常用于填补。单棵树的方差较大，但通过集成方法（如随机森林、Bagging）可以得到稳健的预测器。随机森林填补精度较高，但计算成本较大。Bagging 树填补是精度与效率的折中方案。与 KNN 类似，它将含缺失值的特征视作目标变量，用树模型来预测缺失值。\n\names_recipe %&gt;%\n  step_impute_bag(all_predictors())\n## \n## ── Recipe ──────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:    1\n## predictor: 80\n## \n## ── Operations\n## • Log transformation on: all_outcomes()\n## • Bagged tree imputation for: all_predictors()\n\n\n不同方法比较\n\n展示三种填补方法（均值、KNN、基于树）的对比：\n\n\n\n\n\n\n\n\n\n均值或中位数填补会让所有观测得到相同的预测值，从而削弱特征与响应变量之间的关系。KNN 和基于树的方法更好地保持了特征分布和与响应的关系。"
  },
  {
    "objectID": "bigdata/machlearn/mlbasics.html#特征选择feature-filtering",
    "href": "bigdata/machlearn/mlbasics.html#特征选择feature-filtering",
    "title": "机器学习基础",
    "section": "3.4 特征选择（Feature filtering）",
    "text": "3.4 特征选择（Feature filtering）\n许多数据分析和建模项目会收集到数百甚至数千个特征（预测变量）。特征过多的模型往往更难解释，同时计算成本也更高。大多数模型的训练时间都会随着非信息性预测变量增加而显著增长。不过，不同类别的模型的性能（例如预测偏差）受到非信息性预测变量的影响程度有较大差别，例如 Lasso 和基于决策树的方法不易受到影响，而主成分回归、偏最小二乘法回归受到的影响较大。\n因此，在建模前进行特征过滤或降维，可以大大加快训练速度和提高某些模型性能。最容易被识别并剔除的特征是零方差（zero variance）和近零方差（near-zero variance）变量。零方差变量，即整个特征只有一个唯一值，完全不提供任何有用信息。近零方差变量提供的信息极少，甚至可能在重采样时引发问题。例如，一个特征几乎只有一个主导值，那么某些采样子集中该特征可能只有一个唯一取值。\n判定近零方差变量的经验法则包括两个条件：一是不同取值的占比（具有唯一值的观测 / 样本量）很低（≤ 10%）；二是最常见取值与第二常见取值的频次比很大（≥ 20）。\n假设我们有一个包含1000个样本的数据集，其中有一个变量叫“性别状态”。999个值是“男”，1个值是“女”。唯一值比例 = 2 / 1000 = 0.002 （远小于0.1，满足条件一）；频率比 = 999 / 1 = 999 （远大于20，满足条件二）；所以，这是一个非常典型的近零方差变量。\n\ncaret::nearZeroVar(ames_train, saveMetrics = TRUE) %&gt;% \n  tibble::rownames_to_column() %&gt;% \n  filter(nzv)\n##               rowname  freqRatio percentUnique zeroVar  nzv\n## 1              Street  226.66667    0.09760859   FALSE TRUE\n## 2               Alley   24.25316    0.14641288   FALSE TRUE\n## 3        Land_Contour   19.50000    0.19521718   FALSE TRUE\n## 4           Utilities 1023.00000    0.14641288   FALSE TRUE\n## 5          Land_Slope   22.15909    0.14641288   FALSE TRUE\n## 6         Condition_2  202.60000    0.34163006   FALSE TRUE\n## 7           Roof_Matl  144.35714    0.39043436   FALSE TRUE\n## 8           Bsmt_Cond   20.24444    0.29282577   FALSE TRUE\n## 9      BsmtFin_Type_2   25.85294    0.34163006   FALSE TRUE\n## 10       BsmtFin_SF_2  453.25000    9.37042460   FALSE TRUE\n## 11            Heating  106.00000    0.29282577   FALSE TRUE\n## 12    Low_Qual_Fin_SF 1010.50000    1.31771596   FALSE TRUE\n## 13      Kitchen_AbvGr   21.23913    0.19521718   FALSE TRUE\n## 14         Functional   38.89796    0.39043436   FALSE TRUE\n## 15     Enclosed_Porch  102.05882    7.41825281   FALSE TRUE\n## 16 Three_season_porch  673.66667    1.12249878   FALSE TRUE\n## 17       Screen_Porch  169.90909    4.63640800   FALSE TRUE\n## 18          Pool_Area 2039.00000    0.53684724   FALSE TRUE\n## 19            Pool_QC  509.75000    0.24402147   FALSE TRUE\n## 20       Misc_Feature   34.18966    0.24402147   FALSE TRUE\n## 21           Misc_Val  180.54545    1.56173743   FALSE TRUE\n\n可以在 ames_recipe 中添加 step_zv() 和 step_nzv() 来移除这些零方差或近零方差特征。"
  },
  {
    "objectID": "bigdata/machlearn/mlbasics.html#数值型特征的转换",
    "href": "bigdata/machlearn/mlbasics.html#数值型特征的转换",
    "title": "机器学习基础",
    "section": "3.5 数值型特征的转换",
    "text": "3.5 数值型特征的转换\n数值特征在分布偏态、存在异常值或量级范围较大时，会对某些模型造成问题。基于树的模型对这些特征空间的问题具有较强的稳健性，但其他模型（例如广义线性模型（GLMs）、正则化回归、K近邻、支持向量机、神经网络）可能会因这些问题受到严重影响。对高度偏态的特征进行归一化和标准化可以帮助缓解这些问题。\n\n偏态\n与对目标变量进行归一化类似，具有分布假设的参数模型（例如广义线性模型和正则化模型）也需要减少数值型特征的偏态来提升性能。在对多个变量进行归一化时，最好使用Box-Cox变换（当特征值严格为正时）或Yeo-Johnson变换（当特征值不严格为正时），因为这些方法能够识别是否需要变换以及最佳变换方式。\n非参数模型很少受到偏态特征的影响；然而，对特征进行归一化不会对这些模型的性能产生负面影响。例如，对基于树的算法，归一化特征只会改变最佳分割点。因此，当有疑问时，建议进行归一化。\n\n# 对所有数值列进行归一化\nrecipe(Sale_Price ~ ., data = ames_train) %&gt;%\n  step_YeoJohnson(all_numeric())\n## \n## ── Recipe ──────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:    1\n## predictor: 80\n## \n## ── Operations\n## • Yeo-Johnson transformation on: all_numeric()\n\n\n\n标准化\n我们还必须考虑各个特征的量级。所有特征的最大值和最小值是多少？它们是否跨越了几个数量级？包含输入特征平滑函数的模型对输入的量级敏感。例如，\n5X + 2 是一个简单的输入X的线性函数，其输出的量级直接取决于输入的量级。许多算法在其内部使用线性函数，有些较为明显（例如GLMs和正则化回归），有些则不那么明显（例如神经网络、支持向量机和主成分分析）。其他例子包括使用欧几里得距离等距离度量的算法（例如K近邻、K均值聚类和层次聚类）。\n对于这些模型和建模组件，标准化特征通常是一个好主意。标准化特征包括中心化和缩放，使数值变量具有零均值和单位方差，从而为所有变量提供一个可比较的通用量度单位。\n标准化特征使所有特征能够在统一的数值尺度上进行比较，而不考虑其实际值差异。 图3.8：标准化特征使所有特征能够在统一的数值尺度上进行比较，而不考虑其实际值差异。\n某些软件包（例如glmnet和caret）内置了标准化选项，而其他软件包（例如用于神经网络的keras）则没有。然而，你应该在配方蓝图中标准化变量，以便训练和测试数据的标准化基于相同的均值和方差。这有助于最小化数据泄露。\n\names_recipe %&gt;%\n  step_center(all_numeric(), -all_outcomes()) %&gt;%\n  step_scale(all_numeric(), -all_outcomes())\n## \n## ── Recipe ──────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:    1\n## predictor: 80\n## \n## ── Operations\n## • Log transformation on: all_outcomes()\n## • Centering for: all_numeric() -all_outcomes()\n## • Scaling for: all_numeric() -all_outcomes()\n## 数据配方\n##\n## 输入：\n##\n##       角色 #变量数\n##    结果变量          1\n##    预测变量         80\n##\n## 操作：\n##\n## 对所有结果变量进行对数变换\n## 对所有数值变量（除结果变量外）进行中心化\n## 对所有数值变量（除结果变量外）进行缩放"
  },
  {
    "objectID": "bigdata/machlearn/mlbasics.html#分类型特征的转换",
    "href": "bigdata/machlearn/mlbasics.html#分类型特征的转换",
    "title": "机器学习基础",
    "section": "3.6 分类型特征的转换",
    "text": "3.6 分类型特征的转换\n大多数模型要求预测变量采用数值形式。但是也有例外，例如，基于决策树的模型可以处理数值或分类特征。分类型特征预处理也能提升相关模型的训练效率。\n\n合并（Lumping）\n有时类别特征会包含频次很少的取值。例如，Ames住房数据中有28个社区，但其中属于几个社区的观测值很少。\n\ncount(ames_train, Neighborhood) %&gt;% arrange(n)\n## # A tibble: 28 × 2\n##    Neighborhood            n\n##    &lt;fct&gt;               &lt;int&gt;\n##  1 Landmark                1\n##  2 Green_Hills             2\n##  3 Greens                  3\n##  4 Blueste                 8\n##  5 Veenker                15\n##  6 Northpark_Villa        17\n##  7 Bloomington_Heights    18\n##  8 Meadow_Village         22\n##  9 Briardale              23\n## 10 Clear_Creek            26\n## # ℹ 18 more rows\n\n可以将这些水平合并（lumping）成较少的类别。例如，将训练样本中观测值少于10%的所有水平合并为“其他”类别。可以使用step_other()来实现。不过，使用合并应谨慎，因为会导致模型性能下降。基于决策树的模型通常在高基数特征（分类变量中包含非常多的、不重复的类别，例如，邮编、城市名等）上表现非常好，且不受低代表性水平的影响。\n\n# 对特征进行水平合并\nlumping &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;%\n  step_other(Neighborhood, threshold = 0.01, \n             other = \"other\") \n\n# 应用配方\napply_2_training &lt;- prep(lumping, training = ames_train) %&gt;%\n  bake(ames_train)\n\n# Neighborhood的新分布\ncount(apply_2_training, Neighborhood) %&gt;% arrange(n)\n## # A tibble: 22 × 2\n##    Neighborhood                                n\n##    &lt;fct&gt;                                   &lt;int&gt;\n##  1 Meadow_Village                             22\n##  2 Briardale                                  23\n##  3 Clear_Creek                                26\n##  4 South_and_West_of_Iowa_State_University    32\n##  5 Stone_Brook                                40\n##  6 Northridge                                 51\n##  7 Timberland                                 52\n##  8 other                                      64\n##  9 Brookside                                  74\n## 10 Crawford                                   75\n## # ℹ 12 more rows\n\n\n\n独热编码与哑变量编码\n许多模型要求所有预测变量为数值形式。因此，需要将分类变量转换为数值表示，以便这些算法能够计算。一些软件包（例如h2o和caret）会自动执行此过程，而其他软件包（例如glmnet和keras）则不会。有许多方法可以将分类变量重新编码为数值形式（例如独热编码、序数编码、二进制编码、求和编码、Helmert编码）。\n最常见的方法是独热编码（one-hot encoding），即将分类变量转置，使特征的每个水平表示为布尔值。然而，这会导致完全共线性，对某些预测建模算法（例如普通线性回归和神经网络）造成问题。可以通过删除一个水平（例如删除了水平c）创建满秩编码。这被称为哑变量编码（dummy encoding）。\n\n\n\n独热编码与哑变量编码\n\n\n可以使用相同的函数step_dummy()进行独热编码或哑变量编码。默认情况下，step_dummy()会创建满秩编码，但可以通过设置one_hot = TRUE更改为独热编码。\n\n# 对两个特征进行水平合并\nrecipe(Sale_Price ~ ., data = ames_train) %&gt;%\n  step_dummy(all_nominal(), one_hot = TRUE)\n## \n## ── Recipe ──────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:    1\n## predictor: 80\n## \n## ── Operations\n## • Dummy variables from: all_nominal()\n## 数据配方\n##\n## 输入：\n##\n##       角色 #变量数\n##    结果变量          1\n##    预测变量         80\n##\n## 操作：\n##\n## 从所有标称变量创建哑变量\n\n由于独热编码会增加新特征，可能会显著增加数据的维度。如果数据集中有许多分类变量，并且这些分类变量具有许多唯一水平，特征数量可能会激增。在这些情况下，可能需要探索标签/序数编码或其他替代方法。\n\n\n标签编码\n标签编码（label encoding）是将分类变量的水平进行纯数值转换。如果分类变量是因子且具有预定义的水平，则数值转换将按照水平顺序进行。如果未指定水平，编码将基于字母顺序。\n\n# 原始类别\ncount(ames_train, MS_SubClass)\n## # A tibble: 15 × 2\n##    MS_SubClass                                   n\n##    &lt;fct&gt;                                     &lt;int&gt;\n##  1 One_Story_1946_and_Newer_All_Styles         756\n##  2 One_Story_1945_and_Older                     94\n##  3 One_Story_with_Finished_Attic_All_Ages        4\n##  4 One_and_Half_Story_Unfinished_All_Ages       13\n##  5 One_and_Half_Story_Finished_All_Ages        203\n##  6 Two_Story_1946_and_Newer                    404\n##  7 Two_Story_1945_and_Older                     90\n##  8 Two_and_Half_Story_All_Ages                  14\n##  9 Split_or_Multilevel                          85\n## 10 Split_Foyer                                  37\n## 11 Duplex_All_Styles_and_Ages                   76\n## 12 One_Story_PUD_1946_and_Newer                132\n## 13 Two_Story_PUD_1946_and_Newer                 86\n## 14 PUD_Multilevel_Split_Level_Foyer             12\n## 15 Two_Family_conversion_All_Styles_and_Ages    43\n\n# 标签编码\nrecipe(Sale_Price ~ ., data = ames_train) %&gt;%\n  step_integer(MS_SubClass) %&gt;%\n  prep(ames_train) %&gt;%\n  bake(ames_train) %&gt;%\n  count(MS_SubClass)\n## # A tibble: 15 × 2\n##    MS_SubClass     n\n##          &lt;int&gt; &lt;int&gt;\n##  1           1   756\n##  2           2    94\n##  3           3     4\n##  4           4    13\n##  5           5   203\n##  6           6   404\n##  7           7    90\n##  8           8    14\n##  9           9    85\n## 10          10    37\n## 11          11    76\n## 12          12   132\n## 13          14    86\n## 14          15    12\n## 15          16    43\n\n对无序分类特征进行标签编码时应小心，因为大多数模型会将它们视为有序数值特征。如果分类特征具有天然的顺序，那么标签编码是一个自然的选择（通常也称为序数编码）。\n\n# 原始类别具有天然顺序\ncount(ames_train, Overall_Qual)\n## # A tibble: 10 × 2\n##    Overall_Qual       n\n##    &lt;fct&gt;          &lt;int&gt;\n##  1 Very_Poor          3\n##  2 Poor               9\n##  3 Fair              33\n##  4 Below_Average    147\n##  5 Average          580\n##  6 Above_Average    521\n##  7 Good             408\n##  8 Very_Good        242\n##  9 Excellent         82\n## 10 Very_Excellent    24\n\n# 序数编码\nrecipe(Sale_Price ~ ., data = ames_train) %&gt;%\n  step_integer(Overall_Qual) %&gt;%\n  prep(ames_train) %&gt;%\n  bake(ames_train) %&gt;%\n  count(Overall_Qual)\n## # A tibble: 10 × 2\n##    Overall_Qual     n\n##           &lt;int&gt; &lt;int&gt;\n##  1            1     3\n##  2            2     9\n##  3            3    33\n##  4            4   147\n##  5            5   580\n##  6            6   521\n##  7            7   408\n##  8            8   242\n##  9            9    82\n## 10           10    24"
  },
  {
    "objectID": "bigdata/machlearn/mlbasics.html#降维",
    "href": "bigdata/machlearn/mlbasics.html#降维",
    "title": "机器学习基础",
    "section": "3.7 降维",
    "text": "3.7 降维\n降维是一种替代手动移除变量的方法，用于过滤掉不具有信息性的特征。具体内容后面再深入介绍。\n\nrecipe(Sale_Price ~ ., data = ames_train) %&gt;%\n  step_center(all_numeric()) %&gt;%\n  step_scale(all_numeric()) %&gt;%\n# 通过主成分分析降维，保留能够解释95%方差的成分数量\n  step_pca(all_numeric(), threshold = .95)\n## \n## ── Recipe ──────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:    1\n## predictor: 80\n## \n## ── Operations\n## • Centering for: all_numeric()\n## • Scaling for: all_numeric()\n## • PCA extraction with: all_numeric()"
  },
  {
    "objectID": "bigdata/machlearn/mlbasics.html#实施特征工程",
    "href": "bigdata/machlearn/mlbasics.html#实施特征工程",
    "title": "机器学习基础",
    "section": "3.8 实施特征工程",
    "text": "3.8 实施特征工程\n特征工程应被视为创建蓝图（配方），而不是手动逐一执行每个任务，这是和一般数据分析的主要区别。因此需要思考预处理的顺序，以及如何在重采样过程中恰当应用。\n\n预处理的顺序\n\n如果使用对数或Box-Cox变换，不要先对数据进行中心化或执行可能使数据变成非正数的操作。或者，使用Yeo-Johnson变换，这样就不必担心这个问题。\n\n单热编码或哑变量编码通常会产生稀疏数据，许多算法可以高效地处理此类数据。如果对稀疏数据进行标准化，会生成密集数据，从而失去计算效率。因此，通常优先对数值特征进行标准化，然后再进行单热/哑变量编码。\n\n如果需要将不常出现的类别合并，应在单热/哑变量编码之前完成。\n\n尽管可以对分类特征执行降维过程，但在特征工程中通常主要对数值特征进行降维。\n\n预处理顺序： 1. 过滤掉零方差或近零方差的特征。 2. 如有需要，进行缺失值填补。 3. 归一化以解决数值特征的偏态。 4. 对数值特征进行标准化（中心化和缩放）。 5. 对数值特征进行降维（如PCA）。 6. 对分类特征进行单热或哑变量编码。\n\n\n数据泄漏\n数据泄漏是指在创建模型时使用了训练数据集之外的信息。数据泄漏通常发生在数据预处理期间。为了尽量减少这种情况，特征工程应在每次重采样迭代中独立进行。重采样允许我们估计可泛化的预测误差。因此，我们应将特征工程蓝图独立应用于每次重采样，即先重采样后预处理。这样可以避免从一个数据集泄漏信息到另一个数据集。\n例如，在标准化数值特征时，每个重采样的训练数据应使用其自身的均值和方差估计值，并将这些特定值应用于同一重采样的测试集。这模仿了现实中的预测场景，在现实中我们只知道当前数据的均值和方差估计值；因此，对于需要预测的新数据，假设其特征值遵循过去观察到的相同分布。\n\n\n整合流程\nrecipes包用于以顺序方式开发特征工程蓝图。使用recipes创建和应用特征工程的三个主要步骤：\n\nrecipe：定义特征工程步骤以创建蓝图。\nprepare：根据训练数据估计特征工程参数。\nbake：将蓝图应用于新数据。\n\n第一步是定义蓝图（也称为recipe），需要提供公式（定义目标变量、特征及数据），然后通过各种step_xxx()依次添加特征工程步骤。\n\n# 定义目标变量，特征，数据\nblueprint &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;%\n\n# 移除近零方差的分类（即名义）特征\n  step_nzv(all_nominal())  %&gt;%\n\n# 对质量相关的特征进行序数编码\n  step_integer(matches(\"Qual|Cond|QC|Qu\")) %&gt;%\n\n# 对所有数值特征进行中心化和缩放（即标准化）\n  step_center(all_numeric(), -all_outcomes()) %&gt;%\n  step_scale(all_numeric(), -all_outcomes()) %&gt;%\n\n# 对所有数值特征应用PCA进行降维\n  step_pca(all_numeric(), -all_outcomes())\n  \nblueprint\n## \n## ── Recipe ──────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:    1\n## predictor: 80\n## \n## ── Operations\n## • Sparse, unbalanced variable filter on: all_nominal()\n## • Integer encoding for: matches(\"Qual|Cond|QC|Qu\")\n## • Centering for: all_numeric() -all_outcomes()\n## • Scaling for: all_numeric() -all_outcomes()\n## • PCA extraction with: all_numeric() -all_outcomes()\n\n因为数据泄漏，许多特征工程步骤不能在测试集数据上训练（例如，标准化和PCA）。因此，接下来，在训练集数据上训练蓝图，估计参数。\n\nprepare &lt;- prep(blueprint, training = ames_train)\nprepare\n## \n## ── Recipe ──────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:    1\n## predictor: 80\n## \n## ── Training information\n## Training data contained 2049 data points and no incomplete rows.\n## \n## ── Operations\n## • Sparse, unbalanced variable filter removed: Street Alley, ... | Trained\n## • Integer encoding for: Condition_1 Overall_Qual, ... | Trained\n## • Centering for: Lot_Frontage, Lot_Area, Condition_1, ... | Trained\n## • Scaling for: Lot_Frontage, Lot_Area, Condition_1, ... | Trained\n## • PCA extraction with: Lot_Frontage, Lot_Area, Condition_1, ... | Trained\n\n最后，使用bake()将蓝图应用于新数据（例如，训练数据或未来的测试数据）。\n\nbaked_train &lt;- bake(prepare, new_data = ames_train)\nbaked_test &lt;- bake(prepare, new_data = ames_test)\nbaked_train\n## # A tibble: 2,049 × 27\n##    MS_SubClass MS_Zoning Lot_Shape Lot_Config Neighborhood Bldg_Type House_Style\n##    &lt;fct&gt;       &lt;fct&gt;     &lt;fct&gt;     &lt;fct&gt;      &lt;fct&gt;        &lt;fct&gt;     &lt;fct&gt;      \n##  1 Two_Story_… Resident… Regular   Inside     Briardale    Twnhs     Two_Story  \n##  2 Two_Story_… Resident… Regular   Inside     Briardale    Twnhs     Two_Story  \n##  3 One_Story_… Resident… Regular   FR2        Northpark_V… Twnhs     One_Story  \n##  4 One_Story_… Resident… Regular   Inside     Sawyer_West  TwnhsE    One_Story  \n##  5 One_Story_… Resident… Regular   Corner     Sawyer_West  OneFam    One_Story  \n##  6 Duplex_All… Resident… Regular   Inside     Sawyer       Duplex    One_and_Ha…\n##  7 One_Story_… Resident… Slightly… Inside     Sawyer       OneFam    One_Story  \n##  8 One_Story_… Resident… Slightly… Corner     Sawyer       OneFam    One_Story  \n##  9 Duplex_All… Resident… Slightly… Inside     North_Ames   Duplex    One_Story  \n## 10 One_Story_… Resident… Regular   Inside     North_Ames   OneFam    One_Story  \n## # ℹ 2,039 more rows\n## # ℹ 20 more variables: Roof_Style &lt;fct&gt;, Exterior_1st &lt;fct&gt;,\n## #   Exterior_2nd &lt;fct&gt;, Mas_Vnr_Type &lt;fct&gt;, Foundation &lt;fct&gt;,\n## #   Bsmt_Exposure &lt;fct&gt;, BsmtFin_Type_1 &lt;fct&gt;, Central_Air &lt;fct&gt;,\n## #   Electrical &lt;fct&gt;, Garage_Type &lt;fct&gt;, Garage_Finish &lt;fct&gt;,\n## #   Paved_Drive &lt;fct&gt;, Fence &lt;fct&gt;, Sale_Type &lt;fct&gt;, Sale_Price &lt;int&gt;,\n## #   PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;, PC4 &lt;dbl&gt;, PC5 &lt;dbl&gt;\n\n只需要指定蓝图，caret包会自动在每次重采样中进行prepare和bake。\n首先，我们创建特征工程蓝图来执行以下任务：\n\n。\n对所有基于1-10李克特量表的质量特征进行序数编码。\n对所有数值特征进行标准化（中心化和缩放）。\n对剩余的分类特征进行单热编码。\n\n\n# 指定蓝图\nblueprint &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;%\n# 过滤掉分类特征的近零方差特征\n  step_nzv(all_nominal()) %&gt;%\n# 对质量特征进行序数编码\n  step_integer(matches(\"Qual|Cond|QC|Qu\")) %&gt;%\n# 对数值特征进行标准化\n  step_center(all_numeric(), -all_outcomes()) %&gt;%\n  step_scale(all_numeric(), -all_outcomes()) %&gt;%\n# 对剩余的分类特征进行单热编码\n  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)\n\n# 指定重采样计划\ncv &lt;- trainControl(\n  method = \"repeatedcv\", \n  number = 10, \n  repeats = 5\n)\n\n# 构建超参数值网格\nhyper_grid &lt;- expand.grid(k = seq(2, 25, by = 1))\n\n# 使用网格搜索调整knn模型\nknn_fit2 &lt;- train(\n  blueprint, \n  data = ames_train, \n  method = \"knn\", \n  trControl = cv, \n  tuneGrid = hyper_grid,\n  metric = \"RMSE\"\n)\n\n\n# 打印模型结果\nknn_fit2\n## k-Nearest Neighbors \n## \n## 2049 samples\n##   80 predictor\n## \n## Recipe steps: nzv, integer, center, scale, dummy \n## Resampling: Cross-Validated (10 fold, repeated 5 times) \n## Summary of sample sizes: 1843, 1844, 1844, 1844, 1843, 1845, ... \n## Resampling results across tuning parameters:\n## \n##   k   RMSE      Rsquared   MAE     \n##    2  35817.97  0.8065426  22797.15\n##    3  34762.86  0.8204012  22011.14\n##    4  34662.29  0.8245930  21686.06\n##    5  34558.37  0.8279897  21589.23\n##    6  34303.54  0.8324663  21297.17\n##    7  34018.45  0.8368539  21055.32\n##    8  33786.32  0.8406288  20966.21\n##    9  33615.60  0.8432204  20882.91\n##   10  33645.42  0.8436817  20927.30\n##   11  33584.07  0.8450951  20911.50\n##   12  33681.99  0.8449951  20999.03\n##   13  33756.38  0.8454080  21087.29\n##   14  33888.91  0.8448624  21158.27\n##   15  34029.58  0.8440726  21227.73\n##   16  34139.13  0.8435175  21284.60\n##   17  34313.23  0.8424538  21379.97\n##   18  34377.97  0.8425127  21426.99\n##   19  34455.16  0.8422809  21481.10\n##   20  34549.85  0.8418118  21522.68\n##   21  34621.00  0.8415634  21559.62\n##   22  34722.41  0.8412499  21618.55\n##   23  34803.38  0.8409645  21662.78\n##   24  34879.01  0.8406783  21704.07\n##   25  34933.37  0.8405117  21745.60\n## \n## RMSE was used to select the optimal model using the smallest value.\n## The final value used for the model was k = 11.\n\n\n# 绘制交叉验证结果\nggplot(knn_fit2)\n\n\n\n\n\n\n\n\n查看结果，发现最佳模型是k=9，交叉验证的RMSE为33582.40。与上面2.7中未进行变量预处理的模型（均方根误差为43846.05）相比，特征工程将预测误差减少约10000美元。"
  },
  {
    "objectID": "bigdata/machlearn/mlbasics.html#参考书籍",
    "href": "bigdata/machlearn/mlbasics.html#参考书籍",
    "title": "机器学习基础",
    "section": "参考书籍",
    "text": "参考书籍\n\nBradley Boehmke & Brandon Greenwell，Hands-On Machine Learning with R，CRC Press, 2020.\n\nPang-Ning Tan 数据挖掘导论（第2版），机械工业出版社，2019.\n\nIan Foster等 Big Data and Social Science: Data Science Methods and Tools for Research and Practice, CRC Press, 2021."
  },
  {
    "objectID": "bigdata/machlearn/mltrees.html#参考书籍",
    "href": "bigdata/machlearn/mltrees.html#参考书籍",
    "title": "有监督学习：决策树",
    "section": "参考书籍",
    "text": "参考书籍\n\nBradley Boehmke & Brandon Greenwell，Hands-On Machine Learning with R，CRC Press, 2020.\n\nPang-Ning Tan 数据挖掘导论（第2版），机械工业出版社，2019.\n\nIan Foster等 Big Data and Social Science: Data Science Methods and Tools for Research and Practice, CRC Press, 2021."
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html",
    "href": "bigdata/spatial/spatialmodels.html",
    "title": "空间数据分析模型",
    "section": "",
    "text": "本部分概述空间数据类型、空间数据检索、操作和可视化的 R 包，解释空间统计的理论概念，展示如何在模拟、描述和分析区域数据、地理统计数据和点模式数据，以及介绍如何应用空间计量模型。"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#为什么要使用空间计量模型",
    "href": "bigdata/spatial/spatialmodels.html#为什么要使用空间计量模型",
    "title": "空间数据分析模型",
    "section": "为什么要使用空间计量模型？",
    "text": "为什么要使用空间计量模型？\n\n地理位置和空间结构重要，存在空间相关性\n\n传统计量经济模型假设观测单位独立，忽略空间互动，可能导致估计偏差和不一致\n\n需要建模空间依赖性，以获得有效推断结果"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#理论基础",
    "href": "bigdata/spatial/spatialmodels.html#理论基础",
    "title": "空间数据分析模型",
    "section": "理论基础",
    "text": "理论基础\n\n地理学第一定律（Tobler，1970）：“一切事物都是相关的，但近的事物比远的更相关。”\n\n溢出效应：一个地区的结果受邻近地区影响\n\n战略相互依赖：个体根据邻居行为反应（如税收竞争）\n\n遗漏变量偏差：空间相关但不可观察的因素"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#空间过程与模型匹配",
    "href": "bigdata/spatial/spatialmodels.html#空间过程与模型匹配",
    "title": "空间数据分析模型",
    "section": "空间过程与模型匹配",
    "text": "空间过程与模型匹配\n\n\n\n动机\n含义\n模型\n\n\n\n\ni.i.d.假设被破坏\n空间自相关误差项\nSEM\n\n\n溢出效应\n滞后因变量起作用\nSAR\n\n\n战略互动\n内生空间依赖\nSAR、SDM\n\n\n遗漏变量\n相关误差项\nSEM\n\n\n政策扩散\n间接/溢出效应\nSDM"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#实证动因---实际案例",
    "href": "bigdata/spatial/spatialmodels.html#实证动因---实际案例",
    "title": "空间数据分析模型",
    "section": "实证动因 - 实际案例",
    "text": "实证动因 - 实际案例\n\n城市经济学：房价、急救响应、污染\n\n公共财政：税收模仿、支出溢出\n\n区域发展：基础设施跨区域影响\n\n流行病学：疾病传播模式\n\n忽略空间效应会低估政策影响或误判显著性水平"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#空间滞后",
    "href": "bigdata/spatial/spatialmodels.html#空间滞后",
    "title": "空间数据分析模型",
    "section": "空间滞后",
    "text": "空间滞后\n\n空间滞后是时间序列相关性的空间类比，但更为复杂。\n\n主要区别：\n\n空间滞后是多方向关系，而时间序列相关性是单方向（只能过去影响未来）。\n\n空间滞后会消耗自由度，而时间序列相关性不会。\n\n空间滞后使用预定义的空间权重矩阵（W），而时间序列相关性没有。\n\n空间滞后性可比较不同的W矩阵以获得稳健性结果。\n\n\n空间异质性？？"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#何时考虑空间回归",
    "href": "bigdata/spatial/spatialmodels.html#何时考虑空间回归",
    "title": "空间数据分析模型",
    "section": "何时考虑空间回归？",
    "text": "何时考虑空间回归？\n\n每个观测值对应一个空间位置或区域\n\n一定要进行探索性空间数据分析以识别空间相关性"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#空间自相关指标",
    "href": "bigdata/spatial/spatialmodels.html#空间自相关指标",
    "title": "空间数据分析模型",
    "section": "空间（自）相关指标",
    "text": "空间（自）相关指标\n\n全局莫兰指数（Global Moran’s I）\n\n为整个研究区域提供一个值\n取值范围：-1 到 +1\n0 表示完全随机\n-1 表示完全分散（负相关，高-低/低-高）\n+1 表示完全聚集（正相关，高-高/低-低）\n\n示例：莫兰指数 = 0.3699\n局部空间自相关（LISA（Local Indicators of Spatial Autocorrelation），Anselin, 1995）\n\n为每个空间单位提供一个值\n所有观测值的LISA总和与全局空间关联指标成比例\n聚集类型：高-高 或 低-低\n异常值：高-低 或 低-高\n\n全局与局部统计可扩展到双变量空间相关\n\n双变量全局莫兰指数：一个单位中的变量是否与邻近单位的另一个变量在全局范围内空间相关\n双变量局部莫兰指数：一个单位中的变量是否与邻近单位的另一个变量在局部范围内空间聚集\n与皮尔逊相关系数不同，不仅考虑数值相关性，还考虑地理接近性\n\n示例：1960年凶杀案与失业率（全局莫兰指数，未显著，2053年）"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#空间权重矩阵w",
    "href": "bigdata/spatial/spatialmodels.html#空间权重矩阵w",
    "title": "空间数据分析模型",
    "section": "空间权重矩阵（W）",
    "text": "空间权重矩阵（W）\n\n描述空间单位间的结构或连通性\n\n常为n×n稀疏矩阵，行标准化（行和为1）\n\n\n\n\n空间权重矩阵\n\n\n\n连通型矩阵\n\nRook邻接：共享边界的邻居\nQueen邻接：共享边界或角落的邻居\n示例：中心单元（E）的邻居\n\nRook：B、D、F、H\nQueen：A、B、C、D、F、G、H、I\n\n\n\n\n\n距离型矩阵：\n\n反距离权重：1/d^2\n\n距离带法：指定范围内为邻居\n\nK近邻：最近的K个单位为邻居\n\n\n\n非地理型矩阵：\n\n社交连接：谁与谁在社交网络中互动\n经济相似性：哪些公司属于同一行业或具有相似财务特征\n功能关系：组织内哪些部门在项目中合作\n共享属性：哪些个体具有相似的年龄、收入或政治观点"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#如何选择-w",
    "href": "bigdata/spatial/spatialmodels.html#如何选择-w",
    "title": "空间数据分析模型",
    "section": "如何选择 W？",
    "text": "如何选择 W？\n\nW矩阵的设定具有一定的主观性\n\n不设W（即设为0）意味着假设空间独立，反而更强\n\n不同W可能导致参数估计差异（尤其在早期文献中）\n\n实证研究会有许多不同的矩阵设定，通过敏感性分析消除疑虑"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#shapefile-简介",
    "href": "bigdata/spatial/spatialmodels.html#shapefile-简介",
    "title": "空间数据分析模型",
    "section": "Shapefile 简介",
    "text": "Shapefile 简介\n\nESRI开发的地理矢量数据格式\n\n常见文件包括：.shp, .shx, .dbf 等"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#案例数据",
    "href": "bigdata/spatial/spatialmodels.html#案例数据",
    "title": "空间数据分析模型",
    "section": "案例数据",
    "text": "案例数据\n\n数据集：NC（spData 或 spdep 包）\n\n范围：北卡罗来纳州100个县\n\n变量：\nBIR74, BIR79：1974年和1979年的出生数\nSID74, SID79：1974年和1979年的婴儿猝死数\nNWBIR74, NWBIR79：1974年和1979年非白人母亲的出生数\nNAME, FIPS, CNTY_ID：县标识符和名称\n来源与原始出版：\n\n原始研究：Cressie, Noel. (1993). Statistics for Spatial Data. Wiley.\n由Luc Anselin等人编译，最初用于空间计量建模\n参考文献：Bivand, Roger S., Edzer J. Pebesma, and Virgilio Gómez-Rubio (2008, 2013). Applied Spatial Data Analysis with R.\n\n读取数据与呈现\n\n\nlibrary(sf)\nlibrary(spdep)\nlibrary(ggplot2)\nlibrary(leaflet)\nlibrary(spatialreg)\nlibrary(lmtest)\nlibrary(stargazer)\nlibrary(gt)\nlibrary(dplyr)\n# 载入shp文件\nnc &lt;- st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\n\nReading layer `nc' from data source \n  `/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/sf/shape/nc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n\n# 将空间数据转换为WGS84经纬度坐标系，便于与其他通用地理数据或工具（如Leaflet、ggplot2等）兼容\nnc &lt;- st_transform(nc, crs = 4326)\n\n# 可视化\nggplot(nc) +\n  geom_sf(fill = \"lightblue\", color = \"white\") +\n  theme_minimal() +\n  labs(title = \"North Carolina Counties\")\n\n\n\n\n\n\n\n# 采用leaflet包创建交互式地图\npal &lt;- colorNumeric(\"YlGnBu\", domain = NULL)  # 可根据需要定义变量\n\nleaflet(nc) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~pal(1),  # 可根据变量选定颜色\n    fillOpacity = 0.6,\n    color = \"black\",\n    weight = 1,\n    popup = ~paste(\"ID:\", row.names(nc))\n  )\n\n\n\n\n\n\n出生率示意图\n\n\n# 采用ggplot2绘制静态地图\n\nggplot(nc) +\n  geom_sf(aes(fill = BIR74), color = \"white\") +\n  scale_fill_viridis_c(option = \"C\", name = \"Birth Rate\") +\n  theme_minimal() +\n  labs(title = \"Choropleth of Birth Rate 1974 (Static)\")\n\n\n\n\n\n\n\n# 采取leaflet绘制交互式地图\npal &lt;- colorNumeric(palette = \"YlOrBr\", domain = nc$BIR74)\n\nleaflet(nc) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~pal(BIR74),\n    fillOpacity = 0.7,\n    color = \"black\",\n    weight = 1,\n    popup = ~paste(\"Birth Rate:\", BIR74)\n  ) %&gt;%\n  addLegend(\"bottomright\", pal = pal, values = ~BIR74,\n            title = \"Birth Rate\")\n\n\n\n\n\n\n空间连通性\n\n\n# 创建连通加权矩阵（queen）\nnb_contig &lt;- poly2nb(nc) # 从多边形生成邻居列表\nlistw_contig &lt;- nb2listw(nb_contig, style = \"W\") # 添加按行标准化的权重\n\n# 可视化连通性\ncentroids &lt;- st_centroid(st_geometry(nc))\ncoords &lt;- st_coordinates(centroids) # 获取地理单元的中心点及其坐标\n\nnb_lines &lt;- nb2lines(nb_contig, coords = coords, as_sf = TRUE)\nst_crs(nb_lines) &lt;- st_crs(nc)  # 将邻居列表转化为空间线条数据框，并附加坐标系\n\n# 呈现邻居连通性\nggplot() +\n  geom_sf(data = nc, fill = \"grey95\", color = \"black\") +\n  geom_sf(data = nb_lines, color = \"red\", size = 0.5, alpha = 0.6) +\n  theme_minimal() +\n  labs(title = \"Spatial Contiguity Links\")\n\n\n\n\n\n\n\n\n\n全局莫兰检验和呈现\n\n\n# 莫兰检验和呈现\nmoran.test(nc$BIR74, listw = listw_contig)\n\n\n    Moran I test under randomisation\n\ndata:  nc$BIR74  \nweights: listw_contig    \n\nMoran I statistic standard deviate = 2.4055, p-value = 0.008074\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.139319332      -0.010101010       0.003858258 \n\nmoran.plot(nc$BIR74, listw = listw_contig,\n           labels = FALSE, xlab = \"Birth Rate\",\n           ylab = \"Spatial Lag of Birth Rate\",\n           main = \"Moran Scatterplot of Birth Rate\")\n\n\n\n\n\n\n\n\n\n局部空间相关指数（LISA）的呈现\n\n\n# 利用空间权重矩阵，获取变量的空间滞后项\nnc$lag_BIR &lt;- lag.listw(listw_contig, nc$BIR74)\n\n# 基于空间权重，分别计算每个地理单元局部莫兰指数\nlocal_moran &lt;- localmoran(nc$BIR74, listw=listw_contig)\n\n# 添加局部莫兰指数的结果，包括指数统计量、标准差、显著性水平\nnc$Ii     &lt;- local_moran[, \"Ii\"]\nnc$Z_Ii   &lt;- local_moran[, \"Z.Ii\"]\nnc$p_Ii   &lt;- local_moran[, \"Pr(z != E(Ii))\"] \n\n# 划分LISA集合的类别\n# Global means\nmean_BIR &lt;- mean(nc$BIR74, na.rm = TRUE)\nmean_lag &lt;- mean(nc$lag_BIR, na.rm = TRUE)\nnc$cluster &lt;- factor(\n  ifelse(nc$BIR74 &gt; mean_BIR & nc$lag_BIR &gt; mean_lag & nc$p_Ii &lt;= 0.05, \"High-High\",\n         ifelse(nc$BIR74 &lt; mean_BIR & nc$lag_BIR &lt; mean_lag & nc$p_Ii &lt;= 0.05, \"Low-Low\",\n                ifelse(nc$BIR74 &gt; mean_BIR & nc$lag_BIR &lt; mean_lag & nc$p_Ii &lt;= 0.05, \"High-Low\",\n                       ifelse(nc$BIR74 &lt; mean_BIR & nc$lag_BIR &gt; mean_lag & nc$p_Ii &lt;= 0.05, \"Low-High\",\n                              \"Not Significant\")))),\n  levels = c(\"High-High\", \"Low-Low\", \"High-Low\", \"Low-High\", \"Not Significant\")\n)\n\n# Optional: Make sure 'cluster' is a factor with proper levels\nnc$cluster &lt;- factor(nc$cluster, levels = c(\"High-High\", \"Low-Low\", \"High-Low\", \"Low-High\", \"Not significant\"))\n\n# Define custom colors for each cluster type\ncluster_colors &lt;- c(\n  \"High-High\" = \"red\",\n  \"Low-Low\" = \"blue\",\n  \"High-Low\" = \"orange\",\n  \"Low-High\" = \"lightblue\",\n  \"Not significant\" = \"lightgray\"\n)\n\n# Plot using ggplot\nggplot(nc) +\n  geom_sf(aes(fill = cluster), color = \"white\", size = 0.2) +\n  scale_fill_manual(values = cluster_colors, name = \"LISA Cluster\") +\n  labs(title = \"LISA Cluster Map: BIR74\") +\n  theme_minimal()"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#基本模型规格",
    "href": "bigdata/spatial/spatialmodels.html#基本模型规格",
    "title": "空间数据分析模型",
    "section": "基本模型规格",
    "text": "基本模型规格\n\n空间误差模型（SEM）：Y = α + Xβ + μ, μ = ρWμ + ε\n空间自回归模型（SAR）：Y = α + ρWY + Xβ + ε\n空间杜宾模型（SDM）：Y = α + ρWY + Xβ + WXβ + ε\n模型嵌套关系：\n\nSDM嵌套SAR和SEM模型\n使用测试程序指导简化\nSDM是最通用的（灵活的）模型"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#ols残差的空间依赖性测试",
    "href": "bigdata/spatial/spatialmodels.html#ols残差的空间依赖性测试",
    "title": "空间数据分析模型",
    "section": "OLS残差的空间依赖性测试",
    "text": "OLS残差的空间依赖性测试\n\n零假设：无空间依赖性\n两种变体：\n\nLM-lag：测试SAR模型\nLM-error：测试SEM模型\n\n当两者均显著时，使用稳健LM测试"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#似然比检验likelihood-ratio-test",
    "href": "bigdata/spatial/spatialmodels.html#似然比检验likelihood-ratio-test",
    "title": "空间数据分析模型",
    "section": "似然比检验（Likelihood Ratio Test）",
    "text": "似然比检验（Likelihood Ratio Test）\n\n（内容被截断，可能包含大量重复的“2”）"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#空间参数估计ρ-和-λ",
    "href": "bigdata/spatial/spatialmodels.html#空间参数估计ρ-和-λ",
    "title": "空间数据分析模型",
    "section": "空间参数估计（ρ 和 λ）",
    "text": "空间参数估计（ρ 和 λ）\n\n示例：p = 1, p = 0.5, p = 0.1"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#sdm和sar中的直接与间接效应",
    "href": "bigdata/spatial/spatialmodels.html#sdm和sar中的直接与间接效应",
    "title": "空间数据分析模型",
    "section": "SDM和SAR中的直接与间接效应",
    "text": "SDM和SAR中的直接与间接效应\n\n空间滞后在右侧时，估计系数β不能直接解释为“影响”\n（SAR模型公式被截断）"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#空间矩阵",
    "href": "bigdata/spatial/spatialmodels.html#空间矩阵",
    "title": "空间数据分析模型",
    "section": "空间矩阵",
    "text": "空间矩阵\n\n矩阵是对角矩阵 (I - ρW)^(-1)"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#建议",
    "href": "bigdata/spatial/spatialmodels.html#建议",
    "title": "空间数据分析模型",
    "section": "建议",
    "text": "建议\n\n始终测试空间依赖性\n如有疑问，优先选择SDM模型，然后测试简化为SAR或SEM\n当模型不明确时，使用稳健LM测试\n谨慎解释SDM效应"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#空间面板",
    "href": "bigdata/spatial/spatialmodels.html#空间面板",
    "title": "空间数据分析模型",
    "section": "空间面板",
    "text": "空间面板\n\n双向固定效应：时间与空间\n多种空间模型规格：\n\n参考文献：Cheng and Guo (2021); Han, Xiong, Cheng, and Guo (2022); Guo and Cheng (2018)\n\n动态空间面板：\n\n因变量不仅在时间上相关，还在空间上相关\n参考文献：Cheng, Guo, and Liu (2020)\n\n空间Probit模型：\n\n二元或多项因变量\n\n空间多层次/层级模型：\n\n嵌套结构（如城市嵌入省份）\n\n社交网络分析中的空间模型"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#基本空间模型",
    "href": "bigdata/spatial/spatialmodels.html#基本空间模型",
    "title": "空间数据分析模型",
    "section": "基本空间模型",
    "text": "基本空间模型\n\nOLS：Y = α + Xβ + ε\n\nSEM：Y = α + Xβ + μ，μ = ρWμ + ε\n\nSAR：Y = α + ρWY + Xβ + ε\n\nSDM：Y = α + ρWY + Xβ + WXθ + ε\n\nSDM 是最通用的模型，嵌套 SAR 与 SEM"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#模型选择流程",
    "href": "bigdata/spatial/spatialmodels.html#模型选择流程",
    "title": "空间数据分析模型",
    "section": "模型选择流程",
    "text": "模型选择流程\n\n从具体到一般：\n\n拟合OLS\n对残差进行LM检验\n\nLM-lag 检测SAR\nLM-error 检测SEM\n若均显著，用R-LM检验\n\n\n若确认存在空间依赖，拟合 SDM\n\n使用 LR 检验比较 SDM 与 SAR/SEM"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#sdm-与-sar-中的效应",
    "href": "bigdata/spatial/spatialmodels.html#sdm-与-sar-中的效应",
    "title": "空间数据分析模型",
    "section": "SDM 与 SAR 中的效应",
    "text": "SDM 与 SAR 中的效应\n\nSAR 中 β 不能直接解释为边际效应\n\n空间反馈机制需通过逆矩阵表示\n\nLeSage & Pace（2014）：即使W不同，最终的效应矩阵（I - ρW）⁻¹ 乘以 β 很稳定\n\n\n效应类型：\n\n直接效应：对自身的平均影响（主对角线均值）\n\n总效应：每行和的平均值\n\n间接效应：总效应 - 直接效应"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#实用建议",
    "href": "bigdata/spatial/spatialmodels.html#实用建议",
    "title": "空间数据分析模型",
    "section": "实用建议",
    "text": "实用建议\n\n一定要检测空间依赖性\n\n若不确定，先用 SDM，再通过检验简化\n\n解释 SDM 效应时要小心"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#拓展方向",
    "href": "bigdata/spatial/spatialmodels.html#拓展方向",
    "title": "空间数据分析模型",
    "section": "拓展方向",
    "text": "拓展方向\n\n空间面板模型（双向固定效应）\n\n动态空间面板\n\n空间 Probit（二值/多项）模型\n\n多层/层级空间模型\n\n社会网络中的空间分析"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#推荐文献",
    "href": "bigdata/spatial/spatialmodels.html#推荐文献",
    "title": "空间数据分析模型",
    "section": "推荐文献",
    "text": "推荐文献\n\nMoraga, Paula. (2023). Spatial Statistics for Data Science: Theory and Practice with R. Chapman & Hall/CRC Data Science Series. ISBN 9781032633510\n\nPebesma, E.; Bivand, R. (2023). Spatial Data Science: With Applications in R (1st ed.). 314 pages. Chapman and Hall/CRC, Boca Raton. https://doi.org/10.1201/9780429459016\n\nAnselin (1988). Spatial Econometrics: Methods and Models\n\nLeSage & Pace (2009). Introduction to Spatial Econometrics\n\nLeSage & Pace (2014). https://doi.org/10.3390/econometrics2040217\n\nBivand et al. (2013). Applied Spatial Data Analysis with R\n\nCheng & Guo (2021). https://doi.org/10.1080/03003930.2020.1794845\n\nGuo & Cheng (2017). https://doi.org/10.52324/001c.8009"
  },
  {
    "objectID": "bigdata/spatial/spatialmodels.html#有问题或评论",
    "href": "bigdata/spatial/spatialmodels.html#有问题或评论",
    "title": "空间数据分析模型",
    "section": "有问题或评论？",
    "text": "有问题或评论？\n感谢您的参与！\n郭海老师提到的用到spatial analysis的论文。供大家参考。https://onlinelibrary.wiley.com/doi/abs/10.1111/pbaf.12160 https://www.tandfonline.com/doi/abs/10.1080/23276665.2022.2071305"
  },
  {
    "objectID": "bigdata/spatial/spatialdata.html",
    "href": "bigdata/spatial/spatialdata.html",
    "title": "空间数据分析基础",
    "section": "",
    "text": "本部分概述空间数据类型、R语言中的空间数据对象、空间数据的处理、地理数据可视化、空间大数据处理和开放空间数据下载等内容，解释空间分析的基础理论概念。"
  },
  {
    "objectID": "bigdata/spatial/spatialdata.html#区域数据",
    "href": "bigdata/spatial/spatialdata.html#区域数据",
    "title": "空间数据分析基础",
    "section": "1.1 区域数据",
    "text": "1.1 区域数据\n区域数据是一个固定的集合，由规则或不规则的区域单元组成，在这些单元中观测各类变量。例如，某个区域的温度、降水、植被指数、空气污染浓度、流行病发病率等。\n1974 年美国北卡罗来纳州各县的婴儿猝死综合症数量，数据来自 sf 包。\n\nlibrary(sf)\nlibrary(mapview)\nd &lt;- st_read(system.file(\"shape/nc.shp\", package = \"sf\"),\n             quiet = TRUE)\nmapview(d, zcol = \"SID74\")\n\n\n\n\n\n1980 年俄亥俄州哥伦布市各社区的家庭收入（以千美元为单位），数据来自 spData 包（Bivand, Nowosad, and Lovelace 2022）。\n\nlibrary(spData)\nlibrary(ggplot2)\ndata &lt;- st_read(system.file(\"shapes/columbus.gpkg\",\n                         package = \"spData\"), quiet = TRUE)\nst_crs(data) &lt;- 4326\nggplot(data) + geom_sf(aes(fill = INC))\n\n\n\n\n\n\n\n\n卢森堡栅格单元的高程数据，来自 terra 包（Hijmans 2022）。\n\nlibrary(terra)\nd &lt;- rast(system.file(\"ex/elev.tif\", package = \"terra\"))\nplot(d)"
  },
  {
    "objectID": "bigdata/spatial/spatialdata.html#地统计数据",
    "href": "bigdata/spatial/spatialdata.html#地统计数据",
    "title": "空间数据分析基础",
    "section": "1.2 地统计数据",
    "text": "1.2 地统计数据\n地统计数据是一个连续的固定子集，空间索引在空间中连续改变位置，因此数据可以在任何地方被观测。地统计通常采用在已知空间位置观测到的数据来预测未采样位置的数值。例如，使用多个监测站的空气污染测量数据，结合空间自相关和其他已知预测结果的因素，来预测其他位置的空气污染。地统计的目的包括估计未采样点的数值、量化空间自相关性、预测空间变量的分布。\n荷兰斯特因附近默兹河洪泛区采样点处的表层土壤铅浓度（每千克土壤的毫克数），数据来自 sp 包。\n\nlibrary(sp)\nlibrary(sf)\nlibrary(mapview)\n\ndata(meuse)\nmeuse &lt;- st_as_sf(meuse, coords = c(\"x\", \"y\"), crs = 28992)\nmapview(meuse, zcol = \"lead\",  map.types = \"CartoDB.Voyager\")\n\n\n\n\n\n2017 年希腊雅典公寓单位面积价格（欧元/平方米），数据来自 spData 包。\n\nlibrary(spData)\nmapview(properties, zcol = \"prpsqm\")\n\n\n\n\n\n津巴布韦特定地点的疟疾流行率，数据来自 malariaAtlas 包。流行率计算为每个地点疟疾阳性个体数除以检查个体数。\n\nlibrary(malariaAtlas)\nd &lt;- getPR(country = \"Zimbabwe\", species = \"BOTH\")\nggplot2::autoplot(d)"
  },
  {
    "objectID": "bigdata/spatial/spatialdata.html#点模式",
    "href": "bigdata/spatial/spatialdata.html#点模式",
    "title": "空间数据分析基础",
    "section": "1.3 点模式",
    "text": "1.3 点模式\n在点模式中，位置索引集给出了空间点模式所代表随机事件的位置，可能等于 1，表示事件发生，或者是能提供额外信息的随机数值。\n点模式分析的目的包括研究点的空间分布模式（聚集、随机、均匀）、检验点事件是否受环境影响、识别热点区域或空间聚类。例如，点模式分析包括森林中的火灾位置（González and Moraga 2022）或病患的居住地（Moraga and Montes 2011）产生的潜在空间过程，并评估这种模式是否表现出随机性、聚集或某种规律性。\n1998 年至 2007 年间西班牙卡斯蒂利亚-拉曼恰的火灾，包含在 spatstat 包的 clmfires 数据中。数据 clmfires 是一个标记点模式，包含每个火灾的信息。\n\nlibrary(spatstat)\nplot(clmfires, use.marks = FALSE, pch = \".\")\n\n\n\n\n\n\n\n\n1987 年至 1994 年间英格兰东北部收集的 761 例原发胆汁性肝硬化病例和 30210 名对照者的空间位置，代表了风险人群，数据来自 sparr 包的 pbc 数据。\n\nlibrary(sparr)\ndata(pbc)\nplot(unmark(pbc[which(pbc$marks == \"case\"), ]), main = \"cases\")\naxis(1)\naxis(2)\ntitle(xlab = \"east\", ylab = \"north\")\n\n\n\n\n\n\n\nplot(unmark(pbc[which(pbc$marks == \"control\"), ]),\n     pch = 3, main = \"controls\")\naxis(1)\naxis(2)\ntitle(xlab = \"east\", ylab = \"north\")"
  },
  {
    "objectID": "bigdata/spatial/spatialdata.html#时空数据和数据立方体",
    "href": "bigdata/spatial/spatialdata.html#时空数据和数据立方体",
    "title": "空间数据分析基础",
    "section": "1.4 时空数据和数据立方体",
    "text": "1.4 时空数据和数据立方体\n当数据同时具有空间和时间信息则为时空数据，可以看做是时间上的聚合信息或者是时空过程中的时间快照。\n\n\n\n\n\n\n\n\n\n如果栅格数据是带有不同光谱波段的三维数据，加上时间维度就成为了四维的数据立方体。"
  },
  {
    "objectID": "bigdata/spatial/spatialdata.html#空间函数数据",
    "href": "bigdata/spatial/spatialdata.html#空间函数数据",
    "title": "空间数据分析基础",
    "section": "1.5 空间函数数据",
    "text": "1.5 空间函数数据\n空间数据类型（区域、地统计和点模式）与随机函数结合便为空间函数数据（Spatial Functional Data），它是空间统计与函数型数据分析（Functional Data Analysis, FDA）交叉领域的研究对象，其核心特点是同时具备空间依赖性和函数型特征。例如气象站记录的每日温度曲线是一个关于时间的函数，并且邻近气象站的温度曲线变化模式可能高度相关。\n来自 geoFourierFDA 包（Sassi 2021）的函数地统计数据，表示在 35 个加拿大气象站平均 30 年的每日温度，\\({\\boldsymbol{\\chi_{s_i}}: i =1,\\ldots,35 }\\)。\n\n# install.packages(\"devtools\") # 无法正常安装包，从源文件安装\n# devtools::install_github(\"ropensci/rnaturalearthhires\")\nlibrary(sf)\nlibrary(geoFourierFDA)\nlibrary(rnaturalearth)\n\n# 绘制加拿大地图\nmap &lt;- rnaturalearth::ne_states(\"Canada\", returnclass = \"sf\")\n\n# 气象站坐标\nd &lt;- data.frame(canada$m_coord)\nd$location &lt;- attr(canada$m_coord, \"dimnames\")[[1]]\nd &lt;- st_as_sf(d, coords = c(\"W.longitude\", \"N.latitude\"))\nst_crs(d) &lt;- 4326\n\n# 绘制加拿大地图和气象站位置\nggplot(map) + geom_sf() + geom_sf(data = d, size = 6) +\n  geom_sf_label(data = d, aes(label = location), nudge_y = 2)\n\n\n\n\n\n\n\n# 各气象站随时间变化的温度\nd &lt;- data.frame(canada$m_data)\nd$time &lt;- 1:nrow(d)\n\n# 将数据 d 从宽格式转换为长格式\n# cols：需要转换为长格式的列\n# names_to：新列的名称，包含原始数据的列名\n# values_to：新列的名称，包含原始数据的值\ndf &lt;- tidyr::pivot_longer(data = d,\ncols = names(d)[-which(names(d) == \"time\")],\nnames_to = \"variable\", values_to = \"value\")\n\n# 绘制各气象站随时间变化的温度\nggplot(df, aes(x = time, y = value)) +\n  geom_line(aes(color = variable))"
  },
  {
    "objectID": "bigdata/spatial/spatialdata.html#流动数据",
    "href": "bigdata/spatial/spatialdata.html#流动数据",
    "title": "空间数据分析基础",
    "section": "1.6 流动数据",
    "text": "1.6 流动数据\n除了三种经典的空间数据类型（即区域、地统计和点模式）外，还有包含个体或其他对象在空间移动的流动数据（Mahmood et al. 2022）。\n来自 epiflows 包（Piatkowski et al. 2018; Moraga et al. 2019）的流动数据Brazil_epiflows，包含巴西各州与其他地点之间的旅行者数量。可以根据地理位置之间的流动预测和可视化传染病的传播。\n\nlibrary(\"epiflows\")\ndata(\"Brazil_epiflows\")\n\nloc &lt;- merge(x = YF_locations, y = YF_coordinates,\nby.x = \"location_code\", by.y = \"id\", sort = FALSE)\n\nef &lt;- make_epiflows(flows = YF_flows, locations = loc,\n                    coordinates = c(\"lon\", \"lat\"),\n                    pop_size = \"location_population\",\n                    duration_stay = \"length_of_stay\",\n                    num_cases = \"num_cases_time_window\",\n                    first_date = \"first_date_cases\",\n                    last_date = \"last_date_cases\")\nvis_epiflows(ef)\n\n\n\n\nmap_epiflows(ef)"
  },
  {
    "objectID": "bigdata/spatial/spatialdata.html#向量数据",
    "href": "bigdata/spatial/spatialdata.html#向量数据",
    "title": "空间数据分析基础",
    "section": "2.1 向量数据",
    "text": "2.1 向量数据\nsf 包处理向量数据，用于表示点、线和多边形。向量数据可以用点来表示公共服务部门（医院或学校）的位置，用线来表示道路或河流，用多边形表示省份和市域行政区划的边界。同时，这些向量数据可以关联信息，例如学校的在校学生人数或行政区划内的人口数量。\n向量数据通常使用shapefile的存储格式。需要注意的是，Shape文件不是单个文件，而是一组相关文件的集合。Shape文件包含三个必备文件，即：.shp 文件包含几何数据，.shx 文件是几何数据的位置索引，允许在 .shp 文件中进行查找，.dbf 文件存储每个形状的属性。其他可能包含的文件有：.prj 文件是描述投影的纯文本文件，.sbn 和 .sbx 文件是几何数据的空间索引，.shp.xml 文件包含 XML 格式的空间元数据。因此，在使用shape文件时，需获取所有的组成文件集合，而不仅仅是包含几何数据的 .shp 文件。\nsf 包的 st_read() 函数可用于读取shape文件。下面读取 sf 包中自带的美国北卡州各郡的shape文件。\n\nlibrary(sf)\n# 获取自带文件的路径\npathshp &lt;- system.file(\"shape/nc.shp\", package = \"sf\")\n# 读取shape文件，但不显示文件详细信息\nmap &lt;- st_read(pathshp, quiet = TRUE)\n# 检查shape数据类型与数据结构\nclass(map)\n\n[1] \"sf\"         \"data.frame\"\n\nhead(map)\n\nSimple feature collection with 6 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -81.74107 ymin: 36.07282 xmax: -75.77316 ymax: 36.58965\nGeodetic CRS:  NAD27\n   AREA PERIMETER CNTY_ CNTY_ID        NAME  FIPS FIPSNO CRESS_ID BIR74 SID74\n1 0.114     1.442  1825    1825        Ashe 37009  37009        5  1091     1\n2 0.061     1.231  1827    1827   Alleghany 37005  37005        3   487     0\n3 0.143     1.630  1828    1828       Surry 37171  37171       86  3188     5\n4 0.070     2.968  1831    1831   Currituck 37053  37053       27   508     1\n5 0.153     2.206  1832    1832 Northampton 37131  37131       66  1421     9\n6 0.097     1.670  1833    1833    Hertford 37091  37091       46  1452     7\n  NWBIR74 BIR79 SID79 NWBIR79                       geometry\n1      10  1364     0      19 MULTIPOLYGON (((-81.47276 3...\n2      10   542     3      12 MULTIPOLYGON (((-81.23989 3...\n3     208  3616     6     260 MULTIPOLYGON (((-80.45634 3...\n4     123   830     2     145 MULTIPOLYGON (((-76.00897 3...\n5    1066  1606     3    1197 MULTIPOLYGON (((-77.21767 3...\n6     954  1838     5    1237 MULTIPOLYGON (((-76.74506 3...\n\n# 地图展示数据的第一个属性值\nplot(map[1])"
  },
  {
    "objectID": "bigdata/spatial/spatialdata.html#栅格数据",
    "href": "bigdata/spatial/spatialdata.html#栅格数据",
    "title": "空间数据分析基础",
    "section": "2.2 栅格数据",
    "text": "2.2 栅格数据\n栅格数据（也称为网格数据）是一种空间数据结构，将研究区域划分为大小相同的矩形单元格（称为单元或像素），并可以为每个单元格存储一个或多个值。栅格数据用于表示空间连续现象，例如海拔、温度或空气污染值。\n栅格数据通常以 GeoTIFF 格式存储，文件扩展名为 .tif。可使用 terra::rast() 函数读取 terra 包中的 elev.tif 文件，展示卢森堡的海拔。\n\nlibrary(terra)\npathraster &lt;- system.file(\"ex/elev.tif\", package = \"terra\")\nr &lt;- terra::rast(pathraster)\nr\n\nclass       : SpatRaster \nsize        : 90, 95, 1  (nrow, ncol, nlyr)\nresolution  : 0.008333333, 0.008333333  (x, y)\nextent      : 5.741667, 6.533333, 49.44167, 50.19167  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : elev.tif \nname        : elevation \nmin value   :       141 \nmax value   :       547 \n\nplot(r)"
  },
  {
    "objectID": "bigdata/spatial/spatialdata.html#坐标参考系统",
    "href": "bigdata/spatial/spatialdata.html#坐标参考系统",
    "title": "空间数据分析基础",
    "section": "2.3 坐标参考系统",
    "text": "2.3 坐标参考系统\n空间数据的坐标参考系统（CRS）指定了空间坐标的原点和测量单位。CRS 对于空间数据的操作、分析和可视化非常重要，允许通过将多个数据转换为相同的 CRS 来处理它们。地球上的位置可以使用未投影（也称为地理）或投影的 CRS 进行参照。未投影或地理 CRS 使用纬度和经度值表示地球三维椭球表面上的位置（例如厦门市大致位于东经118度，北纬24.5度）。\n投影 CRS 使用笛卡尔坐标在二维平面上展示地球上的位置。所有的投影坐标参考系都会以某种方式扭曲地球表面，无法同时保留面积、方向、形状和距离的所有属性。\n最常见的 CRS 可以通过提供 EPSG（欧洲石油勘探集团）代码或 Proj4 字符串来指定。常见的空间投影可以在 https://spatialreference.org/ref/ 找到。例如，EPSG 代码 4326 指的是 WGS84 经纬度坐标系（适用于GPS，有时被称为等经纬度投影，实际是未投影坐标系，但是绘制地图时，会自动应用简单投影或将经纬度假设为平面坐标）。而Proj4 字符串则通过一系列+号相连的键与键值对来制定投影方式的参数（例如名称、区域、基准、距离单位等）。可以使用 sf 包的 st_crs() 函数检查给定坐标系的详细信息。\n\n# 坐标系的名称\nst_crs(\"EPSG:4326\")$Name\n\n[1] \"WGS 84\"\n\n# 坐标系的投影字符串标识\nst_crs(\"EPSG:4326\")$proj4string\n\n[1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\n# 坐标系的EPSG代码\nst_crs(\"EPSG:4326\")$epsg\n\n[1] 4326\n\n\n可以使用 sf 和 terra 对坐标系进行转换。通过 st_crs(x) &lt;- value（如果 x 是 sf 对象）或 crs(r) &lt;- value（如果 r 是栅格对象）可以设置空间数据的 CRS。通过 sf::st_transform() 和 terra::project() 可以转换坐标系。\n\nlibrary(sf)\npathshp &lt;- system.file(\"shape/nc.shp\", package = \"sf\")\nmap &lt;- st_read(pathshp, quiet = TRUE)\n\n# 获取 CRS\nst_crs(map)\n\nCoordinate Reference System:\n  User input: NAD27 \n  wkt:\nGEOGCRS[\"NAD27\",\n    DATUM[\"North American Datum 1927\",\n        ELLIPSOID[\"Clarke 1866\",6378206.4,294.978698213898,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4267]]\n\n# 转换 CRS\nmap2 &lt;- st_transform(map, crs = \"EPSG:4326\")\n# 获取新的 CRS\nst_crs(map2)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        MEMBER[\"World Geodetic System 1984 (G2296)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nterra 读取并获取 CRS，转换新的 CRS类似。\n\nlibrary(terra)\npathraster &lt;- system.file(\"ex/elev.tif\", package = \"terra\")\nr &lt;- rast(pathraster)\n\n# 获取 CRS\ncrs(r)\n\n[1] \"GEOGCRS[\\\"WGS 84\\\",\\n    ENSEMBLE[\\\"World Geodetic System 1984 ensemble\\\",\\n        MEMBER[\\\"World Geodetic System 1984 (Transit)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G730)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G873)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1150)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1674)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1762)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G2139)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G2296)\\\"],\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        ENSEMBLEACCURACY[2.0]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    USAGE[\\n        SCOPE[\\\"Horizontal component of 3D system.\\\"],\\n        AREA[\\\"World.\\\"],\\n        BBOX[-90,-180,90,180]],\\n    ID[\\\"EPSG\\\",4326]]\"\n\n# 转换 CRS\nr2 &lt;- terra::project(r, \"EPSG:2169\")\n# 获取 新的CRS\ncrs(r2)\n\n[1] \"PROJCRS[\\\"LUREF / Luxembourg TM\\\",\\n    BASEGEOGCRS[\\\"LUREF\\\",\\n        DATUM[\\\"Luxembourg Reference Frame\\\",\\n            ELLIPSOID[\\\"International 1924\\\",6378388,297,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4181]],\\n    CONVERSION[\\\"Luxembourg TM\\\",\\n        METHOD[\\\"Transverse Mercator\\\",\\n            ID[\\\"EPSG\\\",9807]],\\n        PARAMETER[\\\"Latitude of natural origin\\\",49.8333333333333,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8801]],\\n        PARAMETER[\\\"Longitude of natural origin\\\",6.16666666666667,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8802]],\\n        PARAMETER[\\\"Scale factor at natural origin\\\",1,\\n            SCALEUNIT[\\\"unity\\\",1],\\n            ID[\\\"EPSG\\\",8805]],\\n        PARAMETER[\\\"False easting\\\",80000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8806]],\\n        PARAMETER[\\\"False northing\\\",100000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8807]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"northing (X)\\\",north,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        AXIS[\\\"easting (Y)\\\",east,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n    USAGE[\\n        SCOPE[\\\"Engineering survey, topographic mapping.\\\"],\\n        AREA[\\\"Luxembourg.\\\"],\\n        BBOX[49.44,5.73,50.19,6.53]],\\n    ID[\\\"EPSG\\\",2169]]\"\n\n\nR语言发展迅速，建议尽量使用新的包，旧的包一般不再维护，安装中容易出错，调试起来也比较麻烦。在 sf 包开发之前，sp 包被用来表示和处理向量空间数据。sp 以及 rgdal、rgeos 和 maptools包已不再维护并将退出使用。"
  },
  {
    "objectID": "bigdata/spatial/spatialdata.html#sf-包",
    "href": "bigdata/spatial/spatialdata.html#sf-包",
    "title": "空间数据分析基础",
    "section": "3.1 sf 包",
    "text": "3.1 sf 包\n\nsf对象\nsf 对象是一个 data.frame，包含简单特征（simple feature）（行）和属性（attribute）（列），加上一个包含每个要素（具体空间对象）几何形状信息的列表列（list-column）。\nsf 对象中包含 sf、sfc 和 sfg 等类别的对象： - sf（simple feature，简单特征）：data.frame 的每一行是一个简单特征（在GIS，尤其在ArcGIS中，称作简单要素），由属性和几何形状信息组成。\n- sfc（简单特征几何体列表列）：data.frame 的 geometry 列是一个类为 sfc 的列表列，包含每个简单特征的几何体信息（区域对应多边形的信息）。\n- sfg（simple feature geometry，简单特征几何体）：sfc 列表列的每一行对应于单个简单特征的简单特征几何体信息（sfg），例如区域对应多边形的每个点的位置。\n\n\n简单特征几何体\n简单特征几何体是用几何形状描述特征的一种方式。其主要应用是在二维空间通过点、线、多边形描绘几何形状。\n\n\n\n类型\n描述\n\n\n\n\nPOINT\n单个点几何体\n\n\nMULTIPOINT\n点集\n\n\nLINESTRING\n单一折线\n\n\nMULTILINESTRING\n折线集\n\n\nPOLYGON\n多边形\n\n\nMULTIPOLYGON\n多边形集\n\n\nGEOMETRYCOLLECTION\n以上几何体的集合\n\n\n\n\nlibrary(sf) \npar(mfrow = c(2,4))\npar(mar = c(1,1,1.2,1))\n\n# 1\np &lt;- st_point(0:1)\nplot(p, pch = 16)\ntitle(\"点\")\nbox(col = 'grey')\n\n# 2\nmp &lt;- st_multipoint(rbind(c(1,1), c(2, 2), c(4, 1), c(2, 3), c(1,4)))\nplot(mp, pch = 16)\ntitle(\"点集\")\nbox(col = 'grey')\n\n# 3\nls &lt;- st_linestring(rbind(c(1,1), c(5,5), c(5, 6), c(4, 6), c(3, 4), c(2, 3)))\nplot(ls, lwd = 2)\ntitle(\"线\")\nbox(col = 'grey')\n\n# 4\nmls &lt;- st_multilinestring(list(\n  rbind(c(1,1), c(5,5), c(5, 6), c(4, 6), c(3, 4), c(2, 3)),\n  rbind(c(3,0), c(4,1), c(2,1))))\nplot(mls, lwd = 2)\ntitle(\"线集\")\nbox(col = 'grey')\n\n# 5 polygon\npo &lt;- st_polygon(list(rbind(c(2,1), c(3,1), c(5,2), c(6,3), c(5,3), c(4,4), c(3,4), c(1,3), c(2,1)),\n    rbind(c(2,2), c(3,3), c(4,3), c(4,2), c(2,2))))\nplot(po, border = 'black', col = '#ff8888', lwd = 2)\ntitle(\"多边形\")\nbox(col = 'grey')\n\n# 6 multipolygon\nmpo &lt;- st_multipolygon(list(\n    list(rbind(c(2,1), c(3,1), c(5,2), c(6,3), c(5,3), c(4,4), c(3,4), c(1,3), c(2,1)),\n        rbind(c(2,2), c(3,3), c(4,3), c(4,2), c(2,2))),\n    list(rbind(c(3,7), c(4,7), c(5,8), c(3,9), c(2,8), c(3,7)))))\nplot(mpo, border = 'black', col = '#ff8888', lwd = 2)\ntitle(\"多边形集\")\nbox(col = 'grey')\n\n# 7 geometrycollection\ngc &lt;- st_geometrycollection(list(po, ls + c(0,5), st_point(c(2,5)), st_point(c(5,4))))\nplot(gc, border = 'black', col = '#ff6666', pch = 16, lwd = 2)\ntitle(\"几何集合\")\nbox(col = 'grey')\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n特征几何框架中有个重要的概念是空几何体，即进行几何体操作中获得的空集。例如，点和点之间的交集是空点，不重叠的多边形的交集是空多边形。这些都是空集，仅仅是维度不同。\n\n\n几何体的操作\n可以根据几何体操作的输入和输出对操作进行分类。\n根据操作对象多少，在输入方面，可以分为：\n\n\n一元（unary）：当它们作用于单个几何时\n\n二元（binary）：当它们作用于成对的几何时\n\nN元（n-ary）：当它们作用于几何集时几何体\n\n\n根据操作的输出结果，可以分为：\n\n谓词（predicates）：对属性进行判断，返回布尔值，例如比较两个几何体，返回布尔值，如st_intersects（是否相交）、st_contains（是否包含）或st_within（是否在内部）。\n\n度量（measures）：返回一个量，通常带有测量单位。例如，对单一几何体操作，返回单一值，如st_area（面积）、st_length（长度）或st_dimension（维度）。\n\n变换（transformations）：新生成的几何。例如，结合两个几何体生成新几何体，如st_intersection（交集）、st_union（并集）或st_difference（差集）。\n\n\n\n\n数据读入与选择\n数据集：NC（spData 或 spdep 包），来自美国北卡罗来纳州100个郡\n\nBIR74, BIR79：1974年和1979年的出生数\n\nSID74, SID79：1974年和1979年的婴儿猝死数\n\nNWBIR74, NWBIR79：1974年和1979年非白人母亲的出生数\n\nNAME, FIPS, CNTY_ID：县标识符和名称\n\n\nnc &lt;- st_read(pathshp, quiet=T)\nprint(nc)\n\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\nFirst 10 features:\n    AREA PERIMETER CNTY_ CNTY_ID        NAME  FIPS FIPSNO CRESS_ID BIR74 SID74\n1  0.114     1.442  1825    1825        Ashe 37009  37009        5  1091     1\n2  0.061     1.231  1827    1827   Alleghany 37005  37005        3   487     0\n3  0.143     1.630  1828    1828       Surry 37171  37171       86  3188     5\n4  0.070     2.968  1831    1831   Currituck 37053  37053       27   508     1\n5  0.153     2.206  1832    1832 Northampton 37131  37131       66  1421     9\n6  0.097     1.670  1833    1833    Hertford 37091  37091       46  1452     7\n7  0.062     1.547  1834    1834      Camden 37029  37029       15   286     0\n8  0.091     1.284  1835    1835       Gates 37073  37073       37   420     0\n9  0.118     1.421  1836    1836      Warren 37185  37185       93   968     4\n10 0.124     1.428  1837    1837      Stokes 37169  37169       85  1612     1\n   NWBIR74 BIR79 SID79 NWBIR79                       geometry\n1       10  1364     0      19 MULTIPOLYGON (((-81.47276 3...\n2       10   542     3      12 MULTIPOLYGON (((-81.23989 3...\n3      208  3616     6     260 MULTIPOLYGON (((-80.45634 3...\n4      123   830     2     145 MULTIPOLYGON (((-76.00897 3...\n5     1066  1606     3    1197 MULTIPOLYGON (((-77.21767 3...\n6      954  1838     5    1237 MULTIPOLYGON (((-76.74506 3...\n7      115   350     2     139 MULTIPOLYGON (((-76.00897 3...\n8      254   594     2     371 MULTIPOLYGON (((-76.56251 3...\n9      748  1190     2     844 MULTIPOLYGON (((-78.30876 3...\n10     160  2038     5     176 MULTIPOLYGON (((-80.02567 3...\n\n\n可以使用方括号符号来选择符合要求的要素，并使用 drop 参数来删除几何体列。\n\nnc[1, ] # 第一行\n\nSimple feature collection with 1 feature and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -81.74107 ymin: 36.23436 xmax: -81.23989 ymax: 36.58965\nGeodetic CRS:  NAD27\n   AREA PERIMETER CNTY_ CNTY_ID NAME  FIPS FIPSNO CRESS_ID BIR74 SID74 NWBIR74\n1 0.114     1.442  1825    1825 Ashe 37009  37009        5  1091     1      10\n  BIR79 SID79 NWBIR79                       geometry\n1  1364     0      19 MULTIPOLYGON (((-81.47276 3...\n\nnc[nc$NAME == \"Ashe\", ] # NAME 为 \"Ashe\" 的行\n\nSimple feature collection with 1 feature and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -81.74107 ymin: 36.23436 xmax: -81.23989 ymax: 36.58965\nGeodetic CRS:  NAD27\n   AREA PERIMETER CNTY_ CNTY_ID NAME  FIPS FIPSNO CRESS_ID BIR74 SID74 NWBIR74\n1 0.114     1.442  1825    1825 Ashe 37009  37009        5  1091     1      10\n  BIR79 SID79 NWBIR79                       geometry\n1  1364     0      19 MULTIPOLYGON (((-81.47276 3...\n\nnc[1, \"NWBIR74\"] # 第一行，名称为 NWBIR74 的列\n\nSimple feature collection with 1 feature and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -81.74107 ymin: 36.23436 xmax: -81.23989 ymax: 36.58965\nGeodetic CRS:  NAD27\n  NWBIR74                       geometry\n1      10 MULTIPOLYGON (((-81.47276 3...\n\nnc[1, \"NWBIR74\", drop = TRUE] # 删除几何体\n\n[1] 10\nattr(,\"class\")\n[1] \"numeric\"\n\n\nst_geometry() 函数可用于检索简单特征几何体列表列（sfc）。\n\n# 以缩写形式打印几何体\nst_geometry(nc)\n\nGeometry set for 100 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\nFirst 5 geometries:\n\n# 通过选择一个查看完整的几何体\nst_geometry(nc)[[1]]\n\n\n\n创建 sf 对象\n可以使用 st_sf() 函数创建 sf 对象，提供每个特征属性的 data.frame 和包含简单特征几何体信息的列表列 sfc。先创建简单特征几何体 sfg，并使用 st_sfc() 函数创建几何列表列 sfc。然后，使用 st_sf() 将包含属性的 data.frame 和几何体列表列 sfc 组合在一起。\nsfg 几何体对象可以是，POINT 类型（单个点）、MULTIPOINT 类型（点集）或 POLYGON 类型（多边形），分别可以通过 st_point()、st_multipoint() 和 st_polygon() 创建。\n\n# 单点（点作为一个向量）\np1_sfg &lt;- st_point(c(2, 2))\np2_sfg &lt;- st_point(c(2.5, 3))\n\n# 点集（点作为一个矩阵）\np &lt;- rbind(c(6, 2), c(6.1, 2.6), c(6.8, 2.5),\n           c(6.2, 1.5), c(6.8, 1.8))\nmp_sfg &lt;- st_multipoint(p)\n\n# 多边形。由形成闭合、非自相交环的点序列组成。\n# 第一个环表示外部环，\n# 零个或多个后续环表示外部环中的孔\np1 &lt;- rbind(c(10, 0), c(11, 0), c(13, 2),\n            c(12, 4), c(11, 4), c(10, 0))\np2 &lt;- rbind(c(11, 1), c(11, 2), c(12, 2), c(11, 1))\npol_sfg &lt;- st_polygon(list(p1, p2))\n\n# 创建 sf 对象\np_sfc &lt;- st_sfc(p1_sfg, p2_sfg, mp_sfg, pol_sfg)\ndf &lt;- data.frame(v1 = c(\"A\", \"B\", \"C\", \"D\"))\np_sf &lt;- st_sf(df, geometry = p_sfc)\n\n# 绘制单点、点集和多边形\nlibrary(ggplot2)\nggplot(p_sf) + geom_sf(aes(col = v1), size = 3) + theme_bw()\n\n\n\n\n\n\n\n\n\n\n编辑 sf 对象\n可以通过选择 sf 特定的行来删除一些多边形。还可以使用 st_union() 函数并设置参数 by_feature = FALSE 将所有几何形状组合在一起。使用 st_simplify() 函数简化地图的边界。\n\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(dplyr)  # 确保管道操作 `%&gt;%` 可用\n\n# 加载数据并检查 CRS\npathshp &lt;- system.file(\"shape/nc.shp\", package = \"sf\")\nmap &lt;- st_read(pathshp, quiet = TRUE) %&gt;% \n  st_transform(4326)  # 确保使用 WGS84 (EPSG:4326)\n\n# 检查 CRS\nprint(st_crs(map))  # 应为 EPSG:4326\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        MEMBER[\"World Geodetic System 1984 (G2296)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n# 删除选择的地理单元\nmap &lt;- map[-which(map$FIPS %in% c(\"37125\", \"37051\")), ]\nggplot(map) + geom_sf(aes(fill = SID79)) + coord_sf(lims_method = \"geometry_bbox\") \n\n\n\n\n\n\n\n# 组合地理单元\nggplot(st_union(map, by_feature = FALSE) %&gt;% st_sf()) + geom_sf() + coord_sf(lims_method = \"geometry_bbox\") \n\n\n\n\n\n\n\n# 简化地图边界\nggplot(st_simplify(map, dTolerance = 0.1)) + geom_sf() +coord_sf(lims_method = \"geometry_bbox\") \n\n\n\n\n\n\n\n\n\n\n将点数据转换为 sf 对象\nst_as_sf() 函数将外部对象转换为 sf 对象。\n\nlibrary(sf)\nlibrary(mapview)\n\nd &lt;- data.frame(\nplace = c(\"伦敦\", \"巴黎\", \"马德里\", \"罗马\"),\nlong = c(-0.118092, 2.349014, -3.703339, 12.496366),\nlat = c(51.509865, 48.864716, 40.416729, 41.902782),\nvalue = c(200, 300, 400, 500))\nclass(d)\n\n[1] \"data.frame\"\n\ndsf &lt;- st_as_sf(d, coords = c(\"long\", \"lat\"))\nst_crs(dsf) &lt;- 4326\nclass(dsf)\n\n[1] \"sf\"         \"data.frame\"\n\nmapview(dsf)\n\n\n\n\n\n\n\n计算多边形内的点数\nst_intersects() 函数计算 sf 对象多边形内的点数。返回的对象是一个列表，包含每个多边形中相交的特征 ID。可以继续使用 lengths() 函数计算每个特征内的点数。\n\nlibrary(sf)\nlibrary(ggplot2)\n\n# 读取shape文件\nmap &lt;- read_sf(system.file(\"shape/nc.shp\", package = \"sf\"))\n\n# 在地图上添加随机的样本点\npoints &lt;- st_sample(map, size = 100)\n\n# 绘制点所在的地图\nggplot() + geom_sf(data = map) + geom_sf(data = points)\n\n\n\n\n\n\n\n# 用地图中的特征与点进行匹配（第一个参数为地图特征对象，然后是点对象）\n# 二元谓词，返回多边形表示的郡与点对象之间是否相交的布尔值矩阵\ninter &lt;- st_intersects(map, points)\n\n# 为地图每个多边形特征（郡）添加点计数作为新的属性变量\nmap$count &lt;- lengths(inter)\n\n# 获得每个郡内样本点数量分布图\nggplot(map) + geom_sf(aes(fill = count))\n\n\n\n\n\n\n\n\n\n\n识别包含点的多边形\n也可以使用 st_intersects() 函数来获取每个点所属的多边形。\n\nlibrary(sf)\nlibrary(ggplot2)\n\n# 获取地图文件\nmap &lt;- read_sf(system.file(\"shape/nc.shp\", package = \"sf\"))\n\n# 在地图上随机添加3个点，并转化为sf对象\npoints &lt;- st_sample(map, size = 3) %&gt;% st_as_sf()\n\n# 用点对象与地图中的特征进行匹配（注意此时第一个参数为点，然后是单个特征）\n# 二元谓词，返回每个点对象与每个多边形之间是否相交的布尔值矩阵\ninter &lt;- st_intersects(points, map)\n\n# 给点对象添加对应的区域名称变量 areaname\npoints$areaname &lt;- map[unlist(inter), \"NAME\",\n                       drop = TRUE] # 删除几何体信息\npoints\n\nSimple feature collection with 3 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -81.29232 ymin: 34.85958 xmax: -78.48098 ymax: 36.42278\nGeodetic CRS:  NAD27\n                           x areaname\n1 POINT (-81.29232 36.40393)     Ashe\n2 POINT (-79.26915 36.42278)  Caswell\n3 POINT (-78.48098 34.85958)  Sampson\n\n# 展示点所在郡的名称（点是随机添加的，每次结果会不同）\nggplot(map) + geom_sf() + geom_sf(data = points) + \n  geom_sf_label(data = points,\n                aes(label = areaname), nudge_y = 0.2)\n\n\n\n\n\n\n\n\n\n\n连接地图和数据\n有时地图及其对应的数据是分开提供的，可以使用 dplyr 包的 left_join() 函数将地图和数据连接起来，创建带有数据属性的 sf 地图。\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(viridis)\n\n# 载入标准世界地图\nmap &lt;- st_read(\"shp/worldmap/world.shp\")\n\nReading layer `world' from data source \n  `/Users/liangdan/Downloads/website/course/bigdata/spatial/shp/worldmap/world.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 242 features and 19 fields (with 2 geometries empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.9 ymin: -89.9 xmax: 179.9 ymax: 83.6341\nGeodetic CRS:  WGS 84\n\n# 使用 wbstats 包从世界银行数据库下载空气污染数据\nlibrary(wbstats)\n# 搜素污染指标，可以帮助查看具体的指标名称\nindicators &lt;- wb_search(pattern = \"pollution\")\n# 下载2016年的PM2.5数据\nd &lt;- wb_data(indicator = \"EN.ATM.PM25.MC.M3\",\n             start_date = 2016, end_date = 2016)\n# 链接地图和数据，通过参数by指定链接的变量国家的ISO3标准代码\n# 注意，地图对象在前，数据框在后\nmap1 &lt;- left_join(map, d, by = c(\"ISO_A3\" = \"iso3c\"))\n# 绘制PM2.5分布图\nggplot(map1) + geom_sf(aes(fill = EN.ATM.PM25.MC.M3)) +\n  scale_fill_viridis() + labs(fill = \"PM2.5\") + theme_bw()\n\n\n\n\n\n\n\n\n\n\n空间连接\n与采用相同属性对接不同，空间连接的判断标准依靠空间谓词（例如，contains和 within,covers和covered_by，crosses，disjoint和intersects，equals，is_within_distance等二元谓词）。同时，空间连接时，通常每个记录有多个匹配的记录，为了简化，一般选择与目标记录几何体重叠面积最大几何体记录。\n\n# example of largest = TRUE:\nsystem.file(\"gpkg/nc.gpkg\", package=\"sf\") |&gt; \n    read_sf() |&gt;\n    st_transform('EPSG:2264') -&gt; nc\ngr &lt;- st_sf(\n         label = apply(expand.grid(1:10, LETTERS[10:1])[,2:1], 1, paste0, collapse = \"\"),\n         geom = st_make_grid(nc))\ngr$col &lt;- sf.colors(10, categorical = TRUE, alpha = .3)\n# cut, to verify that NA's work out:\ngr &lt;- gr[-(1:30),]\nsuppressWarnings(nc_j &lt;- st_join(nc, gr, largest = TRUE))\npar(mfrow = c(2,1), mar = rep(0,4))\nplot(st_geometry(nc_j), border = 'grey')\nplot(st_geometry(gr), add = TRUE, col = gr$col)\ntext(st_coordinates(st_centroid(st_geometry(gr))), labels = gr$label, cex = .85)\n# the joined dataset:\nplot(st_geometry(nc_j), border = 'grey', col = nc_j$col)\ntext(st_coordinates(st_centroid(st_geometry(nc_j))), labels = nc_j$label, cex = .7)\nplot(st_geometry(gr), border = '#88ff88aa', add = TRUE)"
  },
  {
    "objectID": "bigdata/spatial/spatialdata.html#terra-包",
    "href": "bigdata/spatial/spatialdata.html#terra-包",
    "title": "空间数据分析基础",
    "section": "3.2 terra 包",
    "text": "3.2 terra 包\nterra 包创建、读取、操作和写入栅格和向量数据。栅格数据常用于表示空间连续现象，通过将研究区域划分为大小相等的矩形网格（称为单元格或像素）来存储感兴趣变量的值。在 terra 中，多层栅格数据使用 SpatRaster 类表示。SpatVector 类用于表示向量数据，如点、线和多边形及其属性。\n\n创建栅格数据\nrast() 函数可用于创建和读取栅格数据。writeRaster() 函数写入栅格数据。\n\nlibrary(terra)\n# 创建栅格数据\nr &lt;- rast(ncol = 10, nrow = 10,\n          xmin = -150, xmax = -80, ymin = 20, ymax = 60)\nr\n\nclass       : SpatRaster \nsize        : 10, 10, 1  (nrow, ncol, nlyr)\nresolution  : 7, 4  (x, y)\nextent      : -150, -80, 20, 60  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \n\n# 函数获取栅格的大小。\nnrow(r) # 行数\n\n[1] 10\n\nncol(r) # 列数\n\n[1] 10\n\ndim(r) # 维度\n\n[1] 10 10  1\n\nncell(r) # 单元格数\n\n[1] 100\n\n# 设置和访问栅格的值。\nvalues(r) &lt;- 1:ncell(r)\n\n# 创建多层对象。\nr2 &lt;- r * r\ns &lt;- c(r, r2)\n\n# 选取特定的图层\nplot(s[[2]]) # 第 2 层\n\n\n\n\n\n\n\n# 通用函数可用于操作栅格\nplot(min(s))\n\n\n\n\n\n\n\nplot(r + r + 10)\n\n\n\n\n\n\n\nplot(round(r))\n\n\n\n\n\n\n\nplot(r == 1)\n\n\n\n\n\n\n\n\n\n\n读取向量数据\nvect() 读取shape文件，writeVector() 写入文件。\n\npathshp &lt;- system.file(\"ex/lux.shp\", package = \"terra\")\n# 读取shape文件并绘图\nv &lt;- vect(pathshp)\nplot(v)\n\n\n\n\n\n\n\n# 设定经度和纬度值\nlong &lt;- c(-0.118092, 2.349014, -3.703339, 12.496366)\nlat &lt;- c(51.509865, 48.864716, 40.416729, 41.902782)\nlonglat &lt;- cbind(long, lat)\n\n# 指定CRS\ncrspoints &lt;- \"+proj=longlat +datum=WGS84\"\n\n# 设定点的属性\nd &lt;- data.frame(\nplace = c(\"伦敦\", \"巴黎\", \"马德里\", \"罗马\"),\nvalue = c(200, 300, 400, 500))\n\n# 构建向量 SpatVector 对象\npts &lt;- vect(longlat, atts = d, crs = crspoints)\n\npts\n\n class       : SpatVector \n geometry    : points \n dimensions  : 4, 2  (geometries, attributes)\n extent      : -3.703339, 12.49637, 40.41673, 51.50986  (xmin, xmax, ymin, ymax)\n coord. ref. : +proj=longlat +datum=WGS84 +no_defs \n names       :  place value\n type        :  &lt;chr&gt; &lt;num&gt;\n values      :   伦敦   200\n                 巴黎   300\n               马德里   400\n\nplot(pts)\n\n\n\n\n\n\n\n\n\n\n裁剪、掩膜和聚合栅格数据\n\nlibrary(terra)\n# 从 WorldClim数据库下载荷兰的月平均温度数据，分辨率为10\nr &lt;- geodata::worldclim_country(country = \"Netherlands\", var = \"tavg\",\n                                res = 10, path = tempdir())\nplot(r)\n\n\n\n\n\n\n\n# 对月温度栅格数据进行平均\nr &lt;- mean(r)\nplot(r)\n\n\n\n\n\n\n\n# 为了更好呈现荷兰本土的温度分布，从 rnaturalearth 包下载荷兰地图，并删除海外领地。\n\nlibrary(ggplot2)\nmap &lt;- rnaturalearth::ne_states(\"Netherlands\", returnclass = \"sf\")\nmap &lt;- map[-which(map$type_en != \"Province\"), ] # 删除非本土省份\nggplot(map) + geom_sf()\n\n\n\n\n\n\n\n# 裁剪操作\n# 使用 terra::ext() 获取地图的空间范围，再使用 crop() 删除空间范围外的栅格数据\nsextent &lt;- terra::ext(map)\nr &lt;- terra::crop(r, sextent)\nplot(r)\n\n\n\n\n\n\n\n# 掩膜操作\n# 使用 mask() 函数将地图外的所有值转换为 NA\nr &lt;- terra::mask(r, vect(map))\nplot(r)\n\n\n\n\n\n\n\n# 聚合操作\n# 聚合栅格创建分辨率较低（即单元格较大）的新栅格。参数 fact 聚合因子规定每个方向（水平和垂直）的栅格格数,参数 fun 指定用于聚合值的函数。\nr &lt;- terra::aggregate(r, fact = 5, fun = \"mean\", na.rm = TRUE)\nplot(r)\n\n\n\n\n\n\n\n\n\n\n按点位提取栅格值\n\nlibrary(terra)\n# 获取栅格文件\nr &lt;- rast(system.file(\"ex/elev.tif\", package = \"terra\"))\n# 获取shape文件\nv &lt;- vect(system.file(\"ex/lux.shp\", package = \"terra\"))\n\n# 获取行政区的中心点的坐标\npoints &lt;- crds(centroids(v))\n\nplot(r)\nplot(v, add = TRUE)\npoints(points)\n\n\n\n\n\n\n\n# 提取行政区中心点的高程数据\npoints &lt;- as.data.frame(points)\nvaluesatpoints &lt;- extract(r, points)\ncbind(points, valuesatpoints)\n\n          x        y ID elevation\n1  6.009082 50.07064  1       444\n2  6.127425 49.86614  2       295\n3  5.886502 49.80014  3       382\n4  6.165081 49.92886  4       404\n5  5.914545 49.93892  5       414\n6  6.378449 49.78511  6       320\n7  6.311601 49.54569  7       193\n8  6.346395 49.68742  8       228\n9  5.963503 49.64159  9       313\n10 6.023816 49.52331 10       282\n11 6.167624 49.61815 11       328\n12 6.113598 49.75744 12       221\n\n\n\n\n提取区域提取栅格值并汇总\n可以使用 extract() 获取栅格对象在多边形内的所有取值。默认情况下，提取的单元格要求其中心在多边形内。还可以设置参数 weights = TRUE 获取单元格在多边形内的面积百分比，可用于计算面积加权的平均值。参数 fun 可用于指定汇总函数（例如，mean）来汇总所有的栅格提取值。\n\n# 每个区域内提取的栅格单元格\nhead(extract(r, v, na.rm = TRUE))\n\n  ID elevation\n1  1       547\n2  1       485\n3  1       497\n4  1       515\n5  1       515\n6  1       515\n\n# 提取栅格单元格及其在区域内的面积百分比\nhead(extract(r, v, na.rm = TRUE, weights = TRUE))\n\n  ID elevation     weight\n1  1        NA 0.04545454\n2  1        NA 0.10909091\n3  1       529 0.24545454\n4  1       542 0.46363635\n5  1       547 0.68181816\n6  1       535 0.11818181\n\n# 区域内的栅格平均值\nv$avg &lt;- extract(r, v, mean, na.rm = TRUE)$elevation\n\n# 还可以计算区域内的面积加权栅格平均值（weights = TRUE）\nv$weightedavg &lt;- extract(r, v, mean, na.rm = TRUE,\n                 weights = TRUE)$elevation\n\nlibrary(ggplot2)\nlibrary(tidyterra)\n\n# 绘制各区域的平均海拔高度图\nggplot(data = v) + geom_spatvector(aes(fill = avg)) +\n  scale_fill_terrain_c()\n\n\n\n\n\n\n\n# 绘制各区域的面积加权平均海拔高度图\nggplot(data = v) + geom_spatvector(aes(fill = weightedavg)) +\n  scale_fill_terrain_c()"
  },
  {
    "objectID": "bigdata/spatial/spatialdata.html#stars-包",
    "href": "bigdata/spatial/spatialdata.html#stars-包",
    "title": "空间数据分析基础",
    "section": "3.5 stars 包",
    "text": "3.5 stars 包\n尽管terra包提供了方便的函数处理栅格数据，但是其所用的栅格数据模型是二维规则栅格，或一组栅格层（“栅格堆栈”）。目前，更多大数据是动态的，以栅格或栅格堆栈的时间序列形式出现。同时，terra 包能够应付本地存储（计算机硬盘）大小的数据，大数据集（如卫星影像、气候模型或天气预报数据）往往远超本地存储的能力。\nstars 包，用于分析栅格和矢量数据立方。具有如下优点：\n\n允许表示动态（随时间变化）的栅格堆栈，\n\n旨在可扩展，处理超出本地磁盘大小的数据，\n\n提供与 GDAL 库中栅格函数的紧密集成，\n处理规则网格、旋转、剪切、折线和曲线栅格，\n与 sf 包紧密集成，\n处理具有非栅格空间维度的数组数据，即矢量数据立方，\n遵循 tidyverse 设计原则。\n\n\n读取和写入栅格数据\n\ntif &lt;- system.file(\"tif/L7_ETMs.tif\", package = \"stars\")\nlibrary(stars)\n(r &lt;- read_stars(tif))\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n             Min. 1st Qu. Median     Mean 3rd Qu. Max.\nL7_ETMs.tif     1      54     69 68.91242      86  255\ndimension(s):\n     from  to  offset delta                     refsys point x/y\nx       1 349  288776  28.5 SIRGAS 2000 / UTM zone 25S FALSE [x]\ny       1 352 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE [y]\nband    1   6      NA    NA                         NA    NA    \n\n\n文件为Landsat7的卫星影像数据集，巴西奥林达地区的6个30米分辨率波段（波段1-5和7），展示起始索引、结束索引、偏移（第一个像素边缘的维度取值，提供了栅格数据在真实世界中的锚点）、单元大小（负值意味着像素索引随着维度值减少而增加）、坐标参考系、单元值是否具有点支持（是否与点的大小、面积、周长等相关）、维度是否与空间栅格x或y轴相关。\n\n# r对象的长度\nlength(r)\n\n[1] 1\n\n# r对象包含元素的类别\nclass(r[[1]])\n\n[1] \"array\"\n\n# r对象包含元素的维度\ndim(r[[1]])\n\n   x    y band \n 349  352    6 \n\n# r对象的空间范围\nst_bbox(r)\n\n     xmin      ymin      xmax      ymax \n 288776.3 9110728.8  298722.8 9120760.8 \n\n# \n\n栅格数据可以保存到本地磁盘\n\ntf &lt;- tempfile(fileext = \".tif\")\nwrite_stars(r, tf)\n\n\n\n获取数据立方的子集\n\n# 直接采用[符号进行选择，第一个参数是属性，此处默认全选；\n# 随后是每个维度，维度1代表的x选取的索引是从1到100\n# 维度2代表的y选取的索引是间隔为5的序列，维度3代表的波段选取为4\nr[,1:100, seq(1, 250, 5), 4] |&gt; dim()\n\n   x    y band \n 100   50    1 \n\n# drop参数为TRUE，单一值的维度会被丢弃\nr[,1:100, seq(1, 250, 5), 4, drop = TRUE] |&gt; dim()\n\n  x   y \n100  50 \n\n# 可以使用filter函数进行条件筛选，注意该操作选择完子集会改变数据的offset\nlibrary(dplyr)\nfilter(r, x &gt; 289000, x &lt; 290000)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n             Min. 1st Qu. Median    Mean 3rd Qu. Max.\nL7_ETMs.tif     5      51     63 64.3337      75  242\ndimension(s):\n     from  to  offset delta                     refsys point x/y\nx       1  35  289004  28.5 SIRGAS 2000 / UTM zone 25S FALSE [x]\ny       1 352 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE [y]\nband    1   6       1     1                         NA    NA    \n\n# 获取数据立方切片，波段为单一值，该维度会被丢弃\nslice(r, band, 3)\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n             Min. 1st Qu. Median     Mean 3rd Qu. Max.\nL7_ETMs.tif    21      49     63 64.35886      77  255\ndimension(s):\n  from  to  offset delta                     refsys point x/y\nx    1 349  288776  28.5 SIRGAS 2000 / UTM zone 25S FALSE [x]\ny    1 352 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE [y]\n\n\n\n\n裁剪\n可以使用 sf、sfc 或 bbox 类的空间对象进行子集选择。\n\nb &lt;- st_bbox(r) |&gt;\n      st_as_sfc() |&gt;\n      st_centroid() |&gt;\n      st_buffer(units::set_units(500, m))\n# 选取子集后维度索引保持不变\nr[b]\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n             Min. 1st Qu. Median     Mean 3rd Qu. Max. NA's\nL7_ETMs.tif    22      54     66 67.68302   78.25  174 2184\ndimension(s):\n     from  to  offset delta                     refsys point x/y\nx     157 193  288776  28.5 SIRGAS 2000 / UTM zone 25S FALSE [x]\ny     159 194 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE [y]\nband    1   6      NA    NA                         NA    NA    \n\n# 绘制直径为五百米的圆形选择的子集的波段1图像\nplot(r[b][,,,1], reset = FALSE)\nplot(b, border = 'brown', lwd = 2, col = NA, add = TRUE)\n\n\n\n\n\n\n\n# 也可以直接裁剪\nst_crop(r, b)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n             Min. 1st Qu. Median     Mean 3rd Qu. Max. NA's\nL7_ETMs.tif    22      54     66 67.68302   78.25  174 2184\ndimension(s):\n     from  to  offset delta                     refsys point x/y\nx     157 193  288776  28.5 SIRGAS 2000 / UTM zone 25S FALSE [x]\ny     159 194 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE [y]\nband    1   6      NA    NA                         NA    NA    \n\n\n\n\n重塑和组合 stars 对象\n\n# 改变维度顺序\naperm(r, c(3, 1, 2))\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n             Min. 1st Qu. Median     Mean 3rd Qu. Max.\nL7_ETMs.tif     1      54     69 68.91242      86  255\ndimension(s):\n     from  to  offset delta                     refsys point x/y\nband    1   6      NA    NA                         NA    NA    \nx       1 349  288776  28.5 SIRGAS 2000 / UTM zone 25S FALSE [x]\ny       1 352 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE [y]\n\n# 将波段维度拆分到6个二维数组的属性上\n(rs &lt;- split(r))\n\nstars object with 2 dimensions and 6 attributes\nattribute(s):\n    Min. 1st Qu. Median     Mean 3rd Qu. Max.\nX1    47      67     78 79.14772      89  255\nX2    32      55     66 67.57465      79  255\nX3    21      49     63 64.35886      77  255\nX4     9      52     63 59.23541      75  255\nX5     1      63     89 83.18266     112  255\nX6     1      32     60 59.97521      88  255\ndimension(s):\n  from  to  offset delta                     refsys point x/y\nx    1 349  288776  28.5 SIRGAS 2000 / UTM zone 25S FALSE [x]\ny    1 352 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE [y]\n\n# 重新组合波段维度\nmerge(rs, name = \"band\") |&gt; setNames(\"L7_ETMs\")\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n         Min. 1st Qu. Median     Mean 3rd Qu. Max.\nL7_ETMs     1      54     69 68.91242      86  255\ndimension(s):\n     from  to  offset delta                     refsys point    values x/y\nx       1 349  288776  28.5 SIRGAS 2000 / UTM zone 25S FALSE      NULL [x]\ny       1 352 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE      NULL [y]\nband    1   6      NA    NA                         NA    NA X1,...,X6    \n\n\n\n\n提取点样本和聚合\n栅格数据立方分析常用操作是提取特定位置的值或计算某些几何体的聚合。\n\nset.seed(115517)\n# 在r对象的边界框上随机采样20个点，并提取点值\npts &lt;- st_bbox(r) |&gt; st_as_sfc() |&gt; st_sample(20)\n# 生成具有20个点6个波段的矢量数据立方\n(e &lt;- st_extract(r, pts))\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n             Min. 1st Qu. Median     Mean 3rd Qu. Max.\nL7_ETMs.tif    12   41.75     63 60.95833    80.5  145\ndimension(s):\n         from to                     refsys point\ngeometry    1 20 SIRGAS 2000 / UTM zone 25S  TRUE\nband        1  6                         NA    NA\n                                                        values\ngeometry POINT (293002.2 9115516),...,POINT (290941.1 9114128)\nband                                                      NULL\n\n\n通过聚合提取信息\n\ncircles &lt;- st_sample(st_as_sfc(st_bbox(r)), 3) |&gt;\n    st_buffer(500)\n# 用给定的3个圆形来聚合r对象的属性值，取范围内的最大值\naggregate(r, circles, max)\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n             Min. 1st Qu. Median     Mean 3rd Qu. Max.\nL7_ETMs.tif   106     118    129 145.3333  162.75  215\ndimension(s):\n         from to                     refsys point\ngeometry    1  3 SIRGAS 2000 / UTM zone 25S FALSE\nband        1  6                         NA    NA\n                                                                values\ngeometry POLYGON ((295489.8 911796...,...,POLYGON ((294732.5 911899...\nband                                                              NULL\n\n\n\n\n预测模型\n如何区分海洋和陆地？\n\nplot(r[,,,1], reset = FALSE)\ncol &lt;- rep(\"yellow\", 20)\ncol[c(8, 14, 15, 18, 19)] = \"red\"\nst_as_sf(e) |&gt; st_coordinates() |&gt; text(labels = 1:20, col = col)\n\n\n\n\n\n\n\n\n肉眼可以识别，8、14、15、18、19位于海洋，其他点位于陆地。可以通过线性判别分类进行海陆分类。\n\n# 拆分波段并转化为属性，准备预测变量\nrs &lt;- split(r)\n# 提取点位信息\ntrn &lt;- st_extract(rs, pts)\n# 给已知点位赋值海陆，标明类别\ntrn$cls &lt;- rep(\"陆地\", 20)\ntrn$cls[c(8, 14, 15, 18, 19)] &lt;- \"海洋\"\n# 建立模型，波段取值为预测变量，海陆类别为因变量，数据来自剔除几何体的数据框\nmodel &lt;- MASS::lda(cls ~ ., st_drop_geometry(trn))\n# 采用LDA模型进行预测分类\npr &lt;- predict(rs, model)\n# 绘制分类图\nplot(pr[1], key.pos = 4, key.width = lcm(3.5), key.length = lcm(2))\n\n\n\n\n\n\n\n\n\n\n栅格数据的计算和函数\n按维度，可以对 stars 对象的选定数组维度应用函数，类似于 apply 对数组的操作。例如，计算每个像素六个波段值的均值：\n\nst_apply(r, c(\"x\", \"y\"), mean)\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n      Min.  1st Qu.   Median     Mean 3rd Qu. Max.\nmean  25.5 53.33333 68.33333 68.91242      82  255\ndimension(s):\n  from  to  offset delta                     refsys point x/y\nx    1 349  288776  28.5 SIRGAS 2000 / UTM zone 25S FALSE [x]\ny    1 352 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE [y]\n\n\n更常见的函数是计算 NDVI（归一化差值植被指数）。\n\nndvi &lt;- function(b1, b2, b3, b4, b5, b6) (b4 - b3)/(b4 + b3)\nst_apply(r, c(\"x\", \"y\"), ndvi)\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n            Min.    1st Qu.      Median        Mean   3rd Qu.      Max.\nndvi  -0.7534247 -0.2030075 -0.06870229 -0.06432464 0.1866667 0.5866667\ndimension(s):\n  from  to  offset delta                     refsys point x/y\nx    1 349  288776  28.5 SIRGAS 2000 / UTM zone 25S FALSE [x]\ny    1 352 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE [y]\n\n\n也可以计算整个图像每个波段的均值\n\nst_apply(r, c(\"band\"), mean) |&gt; as.data.frame()\n\n  band     mean\n1    1 79.14772\n2    2 67.57465\n3    3 64.35886\n4    4 59.23541\n5    5 83.18266\n6    6 59.97521"
  },
  {
    "objectID": "bigdata/spatial/spatialdata.html#矢量大数据",
    "href": "bigdata/spatial/spatialdata.html#矢量大数据",
    "title": "空间数据分析基础",
    "section": "5.1 矢量大数据",
    "text": "5.1 矢量大数据\n\n从本地磁盘读取\nsf包函数 st_read 一般从本地磁盘读取矢量数据，并保存在工作内存中。如果文件过大，无法一次性加载到工作内存，可以通过设置 wkt_filter 参数，提供一个包含几何体的 WKT 文本字符串，仅返回与该几何体相交的目标文件中的几何体。\n\nlibrary(sf)\nfile &lt;- system.file(\"gpkg/nc.gpkg\", package = \"sf\")\n# 设定空间区域范围的文本字符串\nc(xmin = -82, ymin = 36, xmax = -80, ymax = 37) |&gt;\n    st_bbox() |&gt; st_as_sfc() |&gt; st_as_text() -&gt; bb\n# 通过空间范围读取\nread_sf(file, wkt_filter = bb) |&gt; nrow() # 从 100 条记录中返回\n\n[1] 16\n\n\n还可以使用 st_read 的 query 参数，来选择图层中的要素或限制字段，先查询再读取。\n\nq &lt;- \"select BIR74,SID74,geom from 'nc.gpkg' where BIR74 &gt; 1500\"\nread_sf(file, query = q) |&gt; nrow()\n\n[1] 61\n\n\n\n\n从数据库读取\n也可以直接使用 R 的 DBI 数据库驱动程序读写服务器上空间数据库。\n\npg &lt;- DBI::dbConnect(\n    RPostgres::Postgres(),\n    host = Sys.getenv(\"DB_HOST\"),\n    user = Sys.getenv(\"DB_USERNAME\"),\n    dbname = \"postgis\")\nread_sf(pg, query = \n        \"select BIR74,wkb_geometry from nc limit 3\") |&gt; nrow()\n\n\n\n从在线资源或网络服务读取\nGDAL 驱动程序支持在 URL（以 https:// 开头）前添加 /vsicurl/ 前缀从在线资源读取数据。针对特定云服务的驱动程序包括 /vsis3/（Amazon S3）、/vsigs/（Google Cloud Storage）/vsioss/（阿里云）。 /vsicurl/ 的示例在stars部分。\n大多数地理空间数据网络服务可以动态生成数据，并通过 API 提供访问。查询 OpenStreetMap 数据的R包有 OpenStreetMap（以栅格瓦片形式下载数据，通常用作绘制其他要素的背景）、 osmdata（以点、线或多边形的矢量数据形式下载数据，支持 sf 或 sp 格式）、 osmar（返回矢量数据，并提供网络拓扑，包含道路连接方式，可计算最短路径）。\n\n# 通过overpass提供的api，按照指定经纬度的区域获取空间数据\ndownload.file(paste0(\"https://overpass-api.de/api/map?\",\n       \"bbox=7.595,51.969,7.598,51.970\"),\n    \"data/ms.osm\", method = \"auto\")\no &lt;- read_sf(\"data/ms.osm\", \"lines\")\np &lt;- read_sf(\"data/ms.osm\", \"multipolygons\")\nst_bbox(c(xmin = 7.595, ymin = 51.969, \n          xmax = 7.598, ymax = 51.970), crs = 'OGC:CRS84') |&gt;\n    st_as_sfc() |&gt;\n    plot(axes = TRUE, lwd = 2, lty = 2, cex.axis = .5)\nplot(o[,1], lwd = 2, add = TRUE)\nplot(st_geometry(p), border = NA, col = '#88888888', add = TRUE)\n\n\n\n\n\n\n\n\n此外，GeoParquet 和 GeoArrow 是从 Apache 项目 Parquet 和 Arrow 衍生出的两种专为云原生分析设计的格式。两者都提供列式存储的表格数据，这意味着读取单个字段的多个记录很快。目前，这两种格式仍在积极开发中，但从 GDAL 3.5 版本开始已提供读取或创建它们的驱动程序。"
  },
  {
    "objectID": "bigdata/spatial/spatialdata.html#栅格大数据",
    "href": "bigdata/spatial/spatialdata.html#栅格大数据",
    "title": "空间数据分析基础",
    "section": "5.2 栅格大数据",
    "text": "5.2 栅格大数据\n栅格数据集的常见挑战不仅是单个文件较大（单个 Sentinel-2 瓦片约为 1 GB），而且通常需要处理成千上万甚至数百万个文件来覆盖感兴趣的区域和时间段。基于云的地球观测处理平台（如 Google Earth Engine、Sentinel Hub 或 openEO.cloud）能够轻松处理高达Petabyte级别的数据集，并具有高度交互性。\n这些平台的策略是计算尽可能推迟（延迟求值）；仅计算和返回用户请求的数据，不多计算；避免存储中间结果，优先进行即时计算；快速生成并显示有用的结果地图，以支持交互式模型开发。\nstars包也遵循这类思路进行优化。例如，对于大型栅格的绘制，会在绘图前下采样（downsample）数组，大幅节省时间。下采样的程度由绘图区域大小和绘图分辨率（像素密度）决定。对于矢量设备（如 PDF），R 将绘图分辨率设置为 75 dpi，对应于每像素 0.3 毫米。\n\nstars 代理对象\n为了处理过大而无法加载到内存的数据集，stars 提供了 stars_proxy 对象。在运行代码时， 实际上并未加载任何像素值，而是保留了对数据集的引用并填充了维度表，等到需要数据的场景，才加载像素值，例如绘制数据（plot）、写入磁盘（write_stars）、显式加载对象到内存（st_as_stars）等。\n如果整个对象无法加载到内存，plot 和 write_stars 会采用不同的策略来处理：\n\nplot 仅获取通过下采样可见的像素，而不是读取所有可用像素；\n\nwrite_stars 分块读取、处理和写入数据。\n\nstars_proxy 对象的专用方法如下。\n\nmethods(class = \"stars_proxy\")\n\n [1] [               [[&lt;-            [&lt;-             adrop          \n [5] aggregate       aperm           as.data.frame   c              \n [9] coerce          dim             droplevels      filter         \n[13] hist            image           initialize      is.na          \n[17] mapView         Math            merge           mutate         \n[21] Ops             plot            prcomp          predict        \n[25] print           pull            rast            rename         \n[29] replace_na      sds             select          show           \n[33] slice           slotsFromS3     split           st_apply       \n[37] st_as_sf        st_as_stars     st_crop         st_dimensions&lt;-\n[41] st_downsample   st_mosaic       st_normalize    st_redimension \n[45] st_sample       st_set_bbox     transmute       write_stars    \nsee '?methods' for accessing help and source code"
  },
  {
    "objectID": "bigdata/spatial/spatialdata.html#超大数据立方体",
    "href": "bigdata/spatial/spatialdata.html#超大数据立方体",
    "title": "空间数据分析基础",
    "section": "5.3 超大数据立方体",
    "text": "5.3 超大数据立方体\n在某些情况下，数据集过大，无法下载；即使本地存储足够，网络带宽也会限制下载。例如，Landsat 等卫星图像档案，或 ERA5等全球大气、陆地和海洋模型再分析数据等。在这种情况下，可以通过云中的虚拟机访问数据，或者使用虚拟机和存储的系统进行计算。\n\n查找和处理文件\n在云上的虚拟机上工作时，首先要找到要处理的文件。在上百万的文件中，直接通过文件名获取信息效率很低，常用的解决方案是使用目录。STAC（时空资产目录）提供了API，可按边界框、日期、波段和云覆盖率等属性查询图像集合。R 包 rstac 提供了创建查询和管理返回信息的 R 接口。\nZarr 是一种为大型多维数组提供云原生存储的格式。Zarr “文件”实际上是包含压缩数据块的子目录的目录。函数 stars::read_mdim 可以读取整个数据立方体，但也支持通过为每个维度指定偏移、像素数量和步长来读取子立方体，以较低分辨率读取维度。类似地，stars::write_mdim 可以将多维数组写入 Zarr 或 NetCDF 文件。\n要读取远程（基于云的）Zarr 文件，需要在 URL 前添加格式和访问协议的指示符：\n\ndsn = paste0('ZARR:\"/vsicurl/https://ncsa.osn.xsede.org',\n       '/Pangeo/pangeo-forge/gpcp-feedstock/gpcp.zarr\"')\n\nlibrary(stars)\n# 指定边界\nbounds = c(longitude = \"lon_bounds\", latitude = \"lat_bounds\")\n# 读取远程云上的数据文件，count 中的 NA 值表示获取该维度所有可用值\n(r = read_mdim(dsn, bounds = bounds, count = c(NA, NA, 10)))\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n              Min. 1st Qu. Median     Mean  3rd Qu.     Max.\nprecip [mm/d]    0       0      0 2.249929 1.604743 109.2054\ndimension(s):\n          from  to     offset  delta refsys x/y\nlongitude    1 360          0      1     NA [x]\nlatitude     1 180        -90      1     NA [y]\ntime         1  10 1996-10-01 1 days   Date    \n\n# 数据的范围\nst_bbox(r)\n\nxmin ymin xmax ymax \n   0  -90  360   90"
  },
  {
    "objectID": "bigdata/spatial/spatialdata.html#参考文献",
    "href": "bigdata/spatial/spatialdata.html#参考文献",
    "title": "空间数据分析基础",
    "section": "参考文献",
    "text": "参考文献\n\nMoraga, Paula. (2023). Spatial Statistics for Data Science: Theory and Practice with R. Chapman & Hall/CRC Data Science Series.\n\nPebesma, E., Bivand, R. (2023). Spatial Data Science: With Applications in R (1st ed.). Chapman and Hall/CRC, Boca Raton. https://doi.org/10.1201/9780429459016"
  },
  {
    "objectID": "bigdata/index.html#课程作业",
    "href": "bigdata/index.html#课程作业",
    "title": "公共管理大数据分析",
    "section": "课程作业",
    "text": "课程作业\n\n数据检索与选题探索\n\n\n数据清洗与可视化\n\n\n机器学习"
  },
  {
    "objectID": "bigdata/exercises/searchq.html",
    "href": "bigdata/exercises/searchq.html",
    "title": "数据检索与选题探索作业",
    "section": "",
    "text": "熟悉公共管理领域常见的大数据来源，掌握数据获取与理解的基本方法，并通过结合相关文献，初步提出潜在的研究问题，为课程论文选题和设计奠定基础。"
  },
  {
    "objectID": "bigdata/exercises/searchq.html#一国内开放数据源",
    "href": "bigdata/exercises/searchq.html#一国内开放数据源",
    "title": "数据检索与选题探索作业",
    "section": "一、国内开放数据源",
    "text": "一、国内开放数据源\n\n1. 政府与公共服务\n\n中国政府网“数据”栏目 http://data.gov.cn/\n\n全国性政府数据平台，涵盖经济、社会、民生、交通等多领域。\n\n国家统计局数据 http://data.stats.gov.cn/\n\n各类宏观统计指标（人口、经济、教育、医疗、环境）。\n\n地方政府开放数据平台\n\n如：北京开放数据平台（https://data.beijing.gov.cn/），上海开放数据服务网（https://data.sh.gov.cn/）。\n可用于城市治理、公共服务供给公平研究。\n\n\n\n\n2. 环境与可持续发展\n\n中国环境监测总站数据 http://www.cnemc.cn/\n\n空气质量、水质、噪音监测数据。\n\n中国碳排放数据库（CEADs） http://www.ceads.net.cn/\n\n各省、市碳排放清单数据。\n\n国家遥感科学数据中心 http://www.rscloudmart.com/\n\n卫星遥感影像，可用于土地利用、污染检测、生态研究。\n\n\n\n\n3. 社会与民生\n\n中国知网-社会调查数据平台（部分开放） https://www.cnki.net/\n\n部分社会调查和统计数据，可结合问卷调查研究。\n\n中国健康与养老追踪调查（CHARLS） http://charls.pku.edu.cn/\n\n大规模面板数据，关注健康、养老、社会保障。\n\n中国家庭追踪调查（CFPS） http://www.isss.pku.edu.cn/cfps/\n\n涉及教育、就业、家庭结构、收入分配。\n\n\n\n\n4. 风险管理与应急\n\n中国地震局科学数据中心 http://data.earthquake.cn/\n\n地震监测数据。\n\n中国气象科学数据共享服务网 http://data.cma.cn/\n\n气象监测、灾害预警数据。\n\n微博数据（需通过API或爬取）\n\n可用于应急舆情、社会风险感知研究。"
  },
  {
    "objectID": "bigdata/exercises/searchq.html#二国际开放数据源",
    "href": "bigdata/exercises/searchq.html#二国际开放数据源",
    "title": "数据检索与选题探索作业",
    "section": "二、国际开放数据源",
    "text": "二、国际开放数据源\n\n1. 政府与公共政策\n\nWorld Bank Open Data https://data.worldbank.org/\n\n全球发展指标，适合跨国比较。\n\nOECD Data https://data.oecd.org/\n\n涵盖教育、劳动力、环境、治理等主题。\n\nUN Data https://data.un.org/\n\n联合国各类统计数据库。\n\n\n\n\n2. 环境与可持续发展\n\nNASA Earthdata https://earthdata.nasa.gov/\n\n遥感与气候数据。\n\nEuropean Environment Agency (EEA) Data https://www.eea.europa.eu/data-and-maps\n\n欧洲环境、污染、能源数据。\n\nGlobal Carbon Atlas http://www.globalcarbonatlas.org/\n\n全球碳排放数据。\n\n\n\n\n3. 社会与民生\n\nPew Research Center Datasets https://www.pewresearch.org/download-datasets/\n\n公共舆论、社会价值观、治理调查。\n\nEuropean Social Survey (ESS) https://ess-search.nsd.no/\n\n欧洲各国社会态度与政策数据。\n\nIPUMS (Integrated Public Use Microdata Series) https://ipums.org/\n\n全球人口普查与调查微观数据。\n\n\n\n\n4. 风险管理与应急\n\nEM-DAT: The International Disaster Database https://www.emdat.be/\n\n全球自然灾害数据库。\n\nCrisisNET (Archive) https://crisis.net/\n\n人道危机、冲突事件数据。\n\nTwitter API (Academic Research track) https://developer.twitter.com/en/products/twitter-api/academic-research\n\n可用于应急舆情分析。"
  },
  {
    "objectID": "bigdata/exercises/searchq.html#国内开放数据源",
    "href": "bigdata/exercises/searchq.html#国内开放数据源",
    "title": "数据检索与选题探索作业",
    "section": "4.1 国内开放数据源",
    "text": "4.1 国内开放数据源\n\n1. 政府与公共服务\n\n中国政府网“数据”栏目 http://data.gov.cn/\n\n全国性政府数据平台，涵盖经济、社会、民生、交通等多领域。\n\n国家统计局数据 http://data.stats.gov.cn/\n\n各类宏观统计指标（人口、经济、教育、医疗、环境）。\n\n地方政府开放数据平台\n\n如：北京开放数据平台（https://data.beijing.gov.cn/），上海开放数据服务网（https://data.sh.gov.cn/）\n可用于城市治理、公共服务供给公平研究。\n\n\n\n\n2. 环境与可持续发展\n\n中国环境监测总站数据 http://www.cnemc.cn/\n\n空气质量、水质、噪音监测数据。\n\n中国碳排放数据库（CEADs） http://www.ceads.net.cn/\n\n各省、市碳排放清单数据。\n\n国家地球系统科学数据中心 https://www.geodata.cn\n\n多时空尺度综合性地球系统科学数据库群。\n\n\n\n\n3. 社会与民生\n\n中国知网-中国经济社会大数据研究平台 https://data.cnki.net//\n\n统计、普查、调查资料等。\n\n中国健康与养老追踪调查（CHARLS） http://charls.pku.edu.cn/\n\n大规模面板数据，关注健康、养老、社会保障。\n\n中国家庭追踪调查（CFPS） http://www.isss.pku.edu.cn/cfps/\n\n涉及教育、就业、家庭结构、收入分配。\n\n\n\n\n4. 风险管理与应急\n\n中国地震局科学数据中心 http://data.earthquake.cn/\n\n地震监测数据。\n\n中国气象科学数据共享服务网 http://data.cma.cn/\n\n气象监测、灾害预警数据。\n\n微博数据（需通过API或爬取）\n\n可用于应急舆情、社会风险感知研究。"
  },
  {
    "objectID": "bigdata/exercises/searchq.html#国际开放数据源",
    "href": "bigdata/exercises/searchq.html#国际开放数据源",
    "title": "数据检索与选题探索作业",
    "section": "4.2 国际开放数据源",
    "text": "4.2 国际开放数据源\n\n1. 政府与公共政策\n\nWorld Bank Open Data https://data.worldbank.org/\n\n全球发展指标，适合跨国比较。\n\nOECD Data https://data.oecd.org/\n\n涵盖教育、劳动力、环境、治理等主题。\n\nUN Data https://data.un.org/\n\n联合国各类统计数据库。\n\n\n\n\n2. 环境与可持续发展\n\nNASA Earthdata https://earthdata.nasa.gov/\n\n遥感与气候数据。\n\nEuropean Environment Agency (EEA) Data https://www.eea.europa.eu/data-and-maps\n\n欧洲环境、污染、能源数据。\n\nGlobal Carbon Atlas http://www.globalcarbonatlas.org/\n\n全球碳排放数据。\n\n\n\n\n3. 社会与民生\n\nPew Research Center Datasets https://www.pewresearch.org/download-datasets/\n\n公共舆论、社会价值观、治理调查。\n\nEuropean Social Survey (ESS) https://ess-search.nsd.no/\n\n欧洲各国社会态度与政策数据。\n\nIPUMS (Integrated Public Use Microdata Series) https://ipums.org/\n\n全球人口普查与调查微观数据。\n\n\n\n\n4. 风险管理与应急\n\nEM-DAT: The International Disaster Database https://www.emdat.be/\n\n全球自然灾害数据库。\n\nCrisisNET (Archive) https://crisis.net/\n\n人道危机、冲突事件数据。\n\nTwitter API https://developer.x.com/en/docs/x-api\n\n可用于应急舆情分析。"
  },
  {
    "objectID": "bigdata/exercises/cleanvisual.html",
    "href": "bigdata/exercises/cleanvisual.html",
    "title": "数据清洗与可视化作业",
    "section": "",
    "text": "巩固方法理解 —— 将课程中讲到的导入、清洗、转换、重塑、缺失值处理、可视化等操作落到实处；\n锻炼实操能力 —— 用 R 完成数据处理和可视化，从原始数据走到可解释图表；\n思考公共管理应用 —— 在可视化过程中，尝试将分析视角对准公共管理议题，为后续论文选题和设计积累素材。"
  },
  {
    "objectID": "bigdata/exercises/cleanvisual.html#一作业目标",
    "href": "bigdata/exercises/cleanvisual.html#一作业目标",
    "title": "数据清洗与可视化作业",
    "section": "",
    "text": "巩固方法理解 —— 将课程中讲到的导入、清洗、转换、重塑、缺失值处理、可视化等操作落到实处；\n锻炼实操能力 —— 用 R 完成数据处理和可视化，从原始数据走到可解释图表；\n思考公共管理应用 —— 在可视化过程中，尝试将分析视角对准公共管理议题，为后续论文选题和设计积累素材。"
  },
  {
    "objectID": "bigdata/exercises/cleanvisual.html#二作业内容",
    "href": "bigdata/exercises/cleanvisual.html#二作业内容",
    "title": "数据清洗与可视化作业",
    "section": "二、作业内容",
    "text": "二、作业内容\n\nA：数据导入与清洗\n任务说明\n\n从你感兴趣的一个公开数据集导入数据（可为 CSV、Excel、JSON、数据库等格式）或网页爬取数据，数据规模不必很大，但最好包含缺失值、异常值、字符串 / 数值变量混合等典型“脏数据”情形。\n对数据进行以下操作（至少包含以下几项）：\n\n选择变量与观测（删去无关列 / 行）\n变量重编码 / 衍生（例如将分类变量编码、构造新指标、日期处理）\n处理缺失值（删除 / 插补 / 标记等方式）\n异常值检测 / 处理（如去除极端值、Winsorize 处理、箱型图判断等）\n数据重塑 / 重组（例如从宽表变长表、聚合、拆分 / 合并列、分层数据处理）\n\n最终输出一个“干净数据集”的 R 对象 / CSV 文件 + 简要说明：\n\n说明清洗的步骤 / 原因\n遇到的挑战与处理思路\n对清洗后数据的基本描述（如行数、变量数、缺失率、变量基本统计）\n\n\n\n\nB：可视化与探索性分析\n任务说明\n\n基于子任务 A 清洗后的数据，制作 3–5 张可视化图表，类型建议从以下几类中选择：\n\n基本图表：柱状图、折线图、饼图、直方图、箱型图\n组合图 / 多维图表：如带分组的柱状图、堆叠图、散点图 + 回归线\n时间序列可视化：趋势图、滑动平均线、季节性展示\n地图 / 空间可视化（如果数据具有地理属性，如行政区、经纬度等）\n多变量可视化（可使用 ggplot2 + facets）\n\n给出每张图的 图示标题 / 说明 /解读，指出该图揭示了什么样的规律 / 特征，对于公共管理议题可能的启示是什么。\n最后写一段小结，回答以下问题：\n\n哪张图你最满意？为什么？\n在可视化过程中有没有遇到困难（变量类型不一致、极端值影响、图形美观性调整等）？你是如何调整 / 解决的？\n如果用这些可视化探索的结果做论文选题，你会倾向于哪一类公共管理问题？为什么？"
  },
  {
    "objectID": "bigdata/exercises/cleanvisual.html#三作业提交与评估",
    "href": "bigdata/exercises/cleanvisual.html#三作业提交与评估",
    "title": "数据清洗与可视化作业",
    "section": "三、作业提交与评估",
    "text": "三、作业提交与评估\n\n提交形式：R 脚本文件（.R 或 .Rmd）、清洗后的数据（CSV / RData）和可视化报告（PDF）\n评估标准：\n\n导入与清洗步骤合理性与完整性：30 分\n可视化图表数量、类型多样性、设计美观性：25 分\n图表解读与公共管理视角链接：25 分\n文档结构、注释、代码清晰程度：10 分\n遇到问题描述与解决思路：10 分\n\n提交时间和方式：两周之后，具体见数字化教学平台。"
  },
  {
    "objectID": "bigdata/machlearn/mltrees.html",
    "href": "bigdata/machlearn/mltrees.html",
    "title": "有监督学习：决策树",
    "section": "",
    "text": "基于树的模型是一类非参数算法，通过使用一组分割规则将特征空间划分为多个较小的（非重叠）区域，这些区域具有相似的响应值。在每个区域内拟合一个简单模型（例如，区域内响应值的平均值）来获得预测值。这种分而治之的方法可以产生简单易解释的规则，并可以通过树形图进行可视化。单个决策树的预测性能通常较弱，但是通过集成算法（如随机森林和梯度提升机）可以组合多个决策树构建预测能力更强的模型。\n\n\n使用以下R包：\n\n# 辅助包\nlibrary(dplyr)       # 用于数据处理\nlibrary(ggplot2)     # 用于出色的绘图\n\n# 建模包\nlibrary(rpart)       # 决策树直接引擎\nlibrary(caret)       # 决策树元引擎\n\n# 模型解释包\nlibrary(rpart.plot)  # 用于绘制决策树\nlibrary(vip)         # 用于特征重要性\nlibrary(pdp)         # 用于特征效应\n\n使用Ames住房示例来展示主要概念。\n\names &lt;- AmesHousing::make_ames()\n\nlibrary(rsample)\n# 分层抽样划分训练集和测试集数据\nset.seed(123)\nsplit  &lt;- initial_split(ames, prop = 0.7, strata = \"Sale_Price\")\names_train  &lt;- training(split)\names_test   &lt;- testing(split)\n\n\n\n\n构建决策树有许多方法，但最常见的是分类与回归树（CART）算法。基本的决策树将训练数据划分为同质子组（即响应值相似的群体），然后在每个子组内拟合一个简单常数（例如，回归问题中子组内响应值的均值）。子组（也称为节点）是通过使用二元分割递归形成，每次分割基于对每个特征的简单“是否”问题进行（例如，年龄&lt;18岁？）。这一过程重复多次，直到满足适当的停止标准（例如，达到树的最大深度）。在所有分区完成后，模型根据不同的问题的需要预测输出，对于回归问题，使用落入该子组的所有观测值的平均响应值输出预测；对于分类问题，使用子组内占多数的类别预测，同时可以使用子组内各类别的比例获得预测概率。\n结果是一个倒挂的树形结构，本质上，决策树是一组规则，允许通过对每个特征提出简单的“是否”问题来进行预测。例如，如果客户是忠诚客户，家庭收入超过15万美元，并且在商店购物，下面示例的决策树将预测客户会兑换优惠券。\n\n将树的顶部第一个子组称为根节点（此节点包含所有训练数据）。树底部的最终子组称为终端节点或叶节点。之间的子组称为内部节点。节点之间的连接称为分支。\n\n\n\n\nCART使用二元递归分区（递归是因为每个分割或规则依赖于其上方的分割）。每个节点的目标是找到“最佳”特征（\\(x_i\\)）将剩余数据划分为两个区域（\\(R_1\\) 和 \\(R_2\\)），使实际响应值（\\(y_i\\)）与预测常数（\\(c_i\\)）之间的总体误差最小化。对于回归问题，要最小化的目标函数是总平方误差和（SSE），如方程定义：\n\\[\nSSE = \\sum_{i \\in R_1} (y_i - c_1)^2 + \\sum_{i \\in R_2} (y_i - c_2)^2\n\\]\n对于分类问题，分区通常旨在最大化交叉熵或基尼指数的减少。在回归和分类树中，分区的目标是最小化终端节点中的不相似性。\n找到最佳特征/分割组合后，数据被划分为两个区域，并对这两个区域重复分割过程（因此称为二元递归分区）。此过程持续进行，直到达到适当的停止标准（例如，达到最大深度或树变得“过于复杂”）。\n需要注意的是，单个特征可以在树中多次使用。例如，假设从简单正弦函数加随机噪声生成的数据：\\(Y_i \\stackrel{iid}{\\sim} N(\\sin(X_i), \\sigma^2)\\)，其中 \\(i=1,2,\\dots,500\\)。构建的回归树只有一个根节点（通常称为决策桩），在 \\(x=3.1\\) 处发生分割。\n\n\n\n\n\n\n\n\n\n决策树展示了对特征 \\(x\\) 的单一分割（左）。决策边界显示了当 \\(x &lt; 3.1\\) 时，预测值为0.64；当 \\(x &gt; 3.1\\) 时，预测值为-0.67。\n\n\n\n\n\n\n\n\n\n如果继续构建更深的树，可以在同一特征（\\(x\\)）上继续分割。因为 \\(x\\) 是唯一可用于分割的特征，因此将继续沿该特征的值寻找最佳分割点，直到达到预定的停止标准。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n深度为3的决策树，展示了对特征 \\(x\\) 的7次分割，生成8个预测区域。\n即使有多个特征可用，如果某个特征在每次分区后仍能提供最佳分割，它可能仍然占据主导地位。例如，应用于鸢尾花数据集的决策树，其中根据两个特征（萼片宽度和萼片长度）预测花的种类（setosa、versicolor和virginica），结果生成了一个包含每个特征两次分割的最佳决策树。分类问题的决策边界生成包围观测值的矩形区域。预测值是区域内占比最高的响应类别。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n这引出了一个重要问题：树应该有多深（即多复杂）？如果构建过于复杂的树，往往会过拟合训练数据，导致泛化性能较差。\n有56次分割的过拟合决策树。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n因此，需要在树的深度和复杂性之间取得平衡，以优化对未来未见数据的预测性能。为了找到这种平衡，有两种主要方法：早期停止和修剪。\n\n\n早期停止明确限制树的生长。可以通过以下几种方式限制树生长，但最常见的方法是限制树深度到某个水平或限制终端节点的最小观测数。限制树深度意味着在达到某个深度后停止分割（例如，仅生长深度为5层的树）。树越浅，预测的方差越小（对训练集数据的变化不敏感）；然而，过浅的树（如决策桩）可能无法捕捉数据中的交互和复杂模式，从而引入过多偏差。\n限制终端节点的最小大小（例如，叶节点必须包含至少10个观测值用于预测），即决定不分割包含过少数据点的中间节点。在极端情况下，终端节点大小为1允许单个观测值被捕获在叶节点中并用作预测（完全拟合训练集数据），会导致高方差和较差的泛化能力。另一方面，较大的值限制进一步分割，从而减少方差。\n这两种方法可以独立实施，然而，它们会相互影响。列展示了树深度如何影响决策边界，行展示了终端节点最小观测数如何影响决策边界。\n\n\n\n\n\n\n\n\n\n\n\n\n另一种在树的深度和复杂性之间取得平衡的方法是不明确指定决策树的深度，而是先生长一个非常大且复杂的树，然后通过修剪找到最佳子树。例如可以使用成本复杂度参数（\\(\\alpha\\)）惩罚目标函数中的终端节点数（\\(T\\)），如方程所示：\n\\[\n\\text{minimize} \\left\\{ SSE + \\alpha |T| \\right\\}\n\\]\n对于给定的 \\(\\alpha\\) 值，我们找到具有最低惩罚误差的最小修剪树。这和之前讨论的Lasso惩罚很像。与正则化方法类似，较小的惩罚倾向于生成更复杂的模型，导致更大的树。而较大的惩罚导致更小的树。因此，随着树变大，SSE的减少必须大于成本复杂度惩罚。通常，可以评估多个 \\(\\alpha\\) 值的模型，并使用交叉验证（CV）确定最佳值，从而找到泛化能力最好的最佳子树。\n为了修剪树，我们首先生长一个过于复杂的树（左），然后使用成本复杂度参数识别最佳子树（右）。\n\n\n\n\n\n\n\n\n\n\n\n\n\n可以使用rpart拟合回归树，然后使用rpart.plot可视化。回归树和分类树的拟合过程和可视化输出非常相似。两者都使用公式方法表达模型（类似于lm()）。然而，拟合回归树时，需要设置method = \"anova\"。默认情况下，rpart()会根据响应列的数据类型猜测使用的方法，但明确设置更清晰。\n\names_dt1 &lt;- rpart(\n  formula = Sale_Price ~ .,\n  data    = ames_train,\n  method  = \"anova\"\n)\n\n拟合模型后，可以查看决策树输出。打印关于不同分割的各种信息。例如，从根节点的2049个观测值开始，第一次分割的变量（即提供最大SSE减少的变量）是Overall_Qual。在第一个节点，所有Overall_Qual属于 {Very_Poor, Poor, Fair, Below_Average, Average, Above_Average, Good} 的观测值进入第2分支（2)）。沿此分支的总观测数（1701）、平均销售价格（155897）和SSE（4.109012e+12）。如果查看第3分支（3)），会看到348个Overall_Qual属于 {Very_Good, Excellent, Very_Excellent} 的观测值沿此分支，平均销售价格为303245.90，该区域的SSE为2.838371e+12。所以，Overall_Qual是销售价格的重要预测变量，品质高端的房屋平均销售价格几乎是其他房屋的两倍。\n\names_dt1\n## n= 2049 \n## \n## node), split, n, deviance, yval\n##       * denotes terminal node\n## \n##  1) root 2049 1.321981e+13 180922.60  \n##    2) Overall_Qual=Very_Poor,Poor,Fair,Below_Average,Average,Above_Average,Good 1701 4.109012e+12 155897.00  \n##      4) Neighborhood=North_Ames,Old_Town,Edwards,Sawyer,Mitchell,Brookside,Iowa_DOT_and_Rail_Road,South_and_West_of_Iowa_State_University,Meadow_Village,Briardale,Northpark_Villa,Blueste,Landmark 1023 1.370671e+12 131815.70  \n##        8) First_Flr_SF&lt; 1089.5 661 5.643745e+11 119572.90  \n##         16) Overall_Qual=Very_Poor,Poor,Fair,Below_Average 149 1.016868e+11  90363.56 *\n##         17) Overall_Qual=Average,Above_Average,Good 512 2.985678e+11 128073.30 *\n##        9) First_Flr_SF&gt;=1089.5 362 5.263151e+11 154170.60 *\n##      5) Neighborhood=College_Creek,Somerset,Northridge_Heights,Gilbert,Northwest_Ames,Sawyer_West,Crawford,Timberland,Northridge,Stone_Brook,Clear_Creek,Bloomington_Heights,Veenker,Green_Hills 678 1.249971e+12 192232.10  \n##       10) Gr_Liv_Area&lt; 1725.5 484 5.392924e+11 177594.80  \n##         20) Total_Bsmt_SF&lt; 1295.5 342 2.299978e+11 166044.60 *\n##         21) Total_Bsmt_SF&gt;=1295.5 142 1.537824e+11 205413.00 *\n##       11) Gr_Liv_Area&gt;=1725.5 194 3.482733e+11 228749.90 *\n##    3) Overall_Qual=Very_Good,Excellent,Very_Excellent 348 2.838371e+12 303245.90  \n##      6) Overall_Qual=Very_Good 242 9.801339e+11 271271.60  \n##       12) Gr_Liv_Area&lt; 1920.5 143 2.792781e+11 240517.80 *\n##       13) Gr_Liv_Area&gt;=1920.5 99 3.702480e+11 315693.70 *\n##      7) Overall_Qual=Excellent,Very_Excellent 106 1.045983e+12 376243.90  \n##       14) Gr_Liv_Area&lt; 1956.5 47 8.921667e+10 324506.70 *\n##       15) Gr_Liv_Area&gt;=1956.5 59 7.307403e+11 417458.30  \n##         30) Neighborhood=Edwards,Somerset,Veenker 8 8.904433e+10 269794.20 *\n##         31) Neighborhood=College_Creek,Old_Town,Northridge_Heights,Timberland,Northridge,Stone_Brook 51 4.398958e+11 440621.30  \n##           62) First_Flr_SF&lt; 1829 28 9.425118e+10 391454.60 *\n##           63) First_Flr_SF&gt;=1829 23 1.955579e+11 500476.40 *\n\n可以使用rpart.plot()可视化树模型。rpart.plot()函数有许多绘图选项，可以自行探索。然而，在默认输出中，会显示每个节点的数据百分比和该节点的预测结果。此树包含10个内部节点，12个终端节点。训练数据中有80个特征，但是决策树只在10个特征上进行分区。\n\nrpart.plot(ames_dt1)\n\n\n\n\n\n\n\n\nrpart()会自动应用一系列成本复杂度（\\(\\alpha\\)）值来修剪树，找到最佳的树深度。为了比较每个 \\(\\alpha\\) 值的误差，rpart()默认执行10折交叉验证。超过12个终端节点后，回报递减，如图所示（y轴是交叉验证误差，下方x轴是成本复杂度（\\(\\alpha\\)）值，上方x轴是终端节点数（即树大小 \\(|T|\\)））。在实际操作中，建议使用最小交叉验证误差的1标准误差（SE）内的最小树（称为1-SE规则），穿过 \\(|T|=11\\) 点附近的虚线，代表可以使用具有11个终端节点的树，并合理期望在小误差范围内获得相似结果。\n\nplotcp(ames_dt1)\n\n\n\n\n\n\n\n\n修剪复杂度参数（cp）图，展示不同cp值（下方x轴）的相对交叉验证误差（y轴）。较小的cp值导致更大的树（上方x轴），即cp值越小，对模型复杂度的惩罚越轻，树变越大。使用1-SE规则，树大小为10-12时提供最佳交叉验证结果。\n可以通过设置cp = 0（无惩罚导致完全生长的树）强制rpart()生成完整树。在11个终端节点后，误差减少的回报递减。因此，可以修剪完整树，同时实现最小的预期误差。\n\names_dt2 &lt;- rpart(\n    formula = Sale_Price ~ .,\n    data    = ames_train,\n    method  = \"anova\", \n    control = list(cp = 0, xval = 10)\n)\n\nplotcp(ames_dt2)\nabline(v = 11, lty = \"dashed\")\n\n\n\n\n\n\n\n\n默认情况下，rpart()执行了一些自动调参，得到最佳子树，包含10次分割、11个终端节点和交叉验证SSE为0.22。rpart()不提供RMSE或其他指标，但可以使用caret获取。在这两种情况下，较小的惩罚（更深的树）提供了更好的交叉验证结果。\n\n# rpart交叉验证结果\names_dt1$cptable\n##            CP nsplit rel error    xerror       xstd\n## 1  0.47447176      0 1.0000000 1.0013872 0.05662002\n## 2  0.11258635      1 0.5255282 0.5269995 0.02879027\n## 3  0.06144216      2 0.4129419 0.4146178 0.02793907\n## 4  0.02741384      3 0.3514997 0.3624752 0.02241687\n## 5  0.02500852      4 0.3240859 0.3371336 0.02050669\n## 6  0.02117890      5 0.2990774 0.3132772 0.01990067\n## 7  0.01709753      6 0.2778985 0.2999761 0.02033487\n## 8  0.01526499      7 0.2608009 0.2951699 0.02004302\n## 9  0.01241470      8 0.2455360 0.2938842 0.02004706\n## 10 0.01176358      9 0.2331213 0.2854259 0.01964856\n## 11 0.01135317     10 0.2213577 0.2684158 0.01678069\n## 12 0.01000000     11 0.2100045 0.2600794 0.01660921\n\n# caret交叉验证结果\names_dt3 &lt;- train(\n  Sale_Price ~ .,\n  data = ames_train,\n  method = \"rpart\",\n  trControl = trainControl(method = \"cv\", number = 10),\n  tuneLength = 20\n)\n\nggplot(ames_dt3)\n\n\n\n\n\n\n\n\n\n\n\n为了衡量特征重要性，统计每个变量在每次分割中对损失函数（例如，SSE）的减少量。在某些情况下，单个变量可能在树中多次使用；因此，某个变量在所有分割中的损失函数总减少量被加起来，用作总特征重要性。使用caret时，这些值被标准化，使最重要的特征值为100，其余特征根据其相对损失函数减少量评分。此外，由于可能存在重要但未用于分割的候选变量，每次分割还会统计顶级竞争变量。\n图展示了Ames住房决策树的前40个特征。与MARS类似，决策树执行自动特征选择，不使用的特征被视为无信息特征。图中底部四个特征的重要性为零。\n\nvip(ames_dt3, num_features = 40, bar = FALSE)\n\n\n\n\n\n\n\n\n通过部分依赖图，可以看到决策树如何建模特征与目标之间的关系。Gr_Liv_Area具有非线性关系，在1000-2500之间对预测销售价格的影响越来越强，但超过2500后几乎没有影响。Gr_Liv_Area和Year_Built交互效应的三维图显示了决策树与之前MARS的一个关键区别：决策树具有刚性的非平滑预测表面（台阶状的）。MARS是作为CART在回归问题上的改进。\n\n# 构建部分依赖图\np1 &lt;- partial(ames_dt3, pred.var = \"Gr_Liv_Area\") %&gt;% autoplot()\np2 &lt;- partial(ames_dt3, pred.var = \"Year_Built\") %&gt;% autoplot()\np3 &lt;- partial(ames_dt3, pred.var = c(\"Gr_Liv_Area\", \"Year_Built\")) %&gt;% \n  plotPartial(levelplot = FALSE, zlab = \"yhat\", drape = TRUE, \n              colorkey = TRUE, screen = list(z = -20, x = -60))\n\n# 并排显示图\ngridExtra::grid.arrange(p1, p2, p3, ncol = 3)\n\n\n\n\n\n\n\n\n决策树具有许多优点：\n\n决策树需要很少的预处理。特征工程也会改善决策树，但是没有预处理要求。\n\n决策树可以轻松处理分类特征而无需预处理。对于具有两个以上级别的无序分类特征，根据结果对其类别进行排序（对于回归问题，使用响应均值；对于分类问题，使用结果类别的比例）。\n\n大多数决策树实现可以处理特征中的缺失值，不需要插补。最常见的是为分类变量创建新的“缺失”类别等。\n\n然而，单个决策树通常无法达到最先进的预测准确性，因此需要集成模型。"
  },
  {
    "objectID": "bigdata/machlearn/mltrees.html#工具和数据",
    "href": "bigdata/machlearn/mltrees.html#工具和数据",
    "title": "有监督学习：决策树",
    "section": "",
    "text": "使用以下R包：\n\n# 辅助包\nlibrary(dplyr)       # 用于数据处理\nlibrary(ggplot2)     # 用于出色的绘图\n\n# 建模包\nlibrary(rpart)       # 决策树直接引擎\nlibrary(caret)       # 决策树元引擎\n\n# 模型解释包\nlibrary(rpart.plot)  # 用于绘制决策树\nlibrary(vip)         # 用于特征重要性\nlibrary(pdp)         # 用于特征效应\n\n使用Ames住房示例来展示主要概念。\n\names &lt;- AmesHousing::make_ames()\n\nlibrary(rsample)\n# 分层抽样划分训练集和测试集数据\nset.seed(123)\nsplit  &lt;- initial_split(ames, prop = 0.7, strata = \"Sale_Price\")\names_train  &lt;- training(split)\names_test   &lt;- testing(split)"
  },
  {
    "objectID": "bigdata/machlearn/mltrees.html#决策树的结构",
    "href": "bigdata/machlearn/mltrees.html#决策树的结构",
    "title": "有监督学习：决策树",
    "section": "",
    "text": "构建决策树有许多方法，但最常见的是分类与回归树（CART）算法。基本的决策树将训练数据划分为同质子组（即响应值相似的群体），然后在每个子组内拟合一个简单常数（例如，回归问题中子组内响应值的均值）。子组（也称为节点）是通过使用二元分割递归形成，每次分割基于对每个特征的简单“是否”问题进行（例如，年龄&lt;18岁？）。这一过程重复多次，直到满足适当的停止标准（例如，达到树的最大深度）。在所有分区完成后，模型根据不同的问题的需要预测输出，对于回归问题，使用落入该子组的所有观测值的平均响应值输出预测；对于分类问题，使用子组内占多数的类别预测，同时可以使用子组内各类别的比例获得预测概率。\n结果是一个倒挂的树形结构，本质上，决策树是一组规则，允许通过对每个特征提出简单的“是否”问题来进行预测。例如，如果客户是忠诚客户，家庭收入超过15万美元，并且在商店购物，下面示例的决策树将预测客户会兑换优惠券。\n\n将树的顶部第一个子组称为根节点（此节点包含所有训练数据）。树底部的最终子组称为终端节点或叶节点。之间的子组称为内部节点。节点之间的连接称为分支。"
  },
  {
    "objectID": "bigdata/machlearn/mltrees.html#分区",
    "href": "bigdata/machlearn/mltrees.html#分区",
    "title": "有监督学习：决策树",
    "section": "",
    "text": "CART使用二元递归分区（递归是因为每个分割或规则依赖于其上方的分割）。每个节点的目标是找到“最佳”特征（\\(x_i\\)）将剩余数据划分为两个区域（\\(R_1\\) 和 \\(R_2\\)），使实际响应值（\\(y_i\\)）与预测常数（\\(c_i\\)）之间的总体误差最小化。对于回归问题，要最小化的目标函数是总平方误差和（SSE），如方程定义：\n\\[\nSSE = \\sum_{i \\in R_1} (y_i - c_1)^2 + \\sum_{i \\in R_2} (y_i - c_2)^2\n\\]\n对于分类问题，分区通常旨在最大化交叉熵或基尼指数的减少。在回归和分类树中，分区的目标是最小化终端节点中的不相似性。\n找到最佳特征/分割组合后，数据被划分为两个区域，并对这两个区域重复分割过程（因此称为二元递归分区）。此过程持续进行，直到达到适当的停止标准（例如，达到最大深度或树变得“过于复杂”）。\n需要注意的是，单个特征可以在树中多次使用。例如，假设从简单正弦函数加随机噪声生成的数据：\\(Y_i \\stackrel{iid}{\\sim} N(\\sin(X_i), \\sigma^2)\\)，其中 \\(i=1,2,\\dots,500\\)。构建的回归树只有一个根节点（通常称为决策桩），在 \\(x=3.1\\) 处发生分割。\n\n\n\n\n\n\n\n\n\n决策树展示了对特征 \\(x\\) 的单一分割（左）。决策边界显示了当 \\(x &lt; 3.1\\) 时，预测值为0.64；当 \\(x &gt; 3.1\\) 时，预测值为-0.67。\n\n\n\n\n\n\n\n\n\n如果继续构建更深的树，可以在同一特征（\\(x\\)）上继续分割。因为 \\(x\\) 是唯一可用于分割的特征，因此将继续沿该特征的值寻找最佳分割点，直到达到预定的停止标准。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n深度为3的决策树，展示了对特征 \\(x\\) 的7次分割，生成8个预测区域。\n即使有多个特征可用，如果某个特征在每次分区后仍能提供最佳分割，它可能仍然占据主导地位。例如，应用于鸢尾花数据集的决策树，其中根据两个特征（萼片宽度和萼片长度）预测花的种类（setosa、versicolor和virginica），结果生成了一个包含每个特征两次分割的最佳决策树。分类问题的决策边界生成包围观测值的矩形区域。预测值是区域内占比最高的响应类别。"
  },
  {
    "objectID": "bigdata/machlearn/mltrees.html#决策树深度的选择",
    "href": "bigdata/machlearn/mltrees.html#决策树深度的选择",
    "title": "有监督学习：决策树",
    "section": "",
    "text": "这引出了一个重要问题：树应该有多深（即多复杂）？如果构建过于复杂的树，往往会过拟合训练数据，导致泛化性能较差。\n有56次分割的过拟合决策树。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n因此，需要在树的深度和复杂性之间取得平衡，以优化对未来未见数据的预测性能。为了找到这种平衡，有两种主要方法：早期停止和修剪。\n\n\n早期停止明确限制树的生长。可以通过以下几种方式限制树生长，但最常见的方法是限制树深度到某个水平或限制终端节点的最小观测数。限制树深度意味着在达到某个深度后停止分割（例如，仅生长深度为5层的树）。树越浅，预测的方差越小（对训练集数据的变化不敏感）；然而，过浅的树（如决策桩）可能无法捕捉数据中的交互和复杂模式，从而引入过多偏差。\n限制终端节点的最小大小（例如，叶节点必须包含至少10个观测值用于预测），即决定不分割包含过少数据点的中间节点。在极端情况下，终端节点大小为1允许单个观测值被捕获在叶节点中并用作预测（完全拟合训练集数据），会导致高方差和较差的泛化能力。另一方面，较大的值限制进一步分割，从而减少方差。\n这两种方法可以独立实施，然而，它们会相互影响。列展示了树深度如何影响决策边界，行展示了终端节点最小观测数如何影响决策边界。\n\n\n\n\n\n\n\n\n\n\n\n\n另一种在树的深度和复杂性之间取得平衡的方法是不明确指定决策树的深度，而是先生长一个非常大且复杂的树，然后通过修剪找到最佳子树。例如可以使用成本复杂度参数（\\(\\alpha\\)）惩罚目标函数中的终端节点数（\\(T\\)），如方程所示：\n\\[\n\\text{minimize} \\left\\{ SSE + \\alpha |T| \\right\\}\n\\]\n对于给定的 \\(\\alpha\\) 值，我们找到具有最低惩罚误差的最小修剪树。这和之前讨论的Lasso惩罚很像。与正则化方法类似，较小的惩罚倾向于生成更复杂的模型，导致更大的树。而较大的惩罚导致更小的树。因此，随着树变大，SSE的减少必须大于成本复杂度惩罚。通常，可以评估多个 \\(\\alpha\\) 值的模型，并使用交叉验证（CV）确定最佳值，从而找到泛化能力最好的最佳子树。\n为了修剪树，我们首先生长一个过于复杂的树（左），然后使用成本复杂度参数识别最佳子树（右）。"
  },
  {
    "objectID": "bigdata/machlearn/mltrees.html#示例",
    "href": "bigdata/machlearn/mltrees.html#示例",
    "title": "有监督学习：决策树",
    "section": "",
    "text": "可以使用rpart拟合回归树，然后使用rpart.plot可视化。回归树和分类树的拟合过程和可视化输出非常相似。两者都使用公式方法表达模型（类似于lm()）。然而，拟合回归树时，需要设置method = \"anova\"。默认情况下，rpart()会根据响应列的数据类型猜测使用的方法，但明确设置更清晰。\n\names_dt1 &lt;- rpart(\n  formula = Sale_Price ~ .,\n  data    = ames_train,\n  method  = \"anova\"\n)\n\n拟合模型后，可以查看决策树输出。打印关于不同分割的各种信息。例如，从根节点的2049个观测值开始，第一次分割的变量（即提供最大SSE减少的变量）是Overall_Qual。在第一个节点，所有Overall_Qual属于 {Very_Poor, Poor, Fair, Below_Average, Average, Above_Average, Good} 的观测值进入第2分支（2)）。沿此分支的总观测数（1701）、平均销售价格（155897）和SSE（4.109012e+12）。如果查看第3分支（3)），会看到348个Overall_Qual属于 {Very_Good, Excellent, Very_Excellent} 的观测值沿此分支，平均销售价格为303245.90，该区域的SSE为2.838371e+12。所以，Overall_Qual是销售价格的重要预测变量，品质高端的房屋平均销售价格几乎是其他房屋的两倍。\n\names_dt1\n## n= 2049 \n## \n## node), split, n, deviance, yval\n##       * denotes terminal node\n## \n##  1) root 2049 1.321981e+13 180922.60  \n##    2) Overall_Qual=Very_Poor,Poor,Fair,Below_Average,Average,Above_Average,Good 1701 4.109012e+12 155897.00  \n##      4) Neighborhood=North_Ames,Old_Town,Edwards,Sawyer,Mitchell,Brookside,Iowa_DOT_and_Rail_Road,South_and_West_of_Iowa_State_University,Meadow_Village,Briardale,Northpark_Villa,Blueste,Landmark 1023 1.370671e+12 131815.70  \n##        8) First_Flr_SF&lt; 1089.5 661 5.643745e+11 119572.90  \n##         16) Overall_Qual=Very_Poor,Poor,Fair,Below_Average 149 1.016868e+11  90363.56 *\n##         17) Overall_Qual=Average,Above_Average,Good 512 2.985678e+11 128073.30 *\n##        9) First_Flr_SF&gt;=1089.5 362 5.263151e+11 154170.60 *\n##      5) Neighborhood=College_Creek,Somerset,Northridge_Heights,Gilbert,Northwest_Ames,Sawyer_West,Crawford,Timberland,Northridge,Stone_Brook,Clear_Creek,Bloomington_Heights,Veenker,Green_Hills 678 1.249971e+12 192232.10  \n##       10) Gr_Liv_Area&lt; 1725.5 484 5.392924e+11 177594.80  \n##         20) Total_Bsmt_SF&lt; 1295.5 342 2.299978e+11 166044.60 *\n##         21) Total_Bsmt_SF&gt;=1295.5 142 1.537824e+11 205413.00 *\n##       11) Gr_Liv_Area&gt;=1725.5 194 3.482733e+11 228749.90 *\n##    3) Overall_Qual=Very_Good,Excellent,Very_Excellent 348 2.838371e+12 303245.90  \n##      6) Overall_Qual=Very_Good 242 9.801339e+11 271271.60  \n##       12) Gr_Liv_Area&lt; 1920.5 143 2.792781e+11 240517.80 *\n##       13) Gr_Liv_Area&gt;=1920.5 99 3.702480e+11 315693.70 *\n##      7) Overall_Qual=Excellent,Very_Excellent 106 1.045983e+12 376243.90  \n##       14) Gr_Liv_Area&lt; 1956.5 47 8.921667e+10 324506.70 *\n##       15) Gr_Liv_Area&gt;=1956.5 59 7.307403e+11 417458.30  \n##         30) Neighborhood=Edwards,Somerset,Veenker 8 8.904433e+10 269794.20 *\n##         31) Neighborhood=College_Creek,Old_Town,Northridge_Heights,Timberland,Northridge,Stone_Brook 51 4.398958e+11 440621.30  \n##           62) First_Flr_SF&lt; 1829 28 9.425118e+10 391454.60 *\n##           63) First_Flr_SF&gt;=1829 23 1.955579e+11 500476.40 *\n\n可以使用rpart.plot()可视化树模型。rpart.plot()函数有许多绘图选项，可以自行探索。然而，在默认输出中，会显示每个节点的数据百分比和该节点的预测结果。此树包含10个内部节点，12个终端节点。训练数据中有80个特征，但是决策树只在10个特征上进行分区。\n\nrpart.plot(ames_dt1)\n\n\n\n\n\n\n\n\nrpart()会自动应用一系列成本复杂度（\\(\\alpha\\)）值来修剪树，找到最佳的树深度。为了比较每个 \\(\\alpha\\) 值的误差，rpart()默认执行10折交叉验证。超过12个终端节点后，回报递减，如图所示（y轴是交叉验证误差，下方x轴是成本复杂度（\\(\\alpha\\)）值，上方x轴是终端节点数（即树大小 \\(|T|\\)））。在实际操作中，建议使用最小交叉验证误差的1标准误差（SE）内的最小树（称为1-SE规则），穿过 \\(|T|=11\\) 点附近的虚线，代表可以使用具有11个终端节点的树，并合理期望在小误差范围内获得相似结果。\n\nplotcp(ames_dt1)\n\n\n\n\n\n\n\n\n修剪复杂度参数（cp）图，展示不同cp值（下方x轴）的相对交叉验证误差（y轴）。较小的cp值导致更大的树（上方x轴），即cp值越小，对模型复杂度的惩罚越轻，树变越大。使用1-SE规则，树大小为10-12时提供最佳交叉验证结果。\n可以通过设置cp = 0（无惩罚导致完全生长的树）强制rpart()生成完整树。在11个终端节点后，误差减少的回报递减。因此，可以修剪完整树，同时实现最小的预期误差。\n\names_dt2 &lt;- rpart(\n    formula = Sale_Price ~ .,\n    data    = ames_train,\n    method  = \"anova\", \n    control = list(cp = 0, xval = 10)\n)\n\nplotcp(ames_dt2)\nabline(v = 11, lty = \"dashed\")\n\n\n\n\n\n\n\n\n默认情况下，rpart()执行了一些自动调参，得到最佳子树，包含10次分割、11个终端节点和交叉验证SSE为0.22。rpart()不提供RMSE或其他指标，但可以使用caret获取。在这两种情况下，较小的惩罚（更深的树）提供了更好的交叉验证结果。\n\n# rpart交叉验证结果\names_dt1$cptable\n##            CP nsplit rel error    xerror       xstd\n## 1  0.47447176      0 1.0000000 1.0013872 0.05662002\n## 2  0.11258635      1 0.5255282 0.5269995 0.02879027\n## 3  0.06144216      2 0.4129419 0.4146178 0.02793907\n## 4  0.02741384      3 0.3514997 0.3624752 0.02241687\n## 5  0.02500852      4 0.3240859 0.3371336 0.02050669\n## 6  0.02117890      5 0.2990774 0.3132772 0.01990067\n## 7  0.01709753      6 0.2778985 0.2999761 0.02033487\n## 8  0.01526499      7 0.2608009 0.2951699 0.02004302\n## 9  0.01241470      8 0.2455360 0.2938842 0.02004706\n## 10 0.01176358      9 0.2331213 0.2854259 0.01964856\n## 11 0.01135317     10 0.2213577 0.2684158 0.01678069\n## 12 0.01000000     11 0.2100045 0.2600794 0.01660921\n\n# caret交叉验证结果\names_dt3 &lt;- train(\n  Sale_Price ~ .,\n  data = ames_train,\n  method = \"rpart\",\n  trControl = trainControl(method = \"cv\", number = 10),\n  tuneLength = 20\n)\n\nggplot(ames_dt3)"
  },
  {
    "objectID": "bigdata/machlearn/mltrees.html#特征解释",
    "href": "bigdata/machlearn/mltrees.html#特征解释",
    "title": "有监督学习：决策树",
    "section": "",
    "text": "为了衡量特征重要性，统计每个变量在每次分割中对损失函数（例如，SSE）的减少量。在某些情况下，单个变量可能在树中多次使用；因此，某个变量在所有分割中的损失函数总减少量被加起来，用作总特征重要性。使用caret时，这些值被标准化，使最重要的特征值为100，其余特征根据其相对损失函数减少量评分。此外，由于可能存在重要但未用于分割的候选变量，每次分割还会统计顶级竞争变量。\n图展示了Ames住房决策树的前40个特征。与MARS类似，决策树执行自动特征选择，不使用的特征被视为无信息特征。图中底部四个特征的重要性为零。\n\nvip(ames_dt3, num_features = 40, bar = FALSE)\n\n\n\n\n\n\n\n\n通过部分依赖图，可以看到决策树如何建模特征与目标之间的关系。Gr_Liv_Area具有非线性关系，在1000-2500之间对预测销售价格的影响越来越强，但超过2500后几乎没有影响。Gr_Liv_Area和Year_Built交互效应的三维图显示了决策树与之前MARS的一个关键区别：决策树具有刚性的非平滑预测表面（台阶状的）。MARS是作为CART在回归问题上的改进。\n\n# 构建部分依赖图\np1 &lt;- partial(ames_dt3, pred.var = \"Gr_Liv_Area\") %&gt;% autoplot()\np2 &lt;- partial(ames_dt3, pred.var = \"Year_Built\") %&gt;% autoplot()\np3 &lt;- partial(ames_dt3, pred.var = c(\"Gr_Liv_Area\", \"Year_Built\")) %&gt;% \n  plotPartial(levelplot = FALSE, zlab = \"yhat\", drape = TRUE, \n              colorkey = TRUE, screen = list(z = -20, x = -60))\n\n# 并排显示图\ngridExtra::grid.arrange(p1, p2, p3, ncol = 3)\n\n\n\n\n\n\n\n\n决策树具有许多优点：\n\n决策树需要很少的预处理。特征工程也会改善决策树，但是没有预处理要求。\n\n决策树可以轻松处理分类特征而无需预处理。对于具有两个以上级别的无序分类特征，根据结果对其类别进行排序（对于回归问题，使用响应均值；对于分类问题，使用结果类别的比例）。\n\n大多数决策树实现可以处理特征中的缺失值，不需要插补。最常见的是为分类变量创建新的“缺失”类别等。\n\n然而，单个决策树通常无法达到最先进的预测准确性，因此需要集成模型。"
  },
  {
    "objectID": "bigdata/machlearn/mltrees.html#工具与数据",
    "href": "bigdata/machlearn/mltrees.html#工具与数据",
    "title": "有监督学习：决策树",
    "section": "2.1 工具与数据",
    "text": "2.1 工具与数据\n\n# 辅助包\nlibrary(dplyr)       # 用于数据处理\nlibrary(ggplot2)     # 用于出色的绘图\nlibrary(doParallel)  # 用于foreach的并行后端\nlibrary(foreach)     # 用于并行处理的for循环\n\n# 建模包\nlibrary(caret)       # 用于通用模型拟合\nlibrary(rpart)       # 用于拟合决策树\nlibrary(ipred)       # 用于拟合装袋决策树\n\n使用 ames_train 数据集来展示主要概念。"
  },
  {
    "objectID": "bigdata/machlearn/mltrees.html#装袋法使用条件",
    "href": "bigdata/machlearn/mltrees.html#装袋法使用条件",
    "title": "有监督学习：决策树",
    "section": "2.2 装袋法使用条件",
    "text": "2.2 装袋法使用条件\n自举聚合（装袋法）是一种通用的预测模型方法，通过拟合多个版本的预测模型，然后将它们组合（或集成）成一个聚合预测。装袋法是一种相当直接的算法，其中创建 \\(b\\) 个原始训练数据的自举副本，对每个自举样本应用回归或分类算法（通常称为基础学习器或分类器），在回归情境中，通过平均各个基础学习器的预测值生成新预测。对于分类问题，通过多数投票或平均估计的类别概率来组合基础学习器的预测。可以通过方程表示，其中 \\(X\\) 是希望生成预测的记录输入，\\(\\hat{f}_{bag}\\) 是装袋预测，\\(\\hat{f}_1(X), \\hat{f}_2(X), \\dots, \\hat{f}_b(X)\\) 是各个基础学习器的预测。\n\\[\n\\hat{f}_{bag} = \\frac{\\hat{f}_1(X) + \\hat{f}_2(X) + \\dots + \\hat{f}_b(X)}{b} \\tag{10.1}\n\\]\n由于聚合过程，装袋法有效降低了单个基础学习器的方差（即平均化降低了方差）；然而，装袋法并不总是优于单个基础学习器。一些模型的方差比其他模型更大。装袋法对不稳定、高方差的基础学习器尤其有效，因为这些算法的预测输出对训练数据的微小变化会产生重大变化，例如决策树和K近邻（当 \\(k\\) 足够小时）等算法。然而，对于更稳定或高偏差的算法，装袋法对预测输出的改进较小，因为变异性较小（例如，装袋线性回归模型对于足够大的 \\(b\\) 实际上只返回原始预测）。\n装袋法背后的总体理念被称为“群体智慧”（wisdom of the crowd）效应。它本质上意味着大型多样化群体中的信息聚合通常比群体中任何单一成员的决策更好。群体成员越多样化，他们的观点和预测就越多样化，这通常导致更好的聚合信息。\n\n\n\n\n\n\n\n\n\n在图中展示比较了 \\(b=100\\) 个多项式回归模型、MARS模型和CART决策树的装袋效果。可以看到低方差基础学习器（多项式回归）从装袋中获益甚微，而高方差学习器（决策树）获益更多。装袋不仅有助于减少单棵树的变异性（不稳定性），还有助于平滑预测表面。\n最佳性能通常通过装袋50-500棵树获得。具有少量强预测变量的数据集通常需要较少的树；而具有大量噪声或多个强预测变量的数据集可能需要更多树。使用过多树不会导致过拟合。然而，由于运行多个模型，迭代次数越多，计算和时间需求越高。随着这些需求的增加，执行k折交叉验证可能变得计算负担沉重。\n通过装袋创建集成的一个好处是，它基于有放回的重采样，可以提供其自身的内部预测性能估计，即袋外（out-of-bag, OOB）样本。OOB样本可用于测试预测性能，其结果通常与k折交叉验证相当，假设数据集足够大（例如 \\(n \\geq 1,000\\)）。因此，随着数据集变大和装袋迭代增加，通常使用OOB误差估计作为预测性能的代表。\n可以将OOB泛化性能估计视为一种非结构化但免费的交叉验证统计量。"
  },
  {
    "objectID": "bigdata/machlearn/mltrees.html#模型实现",
    "href": "bigdata/machlearn/mltrees.html#模型实现",
    "title": "有监督学习：决策树",
    "section": "2.3 模型实现",
    "text": "2.3 模型实现\n单个决策树在预测Ames住房数据的销售价格时表现不佳，可以使用100棵未修剪的装袋树，而不是单一修剪决策树（不修剪树，保持低偏差和高方差，这时装袋效果最大）。如下代码块所示，获得了比单个（修剪）决策树显著的改进（装袋树的RMSE为26216.47，单棵决策树的RMSE为41019）。\nbagging() 函数来自 ipred 包，使用 nbagg 控制装袋模型中的迭代次数，coob = TRUE 表示使用OOB误差率。默认情况下，bagging() 使用 rpart::rpart() 作为决策树基础学习器，但其他基础学习器也可用。由于装袋只是聚合基础学习器，我们可以照常调整基础学习器的参数。这里，通过 control 参数传递给 rpart()，构建深树（不修剪），每个节点只需两个观测值即可分割。\n\n# 使自举法可重现\nset.seed(123)\n\n# 训练装袋模型\names_bag1 &lt;- bagging(\n  formula = Sale_Price ~ .,\n  data = ames_train,\n  nbagg = 100,  \n  coob = TRUE,\n  control = rpart.control(minsplit = 2, cp = 0)\n)\n\names_bag1\n## \n## Bagging regression trees with 100 bootstrap replications \n## \n## Call: bagging.data.frame(formula = Sale_Price ~ ., data = ames_train, \n##     nbagg = 100, coob = TRUE, control = rpart.control(minsplit = 2, \n##         cp = 0))\n## \n## Out-of-bag estimate of root mean squared error:  26216.47\n\n需要注意的一点是，通常树越多越好。随着添加更多树，可以在更多高方差决策树上进行平均。起初误差会显著减少，但最终误差通常会趋于平稳，表明已达到合适的树数量。一般只需50-100棵树即可稳定误差（在其他情况下可能需要500棵或更多）。对于Ames数据，误差在略超过100棵树时便会稳定，此后通过装袋更多树可能不会显著改进。\n\n\n\n\n\n\n\n\n\n图中显示装袋1-200棵深层未修剪决策树的误差曲线。装袋的好处在170-180左右棵树时达到最优，但是在前100棵树前，误差就已经大量减少。\n还可以在 caret 中应用装袋，并使用10折交叉验证来查看集成模型的泛化能力。可以看到200棵树的交叉验证RMSE与OOB估计值相似（差值为495）。然而，使用OOB误差计算耗时几十秒，而执行以下10折交叉验证大约需要二十多分钟。\n\names_bag2 &lt;- train(\n  Sale_Price ~ .,\n  data = ames_train,\n  method = \"treebag\",\n  trControl = trainControl(method = \"cv\", number = 10),\n  nbagg = 200,  \n  control = rpart.control(minsplit = 2, cp = 0)\n)\n\names_bag2\n## Bagged CART \n## \n## 2049 samples\n##   80 predictor\n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 1844, 1843, 1846, 1844, 1844, 1844, ... \n## Resampling results:\n## \n##   RMSE      Rsquared   MAE    \n##   28315.53  0.8766892  16992.4\n\n# Bagged CART \n# \n# 2049 samples\n#   80 predictor\n# \n# No pre-processing\n# Resampling: Cross-Validated (10 fold) \n# Summary of sample sizes: 1844, 1843, 1846, 1844, 1844, 1844, ... \n# Resampling results:\n# \n#   RMSE      Rsquared   MAE    \n#   28315.53  0.8766892  16992.4"
  },
  {
    "objectID": "bigdata/machlearn/mltrees.html#易于并行化",
    "href": "bigdata/machlearn/mltrees.html#易于并行化",
    "title": "有监督学习：决策树",
    "section": "2.4 易于并行化",
    "text": "2.4 易于并行化\n随着迭代次数的增加，装袋可能需要更多的计算资源。不过，装袋过程是针对每个自举样本独立拟合模型，因此可以单独地并行训练模型后，将结果聚集合并为最终模型。通过访问大型集群或多个计算核心，可以更快地在大型数据集上创建装袋后集成。\n以下代码展示了在Ames住房数据上并行化装袋算法（使用 \\(b=160\\) 棵决策树），使用8个核心并返回测试数据的预测。\n\n# 创建并行套接字集群\ncl &lt;- makeCluster(8) # 使用8个工作进程\nregisterDoParallel(cl) # 注册并行后端\n\n# 并行拟合树并计算测试集的预测\npredictions &lt;- foreach(\n  icount(160), \n  .packages = \"rpart\", \n  .combine = cbind\n  ) %dopar% {\n    # 训练数据的自举副本\n    index &lt;- sample(nrow(ames_train), replace = TRUE)\n    ames_train_boot &lt;- ames_train[index, ]  \n  \n    # 对自举副本拟合树\n    bagged_tree &lt;- rpart(\n      Sale_Price ~ ., \n      control = rpart.control(minsplit = 2, cp = 0),\n      data = ames_train_boot\n    ) \n    \n    predict(bagged_tree, newdata = ames_test)\n}\n\npredictions[1:5, 1:7]\n##   result.1 result.2 result.3 result.4 result.5 result.6 result.7\n## 1   129900   125000   122000   134500   149900   130000   127500\n## 2   193000   185000   189500   169000   185900   181000   195500\n## 3   173000   226500   193000   184000   202500   181000   171500\n## 4   184100   130000   166000   162500   220000   213500   180000\n## 5   246000   278000   275000   286000   230000   226500   262500\n\n然后，可以绘制在不同决策树数量条件下预测的RMSE，测试误差与上面OOB误差的变化趋势很像。\n\npredictions %&gt;%\n  as.data.frame() %&gt;%\n  mutate(\n    observation = 1:n(),\n    actual = ames_test$Sale_Price) %&gt;%\n  tidyr::gather(tree, predicted, -c(observation, actual)) %&gt;%\n  group_by(observation) %&gt;%\n  mutate(tree = stringr::str_extract(tree, '\\\\d+') %&gt;% as.numeric()) %&gt;%\n  ungroup() %&gt;%\n  arrange(observation, tree) %&gt;%\n  group_by(observation) %&gt;%\n  mutate(avg_prediction = cummean(predicted)) %&gt;%\n  group_by(tree) %&gt;%\n  summarize(RMSE = RMSE(avg_prediction, actual)) %&gt;%\n  ggplot(aes(tree, RMSE)) +\n  geom_line() +\n  xlab('决策树数量')\n\n\n\n\n\n\n\n\n最后需要关闭并行集群。\n\n# 关闭并行集群\nstopCluster(cl)"
  },
  {
    "objectID": "bigdata/machlearn/mltrees.html#特征解释-1",
    "href": "bigdata/machlearn/mltrees.html#特征解释-1",
    "title": "有监督学习：决策树",
    "section": "2.5 特征解释",
    "text": "2.5 特征解释\n装袋过程会使得可解释的模型不再具有可解释性。然而，基于给定树中每次分割对损失函数（例如，SSE）的减少量总和来衡量特征重要性，仍然可以推断特征如何影响模型。\n对于每棵树，计算所有分割中损失函数减少的总和。然后，对每个特征在所有树上聚合同一指标。SSE（对于回归）平均减少量最大的特征被认为是最重要的。可以使用 vip 构建 ames_bag2 模型的前40个特征的变量重要性图（VIP）。\n对于单棵决策树，许多无信息特征未在树中使用。然而，装袋使用基于自举样本的多个决策树，所以会有更多特征用于分割。因此，一般而言，装袋模型会涉及更多特征，但单个特征重要性水平会较低。\n\nvip::vip(ames_bag2, num_features = 40, geom = \"point\")\n\n\n\n\n\n\n\n\n装袋模型中特征与预测响应之间的关系可以采用部分依赖图（PDP），它能直观地展现每个特征如何平均影响预测输出。尽管装袋的平均效应降低了最终集成的可解释性，PDP和其他可解释性方法有助于解释任何“黑盒”模型。部分依赖图突出了特征与响应之间可能存在的独特、有时非线性、非单调的关系。下图呈现销售价格与地块面积和临街宽度特征之间的关系。\n\n# 构建部分依赖图\np1 &lt;- pdp::partial(\n  ames_bag2, \n  pred.var = \"Lot_Area\",\n  grid.resolution = 20\n) %&gt;% \n  autoplot()\n\np2 &lt;- pdp::partial(\n  ames_bag2, \n  pred.var = \"Lot_Frontage\", \n  grid.resolution = 20\n) %&gt;% \n  autoplot()\n\ngridExtra::grid.arrange(p1, p2, nrow = 1)\n\n\n\n\n\n\n\n\n装袋法以牺牲可解释性和计算速度为代价，改善了高方差（低偏差）模型的预测准确性。然而，通过使用各种可解释性算法，如VIP和PDP，仍然可以推断装袋模型如何利用特征信息。此外，由于装袋由独立过程组成，该算法易于并行化。\n然而，在装袋树时，仍然存在一个问题。虽然模型构建步骤是独立的，但由于每次分割都考虑所有原始特征，装袋中的树并非完全独立。因为数据结构中存在潜在的强关系，不同自举样本的树通常具有相似的结构，尤其是在树顶部。\n例如，使用波士顿住房数据的不同自举样本创建六棵决策树，会看到树顶部具有相似的结构。尽管有15个预测变量可供分割，所有六棵树的头几次分割都由 lstat 和 rm 变量驱动。\n\n\n\n\n\n\n\n\n\n这种特性被称为树相关性（tree correlation），它阻止装袋进一步降低基础学习器的方差。而下面的随机森林算法可以通过减少这种相关性来扩展和改进装袋决策树，从而提高整体集成的准确性。"
  },
  {
    "objectID": "bigdata/machlearn/mltrees.html#工具与数据-1",
    "href": "bigdata/machlearn/mltrees.html#工具与数据-1",
    "title": "有监督学习：基于决策树的模型",
    "section": "3.1 工具与数据",
    "text": "3.1 工具与数据\n重点在于使用 ranger和 h2o 包实现随机森林。\n\n# 辅助包\nlibrary(dplyr)    # 用于数据处理\nlibrary(ggplot2)  # 用于出色的图形\n\n# 建模包\nlibrary(ranger)   # 随机森林的C++实现\nlibrary(h2o)      # 随机森林的Java实现\n\n继续使用 ames_train 数据集来展示主要概念。"
  },
  {
    "objectID": "bigdata/machlearn/mltrees.html#扩展装袋法",
    "href": "bigdata/machlearn/mltrees.html#扩展装袋法",
    "title": "有监督学习：基于决策树的模型",
    "section": "3.2 扩展装袋法",
    "text": "3.2 扩展装袋法\n随机森林基于决策树和装袋法的基本原理构建。装袋法通过在训练数据的自举副本上构建多棵树，为树的构建过程引入随机成分。然后，装袋法通过聚合所有树的预测来降低整体过程的方差，并提高预测性能。但是，简单地装袋树会导致树相关性（tree correlation），限制了方差减少的效果。\n随机森林通过在树生长过程中注入更多随机性来帮助减少树相关性。具体来说，在装袋过程中生长决策树时，随机森林执行分割变量随机化（split-variable randomization），即每次进行分割时，仅从原始 \\(p\\) 个特征中随机选择 \\(m_{try}\\) 个特征子集进行搜索。通常的默认值是回归问题中 \\(m_{try} = \\frac{p}{3}\\)，分类问题中 \\(m_{try} = \\sqrt{p}\\)，可以看做是一个调优参数。\n回归或分类随机森林的基本算法可概括如下：\n\n给定训练数据集\n选择要构建的树数量（n_trees）\n对于 \\(i = 1\\) 到 n_trees，执行：\n生成原始数据的自举样本\n对自举数据生长回归/分类树\n对于每次分割：\n| 从所有 \\(p\\) 个变量中随机选择 \\(m_{try}\\) 个变量\n| 在 \\(m_{try}\\) 个变量中选择最佳变量/分割点\n| 将节点分割为两个子节点\n结束\n使用典型的树模型停止标准确定树何时完成（但不修剪）\n结束\n输出树的集成\n\n当 \\(m_{try} = p\\) 时，该算法等同于装袋决策树。\n由于算法随机选择自举样本进行训练并在每次分割时随机选择特征子集，生成了更多样化的树集，这往往比装袋树进一步减少树相关性，并显著提高预测能力。"
  },
  {
    "objectID": "bigdata/machlearn/mltrees.html#开箱即用out-of-the-box的性能",
    "href": "bigdata/machlearn/mltrees.html#开箱即用out-of-the-box的性能",
    "title": "有监督学习：基于决策树的模型",
    "section": "3.3 开箱即用（Out-of-the-box）的性能",
    "text": "3.3 开箱即用（Out-of-the-box）的性能\n随机森林之所以流行，是因为它们通常提供非常好的开箱即用性能。尽管有多个可调超参数，但是默认值往往就能产生良好的结果。此外，在流行的机器学习算法中，随机森林在调优时的预测准确性变异性最小。\n例如，使用所有超参数设置为默认值的随机森林模型进行训练，获得的袋外（OOB）RMSE优于之前运行的任何模型（无需任何调优）。\n默认情况下，ranger 将 \\(m_{try}\\) 参数设置为 \\(\\text{floor}(\\sqrt{\\text{特征数量}})\\)；然而，对于回归问题，建议以 \\(\\text{floor}(\\frac{\\text{特征数量}}{3})\\) 开始。设置 respect.unordered.factors = \"order\"，指定如何处理无序因子变量，建议设置为“order”。\n\n# 特征数量\nn_features &lt;- length(setdiff(names(ames_train), \"Sale_Price\"))\n\n# 训练默认随机森林模型\names_rf1 &lt;- ranger(\n  Sale_Price ~ ., \n  data = ames_train,\n  mtry = floor(n_features / 3),\n  respect.unordered.factors = \"order\",\n  seed = 123\n)\n\n# 获取OOB RMSE\n(default_rmse &lt;- sqrt(ames_rf1$prediction.error))\n## [1] 25488.39"
  },
  {
    "objectID": "bigdata/machlearn/mltrees.html#超参数",
    "href": "bigdata/machlearn/mltrees.html#超参数",
    "title": "有监督学习：基于决策树的模型",
    "section": "3.4 超参数",
    "text": "3.4 超参数\n尽管随机森林开箱即用表现良好，但在训练模型时还应考虑几个可调参数。主要考虑的超参数包括：\n\n森林中树的棵数\n每次分割考虑的特征数量：\\(m_{try}\\)\n每棵树的复杂性\n采样方案\n树构建期间使用的分割规则\n\n其中 (1) 和 (2) 对预测准确性影响最大，应始终进行调优。(3) 和 (4) 对预测准确性的影响较小，但仍值得探索。它们还能影响计算效率。(5) 对预测准确性的影响最小，主要用于提高计算效率。\n\n树的棵数\n随机森林中的树的数量，虽然严格来说不是超参数，但树的数量需要足够大以稳定误差率。经验法则是从特征数量的10倍开始；随着其他超参数（如 \\(m_{try}\\) 和节点大小）的调整，可能需要更多或更少的树。更多树提供更稳健和稳定的误差估计和变量重要性度量；然而，计算时间随树数量线性增加。\n建议：从 \\(p \\times 10\\) 棵树开始，并根据需要调整。\nAmes数据有80个特征，从特征数量的10倍开始通常可确保误差估计收敛。\n\n\n\n\n\n\n\n\n\n\n\n\\(m_{try}\\)\n控制随机森林分割变量随机化特征的超参数通常称为 \\(m_{try}\\)，它有助于平衡树根部的相关性与合理的预测强度。对于回归问题，默认值通常为 \\(m_{try} = \\frac{p}{3}\\)，对于分类问题为 \\(m_{try} = \\sqrt{p}\\)。然而，当具有相关关系的预测变量比较少（例如，噪声预测变量）时，较高的 \\(m_{try}\\) 值往往表现更好，因为它更有可能选择信号最强的特征。当有许多预测变量具有相关性时，较低的 \\(m_{try}\\) 可能表现更好。\n建议：将恰好五等分区间[2, p]的4个分割点取值和为中心的默认值作为 \\(m_{try}\\) 值开始。\n对于Ames数据，略低于默认值（26）的 \\(m_{try}\\) 值（21）可提高性能。\n\n\n\n\n\n\n\n\n\n\n\n树复杂性\n随机森林基于单个决策树构建；因此，大多数随机森林模型有一个或多个超参数，允许控制单个树的深度和复杂性。通常包括节点大小、最大深度、最大终端节点数或允许额外分割的所需节点大小等超参数。节点大小是控制树复杂性的最常见超参数，大多数实现使用分类问题为1、回归问题为5的默认值，这些值往往产生良好的结果。然而，如果数据有许多噪声预测变量且较高的 \\(m_{try}\\) 值表现最佳，则增加节点大小（即减少树深度和复杂性）可能提高性能。此外，如果计算时间是一个问题，增加节点大小通常可以显著减少运行时间，且对误差估计的影响较小。\n建议：调整节点大小时，从1-10之间的三个值开始，并根据对准确性和运行时间的影响进行调整。\n增加节点大小以降低树复杂性通常对计算速度（右）的影响大于对误差估计的影响。\n\n\n\n\n\n\n\n\n\n\n\n采样方案\n随机森林的默认采样方案是自举法，其中对100%的观测值进行有放回采样（换句话说，每个自举副本与原始训练数据大小相同）；可以通过调整样本大小以及是否有放回采样来对模型效果产生影响。样本大小参数决定为每棵树的训练抽取多少观测值，减少样本大小会产生更多样化的树，从而降低树间相关性，这可能对预测准确性产生积极影响。如果数据集中有几个主导特征，减少样本大小也有助于最小化树间相关性。\n此外，当有许多具有不同级别数的分类特征（类别变量的各类别的频次差别比较大）时，有放回采样可能导致变量分割选择的偏差。因此，如果有不平衡的类别，无放回采样可提供更少偏差的跨树级别使用。\n建议：评估25%-100%的3-4个样本大小值，如果有不平衡的分类特征，尝试无放回采样。\nAmes数据有几个不平衡的分类特征，如社区、分区、整体质量等。因此，无放回采样提高了性能，因为它导致更少偏差的分割变量选择和更多不相关的树。\n\n\n\n\n\n\n\n\n\n\n\n分割规则\n随机森林树构建期间的默认分割规则包括从（随机选择的 \\(m_{try}\\)）个候选变量的所有分割中选择最小化基尼不纯度（分类问题）或SSE（回归问题）的分割。然而，这些默认分割规则偏向于选择具有更多可能分割的特征（例如，连续变量或具有多个类别的分类变量），而非分割较少的变量（极端情况是只有一种可能分割的二值变量）。条件推理树（conditional inference trees）实现了一种替代分割机制，有助于减少这种变量选择偏差。然而，集成条件推理树在预测准确性方面尚未被证明优于传统方法，且训练时间更长。\n为了提高计算效率，可以随机化分割规则，仅考虑变量的随机子集的可能分割值。如果仅随机选择单一分割值，则称为极随机树（extremely randomized trees）。由于分割点的额外随机性，这种方法通常对预测准确性没有改进，甚至可能有负面影响。\n关于运行时间，极随机树最快，因为分割点完全随机抽取，其次是经典随机森林，而条件推理森林的运行时间最长。\n建议：如果需要显著提高计算时间，尝试完全随机化树；然而，要确保与传统分割规则的预测准确性进行比较，因为这种方法通常对损失函数有负面影响。"
  },
  {
    "objectID": "bigdata/machlearn/mltrees.html#调优策略",
    "href": "bigdata/machlearn/mltrees.html#调优策略",
    "title": "有监督学习：基于决策树的模型",
    "section": "3.5 调优策略",
    "text": "3.5 调优策略\n随着更复杂的算法和更多的超参数，需要考虑调优策略。之前采用的完全笛卡尔网格搜索，即评估所有感兴趣的超参数组合，往往需要较长的计算时间。下面的代码块搜索120种超参数组合，此网格搜索大约需要3分钟。\n\n# 创建超参数网格\nhyper_grid &lt;- expand.grid(\n  mtry = floor(n_features * c(.05, .15, .25, .333, .4)),\n  min.node.size = c(1, 3, 5, 10), \n  replace = c(TRUE, FALSE),                               \n  sample.fraction = c(.5, .63, .8),                       \n  rmse = NA                                               \n)\n\n# 执行完全笛卡尔网格搜索\nfor(i in seq_len(nrow(hyper_grid))) {\n  # 为第i个超参数组合拟合模型\n  fit &lt;- ranger(\n    formula         = Sale_Price ~ ., \n    data            = ames_train, \n    num.trees       = n_features * 10,\n    mtry            = hyper_grid$mtry[i],\n    min.node.size   = hyper_grid$min.node.size[i],\n    replace         = hyper_grid$replace[i],\n    sample.fraction = hyper_grid$sample.fraction[i],\n    verbose         = FALSE,\n    seed            = 123,\n    respect.unordered.factors = 'order',\n  )\n  # 导出OOB误差\n  hyper_grid$rmse[i] &lt;- sqrt(fit$prediction.error)\n}\n\n# 评估前10个模型\nhyper_grid %&gt;%\n  arrange(rmse) %&gt;%\n  mutate(perc_gain = (default_rmse - rmse) / default_rmse * 100) %&gt;%\n  head(10)\n##    mtry min.node.size replace sample.fraction     rmse perc_gain\n## 1    26             1   FALSE             0.8 24713.06  3.041873\n## 2    26             3   FALSE             0.8 24847.98  2.512570\n## 3    20             3   FALSE             0.8 24917.05  2.241554\n## 4    20             1   FALSE             0.8 24929.10  2.194284\n## 5    32             5   FALSE             0.8 24940.14  2.150967\n## 6    32             1   FALSE             0.8 24978.78  1.999392\n## 7    32             3   FALSE             0.8 24990.83  1.952085\n## 8    26             5   FALSE             0.8 25004.10  1.900044\n## 9    20             5   FALSE             0.8 25028.46  1.804464\n## 10   12             1   FALSE             0.8 25029.93  1.798693\n\n从结果来看，前10个模型的RMSE接近或低于25000（比基线模型提高了2.5%-3.5%）。在这些结果中，默认 \\(m_{try}\\) 值 \\(\\lfloor \\frac{\\text{特征数量}}{3} \\rfloor = 26\\) 几乎足够，较小的节点大小（更深的树）表现最佳。最突出的是，采样率低于100%且无放回采样始终表现最佳。低于100%的采样率增加了程序的随机性，有助于进一步降低树的相关性。无放回采样可能提高性能，因为该数据有许多多类别的不平衡的分类特征。\n然而，随着超参数和搜索值的增加以及数据集的扩大，完全笛卡尔搜索可能变得耗时且计算成本高。除了完全笛卡尔搜索，h2o 包提供随机网格搜索，允许从一个随机组合跳转到另一个，并提供早期停止规则，允许在满足特定条件（例如，训练了特定数量的模型、经过特定运行时间或准确性的提升非常小）时停止网格搜索。尽管随机离散搜索路径可能无法找到最优模型，但通常能找到一个已经足够好的模型。\n要使用 h2o 拟合随机森林模型，首先需要启动 h2o 会话。\n\nh2o.no_progress()\nh2o.init(max_mem_size = \"5g\")\n##  Connection successful!\n## \n## R is connected to the H2O cluster: \n##     H2O cluster uptime:         1 hours 6 minutes \n##     H2O cluster timezone:       Asia/Shanghai \n##     H2O data parsing timezone:  UTC \n##     H2O cluster version:        3.44.0.3 \n##     H2O cluster version age:    1 year, 10 months and 28 days \n##     H2O cluster name:           H2O_started_from_R_DELL_cfj836 \n##     H2O cluster total nodes:    1 \n##     H2O cluster total memory:   4.32 GB \n##     H2O cluster total cores:    16 \n##     H2O cluster allowed cores:  16 \n##     H2O cluster healthy:        TRUE \n##     H2O Connection ip:          localhost \n##     H2O Connection port:        54321 \n##     H2O Connection proxy:       NA \n##     H2O Internal Security:      FALSE \n##     R Version:                  R version 4.5.1 (2025-06-13 ucrt)\n\n接下来，我们需要将训练和测试数据集转换为 h2o 可处理的对象。\n\n# 将训练数据转换为h2o对象\ntrain_h2o &lt;- as.h2o(ames_train)\n\n# 设置响应列为Sale_Price\nresponse &lt;- \"Sale_Price\"\n\n# 设置预测变量名称\npredictors &lt;- setdiff(colnames(ames_train), response)\n\n以下代码使用 h2o 拟合默认随机森林模型，展示基线结果（OOB RMSE = 25045.8）与之前拟合的 ranger 基线模型非常相似。\n\nh2o_rf1 &lt;- h2o.randomForest(\n    x = predictors, \n    y = response,\n    training_frame = train_h2o, \n    ntrees = n_features * 10,\n    seed = 123\n)\n\nh2o_rf1\n## Model Details:\n## ==============\n## \n## H2ORegressionModel: drf\n## Model ID:  DRF_model_R_1763370872294_1 \n## Model Summary: \n##   number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n## 1             800                      800            12318766        19\n##   max_depth mean_depth min_leaves max_leaves mean_leaves\n## 1        20   19.99875       1128       1286  1220.41130\n## \n## \n## H2ORegressionMetrics: drf\n## ** Reported on training data. **\n## ** Metrics reported on Out-Of-Bag training samples **\n## \n## MSE:  627291937\n## RMSE:  25045.8\n## MAE:  15236.82\n## RMSLE:  0.1415736\n## Mean Residual Deviance :  627291937\n\n要在 h2o 中执行网格搜索，需要将超参数网格设置为列表。例如，以下代码搜索比之前更大的网格空间，共240个超参数组合。然后，创建随机网格搜索策略，如果最后10个模型与之前的最佳模型相比，没有哪个模型的MSE改进能够达到0.1%，则停止。如果还能继续搜索改进，在300秒（5分钟）后也中断网格搜索。\n\n# 超参数网格\nhyper_grid &lt;- list(\n  mtries = floor(n_features * c(.05, .15, .25, .333, .4)),\n  min_rows = c(1, 3, 5, 10),\n  max_depth = c(10, 20, 30),\n  sample_rate = c(.55, .632, .70, .80)\n)\n\n# 随机网格搜索策略\nsearch_criteria &lt;- list(\n  strategy = \"RandomDiscrete\",\n  stopping_metric = \"mse\",\n  stopping_tolerance = 0.001,   # 如果改进&lt;0.1%则停止\n  stopping_rounds = 10,         # 在最后10个模型上\n  max_runtime_secs = 60*5      # 或在5分钟后停止搜索\n)\n\n然后，可以使用 h2o.grid() 执行网格搜索。以下代码启用早期停止执行网格搜索。在 h2o.grid() 中指定的早期停止方式是在最后10棵树中，如果单个随机森林模型的整体OOB误差改进小于0.05%时则停止生长单个随机森林模型。这可能可以大大减少单个随机森林模型的构建复杂度。此网格搜索需要5分钟。\n\n# 执行网格搜索\nrandom_grid &lt;- h2o.grid(\n  algorithm = \"randomForest\",\n  grid_id = \"rf_random_grid\",\n  x = predictors, \n  y = response, \n  training_frame = train_h2o,\n  hyper_params = hyper_grid,\n  ntrees = n_features * 10,\n  seed = 123,\n  stopping_metric = \"RMSE\",   \n  stopping_rounds = 10,           # 如果最后10棵树没有改进\n  stopping_tolerance = 0.005,     # RMSE没有0.5%的改进\n  search_criteria = search_criteria\n)\n\n该网格搜索在时间停止前评估了129个模型。最佳模型（max_depth = 30, min_rows = 1, mtries = 32, sample_rate = 0.7）实现了OOB RMSE为25030。因此，尽管随机搜索评估的模型数量仅为完全网格搜索的约53%，更有效的随机搜索在指定时间约束内找到了接近最优的模型。\n\n# 收集结果并按我们的模型性能指标排序\nrandom_grid_perf &lt;- h2o.getGrid(\n  grid_id = \"rf_random_grid\", \n  sort_by = \"mse\", \n  decreasing = FALSE\n)\n\nrandom_grid_perf\n## H2O Grid Details\n## ================\n## \n## Grid ID: rf_random_grid \n## Used hyper parameters: \n##   -  max_depth \n##   -  min_rows \n##   -  mtries \n##   -  sample_rate \n## Number of models: 124 \n## Number of failed models: 0 \n## \n## Hyper-Parameter Search Summary: ordered by increasing mse\n##   max_depth min_rows   mtries sample_rate                model_ids\n## 1  30.00000  1.00000 26.00000     0.80000   rf_random_grid_model_6\n## 2  30.00000  1.00000 26.00000     0.70000  rf_random_grid_model_12\n## 3  30.00000  1.00000 32.00000     0.80000  rf_random_grid_model_31\n## 4  30.00000  1.00000 32.00000     0.70000 rf_random_grid_model_124\n## 5  10.00000  1.00000 32.00000     0.70000  rf_random_grid_model_58\n##               mse\n## 1 617795275.89754\n## 2 620841940.47215\n## 3 621158623.68221\n## 4 625972753.73725\n## 5 642445330.04744\n## \n## ---\n##     max_depth min_rows  mtries sample_rate               model_ids\n## 119  30.00000  5.00000 4.00000     0.70000 rf_random_grid_model_81\n## 120  30.00000  5.00000 4.00000     0.55000 rf_random_grid_model_29\n## 121  10.00000  5.00000 4.00000     0.63200 rf_random_grid_model_74\n## 122  20.00000 10.00000 4.00000     0.80000 rf_random_grid_model_53\n## 123  20.00000 10.00000 4.00000     0.70000 rf_random_grid_model_24\n## 124  10.00000 10.00000 4.00000     0.70000 rf_random_grid_model_27\n##                  mse\n## 119 1012083714.25510\n## 120 1017942060.64807\n## 121 1023796531.48220\n## 122 1077752366.41779\n## 123 1088755880.92979\n## 124 1092935860.64335"
  },
  {
    "objectID": "bigdata/machlearn/mltrees.html#特征解释-2",
    "href": "bigdata/machlearn/mltrees.html#特征解释-2",
    "title": "有监督学习：基于决策树的模型",
    "section": "3.6 特征解释",
    "text": "3.6 特征解释\n随机森林的特征重要性和特征效应计算遵循之前讨论的过程。然而，除了基于不纯度的特征重要性度量（其中特征重要性基于所有树中给定特征的损失函数平均总减少量）外，随机森林通常还包括基于排列的特征重要性度量。在基于排列的方法中，对于每棵树，将OOB样本传递到树中并记录预测准确性。然后，逐一随机打乱每个变量的值并再次计算准确性。由于随机打乱特征值导致的准确性下降在所有树上对每个预测变量取平均值。准确性平均下降最大的变量被认为是最重要的。\n例如，可以通过设置 ranger 的 importance 参数来计算两种特征重要性度量。\n对于 ranger，一旦从网格搜索中确定了最优参数值，需要使用这些超参数值重新运行模型。还可以增加树的数量，这有助于创建更稳定的变量重要性值。\n\n# 使用基于不纯度的变量重要性重新运行模型\nrf_impurity &lt;- ranger(\n  formula = Sale_Price ~ ., \n  data = ames_train, \n  num.trees = 2000,\n  mtry = 32,\n  min.node.size = 1,\n  sample.fraction = .80,\n  replace = FALSE,\n  importance = \"impurity\",\n  respect.unordered.factors = \"order\",\n  verbose = FALSE,\n  seed  = 123\n)\n\n# 使用基于排列的变量重要性重新运行模型\nrf_permutation &lt;- ranger(\n  formula = Sale_Price ~ ., \n  data = ames_train, \n  num.trees = 2000,\n  mtry = 32,\n  min.node.size = 1,\n  sample.fraction = .80,\n  replace = FALSE,\n  importance = \"permutation\",\n  respect.unordered.factors = \"order\",\n  verbose = FALSE,\n  seed  = 123\n)\n\n结果的变量重要性图（VIP）如图11.5所示。通常，两种方法下的变量重要性顺序不同；通常会在图的顶部（和底部）看到相似的变量。因此，在本例中，可以有信心地说，有足够证据表明以下三个变量最具影响力：\n\nOverall_Qual\nGr_Liv_Area\nNeighborhood\n\n查看两个图中的接下来的约10个变量，还会看到一些共同的影响变量（例如，Garage_Cars、Exter_Qual、Bsmt_Qual 和 Year_Built）。\n\np1 &lt;- vip::vip(rf_impurity, num_features = 25, bar = FALSE)\np2 &lt;- vip::vip(rf_permutation, num_features = 25, bar = FALSE)\n\ngridExtra::grid.arrange(p1, p2, nrow = 1)\n\n\n\n\n\n\n\n\n基于不纯度（左）和排列（右）的前25个最重要的变量。\n随机森林提供了一种非常强大的开箱即用算法，通常具有出色的预测准确性。它们具有决策树（除了代理分割外）和装袋法的所有优点，但大大降低了不稳定性和树间相关性。由于增加了分割变量选择属性，随机森林比装袋法更快，因为每次树分割的特征搜索空间更小。然而，随着数据集的扩大，随机森林仍会面临计算速度慢的问题，但与装袋法类似，该算法基于独立步骤，大多数现代实现（例如，ranger、h2o）允许并行化以改善训练时间。"
  },
  {
    "objectID": "bigdata/machlearn/mlforest.html",
    "href": "bigdata/machlearn/mlforest.html",
    "title": "有监督学习：随机森林与梯度提升",
    "section": "",
    "text": "随机森林是对装袋决策树的改进，通过构建大量去相关（de-correlated）的树来进一步提高预测性能。随机森林已成为一种非常流行的“开箱即用”或“现成”学习算法，凭借相对较少的超参数调优即可获得良好的预测性能。\n\n\n重点在于使用 ranger和 h2o 包实现随机森林。\n\n# 辅助包\nlibrary(dplyr)    # 用于数据处理\nlibrary(ggplot2)  # 用于出色的图形\n\n# 建模包\nlibrary(ranger)   # 随机森林的C++实现\nlibrary(h2o)      # 随机森林的Java实现\n\n\n----------------------------------------------------------------------\n\nYour next step is to start H2O:\n    &gt; h2o.init()\n\nFor H2O package documentation, ask for help:\n    &gt; ??h2o\n\nAfter starting H2O, you can use the Web UI at http://localhost:54321\nFor more information visit https://docs.h2o.ai\n\n----------------------------------------------------------------------\n\n\n\nAttaching package: 'h2o'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    day, hour, month, week, year\n\n\nThe following objects are masked from 'package:stats':\n\n    cor, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    &&, %*%, %in%, ||, apply, as.factor, as.numeric, colnames,\n    colnames&lt;-, ifelse, is.character, is.factor, is.numeric, log,\n    log10, log1p, log2, round, signif, trunc\n\n\n继续使用 ames_train 数据集来展示主要概念。\n\names &lt;- AmesHousing::make_ames()\n\nlibrary(rsample)\n# 分层抽样划分训练集和测试集数据\nset.seed(123)\nsplit  &lt;- initial_split(ames, prop = 0.7, strata = \"Sale_Price\")\names_train  &lt;- training(split)\names_test   &lt;- testing(split)\n\n\n\n\n随机森林基于决策树和装袋法的基本原理构建。装袋法通过在训练数据的自举副本上构建多棵树，为树的构建过程引入随机成分。然后，装袋法通过聚合所有树的预测来降低整体过程的方差，并提高预测性能。但是，简单地装袋树会导致树相关性（tree correlation），限制了方差减少的效果。\n随机森林通过在树生长过程中注入更多随机性来帮助减少树相关性。具体来说，在装袋过程中生长决策树时，随机森林执行分割变量随机化（split-variable randomization），即每次进行分割时，仅从原始 \\(p\\) 个特征中随机选择 \\(m_{try}\\) 个特征子集进行搜索。通常的默认值是回归问题中 \\(m_{try} = \\frac{p}{3}\\)，分类问题中 \\(m_{try} = \\sqrt{p}\\)，可以看做是一个调优参数。\n回归或分类随机森林的基本算法可概括如下：\n\n给定训练数据集\n选择要构建的树数量（n_trees）\n对于 \\(i = 1\\) 到 n_trees，执行：\n生成原始数据的自举样本\n对自举数据生长回归/分类树\n对于每次分割：\n| 从所有 \\(p\\) 个变量中随机选择 \\(m_{try}\\) 个变量\n| 在 \\(m_{try}\\) 个变量中选择最佳变量/分割点\n| 将节点分割为两个子节点\n结束\n使用典型的树模型停止标准确定树何时完成（但不修剪）\n结束\n输出树的集成\n\n当 \\(m_{try} = p\\) 时，该算法等同于装袋决策树。\n由于算法随机选择自举样本进行训练并在每次分割时随机选择特征子集，生成了更多样化的树集，这往往比装袋树进一步减少树相关性，并显著提高预测能力。\n\n\n\n随机森林之所以流行，是因为它们通常提供非常好的开箱即用性能。尽管有多个可调超参数，但是默认值往往就能产生良好的结果。此外，与流行的机器学习算法相比，随机森林在调优时的预测准确性变异性最小。\n例如，使用所有超参数设置为默认值的随机森林模型进行训练，获得的袋外（OOB）RMSE优于之前运行的任何模型（无需任何调优）。\n默认情况下，ranger 将 \\(m_{try}\\) 参数设置为 \\(\\text{floor}(\\sqrt{\\text{特征数量}})\\)；然而，对于回归问题，建议以 \\(\\text{floor}(\\frac{\\text{特征数量}}{3})\\) 开始。设置 respect.unordered.factors = \"order\"，指定如何处理无序因子变量，建议设置为“order”。\n\n# 特征数量\nn_features &lt;- length(setdiff(names(ames_train), \"Sale_Price\"))\n\n# 训练默认随机森林模型\names_rf1 &lt;- ranger(\n  Sale_Price ~ ., \n  data = ames_train,\n  mtry = floor(n_features / 3),\n  respect.unordered.factors = \"order\",\n  seed = 123\n)\n\n# 获取OOB RMSE\n(default_rmse &lt;- sqrt(ames_rf1$prediction.error))\n\n[1] 25423.06\n\n\n\n\n\n尽管随机森林开箱即用表现良好，但在训练模型时还应考虑几个可调参数。主要考虑的超参数包括：\n\n森林中树的棵数\n每次分割考虑的特征数量：\\(m_{try}\\)\n每棵树的复杂性\n采样方案\n树构建期间使用的分割规则\n\n其中 (1) 和 (2) 对预测准确性影响最大，应始终进行调优。(3) 和 (4) 对预测准确性的影响较小，但仍值得探索。它们还能影响计算效率。(5) 对预测准确性的影响最小，主要用于提高计算效率。\n\n\n随机森林中的树的数量，虽然严格来说不是超参数，但树的数量需要足够大以稳定误差率。经验法则是从特征数量的10倍开始；随着其他超参数（如 \\(m_{try}\\) 和节点大小）的调整，可能需要更多或更少的树。更多树提供更稳健和稳定的误差估计和变量重要性度量；然而，计算时间随树数量线性增加。\n建议：从 \\(p \\times 10\\) 棵树开始，并根据需要调整。\n\n\n\n控制随机森林分割变量随机化特征的超参数通常称为 \\(m_{try}\\)，它有助于平衡树根部的相关性与合理的预测强度。对于回归问题，默认值通常为 \\(m_{try} = \\frac{p}{3}\\)，对于分类问题为 \\(m_{try} = \\sqrt{p}\\)。然而，当具有相关关系的预测变量比较少（例如，噪声预测变量）时，较高的 \\(m_{try}\\) 值往往表现更好，因为它更有可能选择信号最强的特征。当有许多预测变量具有相关性时，较低的 \\(m_{try}\\) 可能表现更好。\n建议：将恰好五等分区间[2, p]的4个分割点取值和为中心的默认值作为 \\(m_{try}\\) 值开始。\n\n\n\n随机森林基于单个决策树构建；因此，大多数随机森林模型有一个或多个超参数来控制单个树的深度和复杂性。通常包括节点大小、最大深度、最大终端节点数或允许额外分割的所需节点大小等超参数。节点大小是控制树复杂性的最常见超参数，一般分类问题设定为1、回归问题设定为5的默认值。然而，如果数据有许多噪声预测变量且较高的 \\(m_{try}\\) 值时表现较好，则应增加节点大小（即减少树深度和复杂性）能提高性能。此外，如果要考虑计算时间，增加节点大小通常可以显著减少运行时间，且对误差估计的影响较小。\n建议：调整节点大小时，从1-10之间的三个值开始，并根据对准确性和运行时间的影响进行调整。\n\n\n\n随机森林的默认采样方案是自举法，其中对100%的观测值进行有放回采样（即每个自举副本与原始训练数据大小相同）；可以通过调整样本大小以及是否有放回采样来对模型效果产生影响。样本大小参数决定为每棵树的训练抽取多少观测值，减少样本大小会产生更多样化的树，从而降低树间相关性，这会对预测准确性产生积极影响。如果数据集中有几个主导特征，减少样本大小也有助于最小化树间相关性。\n此外，当有许多具有不同级别数的分类特征（类别变量的各类别的频次差别比较大）时，有放回采样可能导致变量分割选择的偏差。因此，如果有不平衡的类别，无放回采样可产生更少偏差。\n建议：评估25%-100%的3-4个样本大小值，如果有不平衡的分类特征，尝试无放回采样。\n\n\n\n随机森林树构建期间的默认分割规则包括从（随机选择的 \\(m_{try}\\)）个候选变量的所有分割中选择最小化基尼不纯度（分类问题）或SSE（回归问题）的分割。然而，这些默认分割规则偏向于选择具有更多可能分割的特征（例如，连续变量或具有多个类别的分类变量），而非分割较少的变量（极端情况是只有一种可能分割的二值变量）。条件推理树（conditional inference trees）采用替代分割机制，可以减少变量选择偏差，但是训练时间更长。\n为了提高计算效率，可以随机化分割规则，仅考虑变量的随机子集的可能分割值。如果仅随机选择单一分割值，则称为极随机树（extremely randomized trees）。\n从运行时间来看，极随机树最快，其次是经典随机森林，而条件推理森林的运行时间最长。\n建议：如果需要显著改善计算时间，可以尝试完全随机化树；然而，要确保与传统分割规则的预测准确性进行比较，因为这种方法通常对损失函数有负面影响。\n\n\n\n\n随着更复杂的算法和更多的超参数，需要考虑调优策略。之前采用的完全笛卡尔网格搜索，即评估所有感兴趣的超参数组合，往往需要较长的计算时间。下面的代码块搜索120种超参数组合，此网格搜索大约需要3分钟。\n\n# 创建超参数网格\nhyper_grid &lt;- expand.grid(\n  mtry = floor(n_features * c(.05, .15, .25, .333, .4)),\n  min.node.size = c(1, 3, 5, 10), \n  replace = c(TRUE, FALSE),                               \n  sample.fraction = c(.5, .63, .8),                       \n  rmse = NA                                               \n)\n\n# 执行完全笛卡尔网格搜索\nfor(i in seq_len(nrow(hyper_grid))) {\n  # 为第i个超参数组合拟合模型\n  fit &lt;- ranger(\n    formula         = Sale_Price ~ ., \n    data            = ames_train, \n    num.trees       = n_features * 10,\n    mtry            = hyper_grid$mtry[i],\n    min.node.size   = hyper_grid$min.node.size[i],\n    replace         = hyper_grid$replace[i],\n    sample.fraction = hyper_grid$sample.fraction[i],\n    verbose         = FALSE,\n    seed            = 123,\n    respect.unordered.factors = 'order',\n  )\n  # 导出OOB误差\n  hyper_grid$rmse[i] &lt;- sqrt(fit$prediction.error)\n}\n\n# 评估前10个模型\nhyper_grid %&gt;%\n  arrange(rmse) %&gt;%\n  mutate(perc_gain = (default_rmse - rmse) / default_rmse * 100) %&gt;%\n  head(10)\n\n   mtry min.node.size replace sample.fraction     rmse perc_gain\n1    26             1   FALSE             0.8 24910.63  2.015610\n2    12             1   FALSE             0.8 24984.93  1.723339\n3    12             5   FALSE             0.8 25034.17  1.529646\n4    20             1   FALSE             0.8 25040.19  1.505984\n5    32             1   FALSE             0.8 25049.52  1.469293\n6    20             3   FALSE             0.8 25056.39  1.442254\n7    26             3   FALSE             0.8 25074.46  1.371189\n8    26             5   FALSE             0.8 25113.53  1.217501\n9    12             3   FALSE             0.8 25113.95  1.215830\n10   32             3   FALSE             0.8 25160.93  1.031053\n\n\n从结果来看，前10个模型的RMSE接近或低于25000（比基线模型提高了2.5%-3.5%）。在这些结果中，默认 \\(m_{try}\\) 值 \\(\\lfloor \\frac{\\text{特征数量}}{3} \\rfloor = 26\\) 就是比较理想的参数，较小的节点大小（更深的树）表现最佳。最突出的是，采样率低于100%且无放回采样始终表现最佳。低于100%的采样率增加了程序的随机性，有助于进一步降低树的相关性。无放回采样能提高性能，是因为该数据有许多多类别的不平衡的分类特征。\n然而，随着超参数和搜索值的增加以及数据集的扩大，完全笛卡尔搜索可能变得耗时且计算成本高。可以采用h2o 包提供的随机网格搜索，能够从一个随机组合跳转到另一个，并提供早期停止规则，在满足特定条件（例如，训练了特定数量的模型、经过特定运行时间或准确性的提升非常小）时停止网格搜索。尽管随机离散搜索路径可能无法找到最优模型，但通常能找到一个已经足够好的模型。\n要使用 h2o 拟合随机森林模型，首先需要启动 h2o 会话。\n\nh2o.no_progress()\nh2o.init(max_mem_size = \"5g\")\n\n Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         8 days 5 hours \n    H2O cluster timezone:       Asia/Shanghai \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.44.0.3 \n    H2O cluster version age:    1 year, 11 months and 17 days \n    H2O cluster name:           H2O_started_from_R_liangdan_ats696 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   0.84 GB \n    H2O cluster total cores:    10 \n    H2O cluster allowed cores:  10 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.4.2 (2024-10-31) \n\n\nWarning in h2o.clusterInfo(): \nYour H2O cluster version is (1 year, 11 months and 17 days) old. There may be a newer version available.\nPlease download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n\n\n接下来，我们需要将训练和测试数据集转换为 h2o 可处理的对象。\n\n# 将训练数据转换为h2o对象\ntrain_h2o &lt;- as.h2o(ames_train)\n\n# 设置响应列为Sale_Price\nresponse &lt;- \"Sale_Price\"\n\n# 设置预测变量名称\npredictors &lt;- setdiff(colnames(ames_train), response)\n\n以下代码使用 h2o 拟合默认随机森林模型，展示基线结果（OOB RMSE = 25045.8）与之前拟合的 ranger 基线模型非常相似。\n\nh2o_rf1 &lt;- h2o.randomForest(\n    x = predictors, \n    y = response,\n    training_frame = train_h2o, \n    ntrees = n_features * 10,\n    seed = 123\n)\n\nh2o_rf1\n\nModel Details:\n==============\n\nH2ORegressionModel: drf\nModel ID:  DRF_model_R_1764404962582_6 \nModel Summary: \n  number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n1             800                      800            12318752        19\n  max_depth mean_depth min_leaves max_leaves mean_leaves\n1        20   19.99875       1128       1286  1220.41130\n\n\nH2ORegressionMetrics: drf\n** Reported on training data. **\n** Metrics reported on Out-Of-Bag training samples **\n\nMSE:  627291937\nRMSE:  25045.8\nMAE:  15236.82\nRMSLE:  0.1415736\nMean Residual Deviance :  627291937\n\n\n要在 h2o 中执行网格搜索，需要将超参数网格设置为列表。例如，以下代码搜索比之前更大的网格空间，共240个超参数组合。然后，创建随机网格搜索策略，如果最后10个模型与之前的最佳模型相比，没有哪个模型的MSE改进能够达到0.1%，则停止。如果还能继续搜索改进，在300秒（5分钟）后也中断网格搜索。\n\n# 超参数网格\nhyper_grid &lt;- list(\n  mtries = floor(n_features * c(.05, .15, .25, .333, .4)),\n  min_rows = c(1, 3, 5, 10),\n  max_depth = c(10, 20, 30),\n  sample_rate = c(.55, .632, .70, .80)\n)\n\n# 随机网格搜索策略\nsearch_criteria &lt;- list(\n  strategy = \"RandomDiscrete\",\n  stopping_metric = \"mse\",\n  stopping_tolerance = 0.001,   # 如果改进&lt;0.1%则停止\n  stopping_rounds = 10,         # 在最后10个模型上\n  max_runtime_secs = 60*2      # 或在2分钟后停止搜索\n)\n\n然后，可以使用 h2o.grid() 执行网格搜索。以下代码启用早期停止执行网格搜索。在 h2o.grid() 中指定的早期停止方式是在最后10棵树中，如果单个随机森林模型的整体OOB误差改进小于0.05%时则停止生长单个随机森林模型。这可能可以大大减少单个随机森林模型的构建复杂度。此网格搜索需要2分钟。\n\nif (\"rf_random_grid\" %in% h2o.ls()$key) {\n  h2o.rm(\"rf_random_grid\")\n}   # 剔除由于反复运行可能会出现的id重复的对象\n\n# 执行网格搜索\nrandom_grid &lt;- h2o.grid(\n  algorithm = \"randomForest\",\n  grid_id = \"rf_random_grid\",\n  x = predictors, \n  y = response, \n  training_frame = train_h2o,\n  hyper_params = hyper_grid,\n  ntrees = n_features * 10,\n  seed = 123,\n  stopping_metric = \"RMSE\",   \n  stopping_rounds = 10,           # 如果最后10棵树没有改进\n  stopping_tolerance = 0.005,     # RMSE没有0.5%的改进\n  search_criteria = search_criteria\n)\n\n该网格搜索在时间停止前评估了137个模型。最佳模型（max_depth = 10, min_rows = 1, mtries = 32, sample_rate = 0.7）实现了OOB RMSE为25346.50。因此，尽管随机搜索评估的模型数量仅为完全网格搜索的约53%，更有效的随机搜索在指定时间约束内找到了接近最优的模型。\n\n# 收集结果并按我们的模型性能指标排序\nrandom_grid_perf &lt;- h2o.getGrid(\n  grid_id = \"rf_random_grid\", \n  sort_by = \"rmse\", \n  decreasing = FALSE\n)\n\nrandom_grid_perf\n\nH2O Grid Details\n================\n\nGrid ID: rf_random_grid \nUsed hyper parameters: \n  -  max_depth \n  -  min_rows \n  -  mtries \n  -  sample_rate \nNumber of models: 122 \nNumber of failed models: 0 \n\nHyper-Parameter Search Summary: ordered by increasing rmse\n  max_depth min_rows   mtries sample_rate               model_ids        rmse\n1  20.00000  1.00000 32.00000     0.70000 rf_random_grid_model_73 25032.02025\n2  30.00000  1.00000 32.00000     0.80000 rf_random_grid_model_50 25406.24586\n3  20.00000  1.00000 26.00000     0.63200 rf_random_grid_model_74 25563.08628\n4  30.00000  1.00000 20.00000     0.70000 rf_random_grid_model_55 25582.26141\n5  20.00000  1.00000 20.00000     0.70000 rf_random_grid_model_37 25583.02405\n\n---\n    max_depth min_rows  mtries sample_rate                model_ids        rmse\n117  20.00000 10.00000 4.00000     0.70000  rf_random_grid_model_65 32996.30102\n118  30.00000 10.00000 4.00000     0.70000  rf_random_grid_model_95 32996.30102\n119  20.00000 10.00000 4.00000     0.63200  rf_random_grid_model_78 33051.43114\n120  10.00000 10.00000 4.00000     0.70000  rf_random_grid_model_10 33059.58047\n121  10.00000 10.00000 4.00000     0.63200 rf_random_grid_model_112 33112.30804\n122  10.00000 10.00000 4.00000     0.55000  rf_random_grid_model_58 33576.85787\n\n\n\n\n\n随机森林的特征重要性和特征效应计算与决策树和装袋法相似。然而，除了基于不纯度的特征重要性度量（其中特征重要性基于所有树中给定特征的损失函数平均总减少量）外，随机森林通常还包括基于排列的特征重要性度量。在基于排列的方法中，对于每棵树，将OOB样本传递到树中并记录预测准确性。然后，逐一随机打乱每个变量的值并再次计算准确性。由于随机打乱特征值导致的准确性下降在所有树上对每个预测变量取平均值。准确性平均下降最大的变量被认为是最重要的。\n例如，可以通过设置 ranger 的 importance 参数来计算两种特征重要性度量。\n\n# 使用基于不纯度的变量重要性重新运行模型\nrf_impurity &lt;- ranger(\n  formula = Sale_Price ~ ., \n  data = ames_train, \n  num.trees = 2000,\n  mtry = 32,\n  min.node.size = 1,\n  sample.fraction = .80,\n  replace = FALSE,\n  importance = \"impurity\",\n  respect.unordered.factors = \"order\",\n  verbose = FALSE,\n  seed  = 123\n)\n\n# 使用基于排列的变量重要性重新运行模型\nrf_permutation &lt;- ranger(\n  formula = Sale_Price ~ ., \n  data = ames_train, \n  num.trees = 2000,\n  mtry = 32,\n  min.node.size = 1,\n  sample.fraction = .80,\n  replace = FALSE,\n  importance = \"permutation\",\n  respect.unordered.factors = \"order\",\n  verbose = FALSE,\n  seed  = 123\n)\n\n结果的变量重要性图（VIP）如图所示。通常，两种方法下的变量重要性顺序不同；通常会在图的顶部（和底部）看到相似的变量。因此，在本例中，可以有信心地说，有足够证据表明以下三个变量最具影响力：\n\nOverall_Qual\nGr_Liv_Area\nNeighborhood\n\n查看两个图中的接下来的约10个变量，还会看到一些共同的影响变量（例如，Garage_Cars、Exter_Qual、Bsmt_Qual 和 Year_Built）。\n\np1 &lt;- vip::vip(rf_impurity, num_features = 25, bar = FALSE)\np2 &lt;- vip::vip(rf_permutation, num_features = 25, bar = FALSE)\n\ngridExtra::grid.arrange(p1, p2, nrow = 1)\n\n\n\n\n\n\n\n\n基于不纯度（左）和排列（右）的前25个最重要的变量。\n随机森林提供了一种非常强大的开箱即用算法，通常具有出色的预测准确性。它们具有决策树（除了代理分割外）和装袋法的所有优点，但大大降低了不稳定性和树间相关性。由于增加了分割变量选择属性，随机森林比装袋法更快，因为每次树分割的特征搜索空间更小。然而，随着数据集的扩大，随机森林仍会面临计算速度慢的问题，但与装袋法类似，该算法基于独立步骤，大多数现代实现（例如，ranger、h2o）允许并行化来改善训练时间。"
  },
  {
    "objectID": "bigdata/machlearn/mlforest.html#工具与数据",
    "href": "bigdata/machlearn/mlforest.html#工具与数据",
    "title": "有监督学习：随机森林与梯度提升",
    "section": "",
    "text": "重点在于使用 ranger和 h2o 包实现随机森林。\n\n# 辅助包\nlibrary(dplyr)    # 用于数据处理\nlibrary(ggplot2)  # 用于出色的图形\n\n# 建模包\nlibrary(ranger)   # 随机森林的C++实现\nlibrary(h2o)      # 随机森林的Java实现\n\n\n----------------------------------------------------------------------\n\nYour next step is to start H2O:\n    &gt; h2o.init()\n\nFor H2O package documentation, ask for help:\n    &gt; ??h2o\n\nAfter starting H2O, you can use the Web UI at http://localhost:54321\nFor more information visit https://docs.h2o.ai\n\n----------------------------------------------------------------------\n\n\n\nAttaching package: 'h2o'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    day, hour, month, week, year\n\n\nThe following objects are masked from 'package:stats':\n\n    cor, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    &&, %*%, %in%, ||, apply, as.factor, as.numeric, colnames,\n    colnames&lt;-, ifelse, is.character, is.factor, is.numeric, log,\n    log10, log1p, log2, round, signif, trunc\n\n\n继续使用 ames_train 数据集来展示主要概念。\n\names &lt;- AmesHousing::make_ames()\n\nlibrary(rsample)\n# 分层抽样划分训练集和测试集数据\nset.seed(123)\nsplit  &lt;- initial_split(ames, prop = 0.7, strata = \"Sale_Price\")\names_train  &lt;- training(split)\names_test   &lt;- testing(split)"
  },
  {
    "objectID": "bigdata/machlearn/mlforest.html#扩展装袋法",
    "href": "bigdata/machlearn/mlforest.html#扩展装袋法",
    "title": "有监督学习：随机森林与梯度提升",
    "section": "",
    "text": "随机森林基于决策树和装袋法的基本原理构建。装袋法通过在训练数据的自举副本上构建多棵树，为树的构建过程引入随机成分。然后，装袋法通过聚合所有树的预测来降低整体过程的方差，并提高预测性能。但是，简单地装袋树会导致树相关性（tree correlation），限制了方差减少的效果。\n随机森林通过在树生长过程中注入更多随机性来帮助减少树相关性。具体来说，在装袋过程中生长决策树时，随机森林执行分割变量随机化（split-variable randomization），即每次进行分割时，仅从原始 \\(p\\) 个特征中随机选择 \\(m_{try}\\) 个特征子集进行搜索。通常的默认值是回归问题中 \\(m_{try} = \\frac{p}{3}\\)，分类问题中 \\(m_{try} = \\sqrt{p}\\)，可以看做是一个调优参数。\n回归或分类随机森林的基本算法可概括如下：\n\n给定训练数据集\n选择要构建的树数量（n_trees）\n对于 \\(i = 1\\) 到 n_trees，执行：\n生成原始数据的自举样本\n对自举数据生长回归/分类树\n对于每次分割：\n| 从所有 \\(p\\) 个变量中随机选择 \\(m_{try}\\) 个变量\n| 在 \\(m_{try}\\) 个变量中选择最佳变量/分割点\n| 将节点分割为两个子节点\n结束\n使用典型的树模型停止标准确定树何时完成（但不修剪）\n结束\n输出树的集成\n\n当 \\(m_{try} = p\\) 时，该算法等同于装袋决策树。\n由于算法随机选择自举样本进行训练并在每次分割时随机选择特征子集，生成了更多样化的树集，这往往比装袋树进一步减少树相关性，并显著提高预测能力。"
  },
  {
    "objectID": "bigdata/machlearn/mlforest.html#开箱即用out-of-the-box的性能",
    "href": "bigdata/machlearn/mlforest.html#开箱即用out-of-the-box的性能",
    "title": "有监督学习：随机森林与梯度提升",
    "section": "",
    "text": "随机森林之所以流行，是因为它们通常提供非常好的开箱即用性能。尽管有多个可调超参数，但是默认值往往就能产生良好的结果。此外，与流行的机器学习算法相比，随机森林在调优时的预测准确性变异性最小。\n例如，使用所有超参数设置为默认值的随机森林模型进行训练，获得的袋外（OOB）RMSE优于之前运行的任何模型（无需任何调优）。\n默认情况下，ranger 将 \\(m_{try}\\) 参数设置为 \\(\\text{floor}(\\sqrt{\\text{特征数量}})\\)；然而，对于回归问题，建议以 \\(\\text{floor}(\\frac{\\text{特征数量}}{3})\\) 开始。设置 respect.unordered.factors = \"order\"，指定如何处理无序因子变量，建议设置为“order”。\n\n# 特征数量\nn_features &lt;- length(setdiff(names(ames_train), \"Sale_Price\"))\n\n# 训练默认随机森林模型\names_rf1 &lt;- ranger(\n  Sale_Price ~ ., \n  data = ames_train,\n  mtry = floor(n_features / 3),\n  respect.unordered.factors = \"order\",\n  seed = 123\n)\n\n# 获取OOB RMSE\n(default_rmse &lt;- sqrt(ames_rf1$prediction.error))\n\n[1] 25423.06"
  },
  {
    "objectID": "bigdata/machlearn/mlforest.html#超参数",
    "href": "bigdata/machlearn/mlforest.html#超参数",
    "title": "有监督学习：随机森林与梯度提升",
    "section": "",
    "text": "尽管随机森林开箱即用表现良好，但在训练模型时还应考虑几个可调参数。主要考虑的超参数包括：\n\n森林中树的棵数\n每次分割考虑的特征数量：\\(m_{try}\\)\n每棵树的复杂性\n采样方案\n树构建期间使用的分割规则\n\n其中 (1) 和 (2) 对预测准确性影响最大，应始终进行调优。(3) 和 (4) 对预测准确性的影响较小，但仍值得探索。它们还能影响计算效率。(5) 对预测准确性的影响最小，主要用于提高计算效率。\n\n\n随机森林中的树的数量，虽然严格来说不是超参数，但树的数量需要足够大以稳定误差率。经验法则是从特征数量的10倍开始；随着其他超参数（如 \\(m_{try}\\) 和节点大小）的调整，可能需要更多或更少的树。更多树提供更稳健和稳定的误差估计和变量重要性度量；然而，计算时间随树数量线性增加。\n建议：从 \\(p \\times 10\\) 棵树开始，并根据需要调整。\n\n\n\n控制随机森林分割变量随机化特征的超参数通常称为 \\(m_{try}\\)，它有助于平衡树根部的相关性与合理的预测强度。对于回归问题，默认值通常为 \\(m_{try} = \\frac{p}{3}\\)，对于分类问题为 \\(m_{try} = \\sqrt{p}\\)。然而，当具有相关关系的预测变量比较少（例如，噪声预测变量）时，较高的 \\(m_{try}\\) 值往往表现更好，因为它更有可能选择信号最强的特征。当有许多预测变量具有相关性时，较低的 \\(m_{try}\\) 可能表现更好。\n建议：将恰好五等分区间[2, p]的4个分割点取值和为中心的默认值作为 \\(m_{try}\\) 值开始。\n\n\n\n随机森林基于单个决策树构建；因此，大多数随机森林模型有一个或多个超参数来控制单个树的深度和复杂性。通常包括节点大小、最大深度、最大终端节点数或允许额外分割的所需节点大小等超参数。节点大小是控制树复杂性的最常见超参数，一般分类问题设定为1、回归问题设定为5的默认值。然而，如果数据有许多噪声预测变量且较高的 \\(m_{try}\\) 值时表现较好，则应增加节点大小（即减少树深度和复杂性）能提高性能。此外，如果要考虑计算时间，增加节点大小通常可以显著减少运行时间，且对误差估计的影响较小。\n建议：调整节点大小时，从1-10之间的三个值开始，并根据对准确性和运行时间的影响进行调整。\n\n\n\n随机森林的默认采样方案是自举法，其中对100%的观测值进行有放回采样（即每个自举副本与原始训练数据大小相同）；可以通过调整样本大小以及是否有放回采样来对模型效果产生影响。样本大小参数决定为每棵树的训练抽取多少观测值，减少样本大小会产生更多样化的树，从而降低树间相关性，这会对预测准确性产生积极影响。如果数据集中有几个主导特征，减少样本大小也有助于最小化树间相关性。\n此外，当有许多具有不同级别数的分类特征（类别变量的各类别的频次差别比较大）时，有放回采样可能导致变量分割选择的偏差。因此，如果有不平衡的类别，无放回采样可产生更少偏差。\n建议：评估25%-100%的3-4个样本大小值，如果有不平衡的分类特征，尝试无放回采样。\n\n\n\n随机森林树构建期间的默认分割规则包括从（随机选择的 \\(m_{try}\\)）个候选变量的所有分割中选择最小化基尼不纯度（分类问题）或SSE（回归问题）的分割。然而，这些默认分割规则偏向于选择具有更多可能分割的特征（例如，连续变量或具有多个类别的分类变量），而非分割较少的变量（极端情况是只有一种可能分割的二值变量）。条件推理树（conditional inference trees）采用替代分割机制，可以减少变量选择偏差，但是训练时间更长。\n为了提高计算效率，可以随机化分割规则，仅考虑变量的随机子集的可能分割值。如果仅随机选择单一分割值，则称为极随机树（extremely randomized trees）。\n从运行时间来看，极随机树最快，其次是经典随机森林，而条件推理森林的运行时间最长。\n建议：如果需要显著改善计算时间，可以尝试完全随机化树；然而，要确保与传统分割规则的预测准确性进行比较，因为这种方法通常对损失函数有负面影响。"
  },
  {
    "objectID": "bigdata/machlearn/mlforest.html#调优策略",
    "href": "bigdata/machlearn/mlforest.html#调优策略",
    "title": "有监督学习：随机森林与梯度提升",
    "section": "",
    "text": "随着更复杂的算法和更多的超参数，需要考虑调优策略。之前采用的完全笛卡尔网格搜索，即评估所有感兴趣的超参数组合，往往需要较长的计算时间。下面的代码块搜索120种超参数组合，此网格搜索大约需要3分钟。\n\n# 创建超参数网格\nhyper_grid &lt;- expand.grid(\n  mtry = floor(n_features * c(.05, .15, .25, .333, .4)),\n  min.node.size = c(1, 3, 5, 10), \n  replace = c(TRUE, FALSE),                               \n  sample.fraction = c(.5, .63, .8),                       \n  rmse = NA                                               \n)\n\n# 执行完全笛卡尔网格搜索\nfor(i in seq_len(nrow(hyper_grid))) {\n  # 为第i个超参数组合拟合模型\n  fit &lt;- ranger(\n    formula         = Sale_Price ~ ., \n    data            = ames_train, \n    num.trees       = n_features * 10,\n    mtry            = hyper_grid$mtry[i],\n    min.node.size   = hyper_grid$min.node.size[i],\n    replace         = hyper_grid$replace[i],\n    sample.fraction = hyper_grid$sample.fraction[i],\n    verbose         = FALSE,\n    seed            = 123,\n    respect.unordered.factors = 'order',\n  )\n  # 导出OOB误差\n  hyper_grid$rmse[i] &lt;- sqrt(fit$prediction.error)\n}\n\n# 评估前10个模型\nhyper_grid %&gt;%\n  arrange(rmse) %&gt;%\n  mutate(perc_gain = (default_rmse - rmse) / default_rmse * 100) %&gt;%\n  head(10)\n\n   mtry min.node.size replace sample.fraction     rmse perc_gain\n1    26             1   FALSE             0.8 24910.63  2.015610\n2    12             1   FALSE             0.8 24984.93  1.723339\n3    12             5   FALSE             0.8 25034.17  1.529646\n4    20             1   FALSE             0.8 25040.19  1.505984\n5    32             1   FALSE             0.8 25049.52  1.469293\n6    20             3   FALSE             0.8 25056.39  1.442254\n7    26             3   FALSE             0.8 25074.46  1.371189\n8    26             5   FALSE             0.8 25113.53  1.217501\n9    12             3   FALSE             0.8 25113.95  1.215830\n10   32             3   FALSE             0.8 25160.93  1.031053\n\n\n从结果来看，前10个模型的RMSE接近或低于25000（比基线模型提高了2.5%-3.5%）。在这些结果中，默认 \\(m_{try}\\) 值 \\(\\lfloor \\frac{\\text{特征数量}}{3} \\rfloor = 26\\) 就是比较理想的参数，较小的节点大小（更深的树）表现最佳。最突出的是，采样率低于100%且无放回采样始终表现最佳。低于100%的采样率增加了程序的随机性，有助于进一步降低树的相关性。无放回采样能提高性能，是因为该数据有许多多类别的不平衡的分类特征。\n然而，随着超参数和搜索值的增加以及数据集的扩大，完全笛卡尔搜索可能变得耗时且计算成本高。可以采用h2o 包提供的随机网格搜索，能够从一个随机组合跳转到另一个，并提供早期停止规则，在满足特定条件（例如，训练了特定数量的模型、经过特定运行时间或准确性的提升非常小）时停止网格搜索。尽管随机离散搜索路径可能无法找到最优模型，但通常能找到一个已经足够好的模型。\n要使用 h2o 拟合随机森林模型，首先需要启动 h2o 会话。\n\nh2o.no_progress()\nh2o.init(max_mem_size = \"5g\")\n\n Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         8 days 5 hours \n    H2O cluster timezone:       Asia/Shanghai \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.44.0.3 \n    H2O cluster version age:    1 year, 11 months and 17 days \n    H2O cluster name:           H2O_started_from_R_liangdan_ats696 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   0.84 GB \n    H2O cluster total cores:    10 \n    H2O cluster allowed cores:  10 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.4.2 (2024-10-31) \n\n\nWarning in h2o.clusterInfo(): \nYour H2O cluster version is (1 year, 11 months and 17 days) old. There may be a newer version available.\nPlease download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n\n\n接下来，我们需要将训练和测试数据集转换为 h2o 可处理的对象。\n\n# 将训练数据转换为h2o对象\ntrain_h2o &lt;- as.h2o(ames_train)\n\n# 设置响应列为Sale_Price\nresponse &lt;- \"Sale_Price\"\n\n# 设置预测变量名称\npredictors &lt;- setdiff(colnames(ames_train), response)\n\n以下代码使用 h2o 拟合默认随机森林模型，展示基线结果（OOB RMSE = 25045.8）与之前拟合的 ranger 基线模型非常相似。\n\nh2o_rf1 &lt;- h2o.randomForest(\n    x = predictors, \n    y = response,\n    training_frame = train_h2o, \n    ntrees = n_features * 10,\n    seed = 123\n)\n\nh2o_rf1\n\nModel Details:\n==============\n\nH2ORegressionModel: drf\nModel ID:  DRF_model_R_1764404962582_6 \nModel Summary: \n  number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n1             800                      800            12318752        19\n  max_depth mean_depth min_leaves max_leaves mean_leaves\n1        20   19.99875       1128       1286  1220.41130\n\n\nH2ORegressionMetrics: drf\n** Reported on training data. **\n** Metrics reported on Out-Of-Bag training samples **\n\nMSE:  627291937\nRMSE:  25045.8\nMAE:  15236.82\nRMSLE:  0.1415736\nMean Residual Deviance :  627291937\n\n\n要在 h2o 中执行网格搜索，需要将超参数网格设置为列表。例如，以下代码搜索比之前更大的网格空间，共240个超参数组合。然后，创建随机网格搜索策略，如果最后10个模型与之前的最佳模型相比，没有哪个模型的MSE改进能够达到0.1%，则停止。如果还能继续搜索改进，在300秒（5分钟）后也中断网格搜索。\n\n# 超参数网格\nhyper_grid &lt;- list(\n  mtries = floor(n_features * c(.05, .15, .25, .333, .4)),\n  min_rows = c(1, 3, 5, 10),\n  max_depth = c(10, 20, 30),\n  sample_rate = c(.55, .632, .70, .80)\n)\n\n# 随机网格搜索策略\nsearch_criteria &lt;- list(\n  strategy = \"RandomDiscrete\",\n  stopping_metric = \"mse\",\n  stopping_tolerance = 0.001,   # 如果改进&lt;0.1%则停止\n  stopping_rounds = 10,         # 在最后10个模型上\n  max_runtime_secs = 60*2      # 或在2分钟后停止搜索\n)\n\n然后，可以使用 h2o.grid() 执行网格搜索。以下代码启用早期停止执行网格搜索。在 h2o.grid() 中指定的早期停止方式是在最后10棵树中，如果单个随机森林模型的整体OOB误差改进小于0.05%时则停止生长单个随机森林模型。这可能可以大大减少单个随机森林模型的构建复杂度。此网格搜索需要2分钟。\n\nif (\"rf_random_grid\" %in% h2o.ls()$key) {\n  h2o.rm(\"rf_random_grid\")\n}   # 剔除由于反复运行可能会出现的id重复的对象\n\n# 执行网格搜索\nrandom_grid &lt;- h2o.grid(\n  algorithm = \"randomForest\",\n  grid_id = \"rf_random_grid\",\n  x = predictors, \n  y = response, \n  training_frame = train_h2o,\n  hyper_params = hyper_grid,\n  ntrees = n_features * 10,\n  seed = 123,\n  stopping_metric = \"RMSE\",   \n  stopping_rounds = 10,           # 如果最后10棵树没有改进\n  stopping_tolerance = 0.005,     # RMSE没有0.5%的改进\n  search_criteria = search_criteria\n)\n\n该网格搜索在时间停止前评估了137个模型。最佳模型（max_depth = 10, min_rows = 1, mtries = 32, sample_rate = 0.7）实现了OOB RMSE为25346.50。因此，尽管随机搜索评估的模型数量仅为完全网格搜索的约53%，更有效的随机搜索在指定时间约束内找到了接近最优的模型。\n\n# 收集结果并按我们的模型性能指标排序\nrandom_grid_perf &lt;- h2o.getGrid(\n  grid_id = \"rf_random_grid\", \n  sort_by = \"rmse\", \n  decreasing = FALSE\n)\n\nrandom_grid_perf\n\nH2O Grid Details\n================\n\nGrid ID: rf_random_grid \nUsed hyper parameters: \n  -  max_depth \n  -  min_rows \n  -  mtries \n  -  sample_rate \nNumber of models: 122 \nNumber of failed models: 0 \n\nHyper-Parameter Search Summary: ordered by increasing rmse\n  max_depth min_rows   mtries sample_rate               model_ids        rmse\n1  20.00000  1.00000 32.00000     0.70000 rf_random_grid_model_73 25032.02025\n2  30.00000  1.00000 32.00000     0.80000 rf_random_grid_model_50 25406.24586\n3  20.00000  1.00000 26.00000     0.63200 rf_random_grid_model_74 25563.08628\n4  30.00000  1.00000 20.00000     0.70000 rf_random_grid_model_55 25582.26141\n5  20.00000  1.00000 20.00000     0.70000 rf_random_grid_model_37 25583.02405\n\n---\n    max_depth min_rows  mtries sample_rate                model_ids        rmse\n117  20.00000 10.00000 4.00000     0.70000  rf_random_grid_model_65 32996.30102\n118  30.00000 10.00000 4.00000     0.70000  rf_random_grid_model_95 32996.30102\n119  20.00000 10.00000 4.00000     0.63200  rf_random_grid_model_78 33051.43114\n120  10.00000 10.00000 4.00000     0.70000  rf_random_grid_model_10 33059.58047\n121  10.00000 10.00000 4.00000     0.63200 rf_random_grid_model_112 33112.30804\n122  10.00000 10.00000 4.00000     0.55000  rf_random_grid_model_58 33576.85787"
  },
  {
    "objectID": "bigdata/machlearn/mlforest.html#特征解释",
    "href": "bigdata/machlearn/mlforest.html#特征解释",
    "title": "有监督学习：随机森林与梯度提升",
    "section": "",
    "text": "随机森林的特征重要性和特征效应计算与决策树和装袋法相似。然而，除了基于不纯度的特征重要性度量（其中特征重要性基于所有树中给定特征的损失函数平均总减少量）外，随机森林通常还包括基于排列的特征重要性度量。在基于排列的方法中，对于每棵树，将OOB样本传递到树中并记录预测准确性。然后，逐一随机打乱每个变量的值并再次计算准确性。由于随机打乱特征值导致的准确性下降在所有树上对每个预测变量取平均值。准确性平均下降最大的变量被认为是最重要的。\n例如，可以通过设置 ranger 的 importance 参数来计算两种特征重要性度量。\n\n# 使用基于不纯度的变量重要性重新运行模型\nrf_impurity &lt;- ranger(\n  formula = Sale_Price ~ ., \n  data = ames_train, \n  num.trees = 2000,\n  mtry = 32,\n  min.node.size = 1,\n  sample.fraction = .80,\n  replace = FALSE,\n  importance = \"impurity\",\n  respect.unordered.factors = \"order\",\n  verbose = FALSE,\n  seed  = 123\n)\n\n# 使用基于排列的变量重要性重新运行模型\nrf_permutation &lt;- ranger(\n  formula = Sale_Price ~ ., \n  data = ames_train, \n  num.trees = 2000,\n  mtry = 32,\n  min.node.size = 1,\n  sample.fraction = .80,\n  replace = FALSE,\n  importance = \"permutation\",\n  respect.unordered.factors = \"order\",\n  verbose = FALSE,\n  seed  = 123\n)\n\n结果的变量重要性图（VIP）如图所示。通常，两种方法下的变量重要性顺序不同；通常会在图的顶部（和底部）看到相似的变量。因此，在本例中，可以有信心地说，有足够证据表明以下三个变量最具影响力：\n\nOverall_Qual\nGr_Liv_Area\nNeighborhood\n\n查看两个图中的接下来的约10个变量，还会看到一些共同的影响变量（例如，Garage_Cars、Exter_Qual、Bsmt_Qual 和 Year_Built）。\n\np1 &lt;- vip::vip(rf_impurity, num_features = 25, bar = FALSE)\np2 &lt;- vip::vip(rf_permutation, num_features = 25, bar = FALSE)\n\ngridExtra::grid.arrange(p1, p2, nrow = 1)\n\n\n\n\n\n\n\n\n基于不纯度（左）和排列（右）的前25个最重要的变量。\n随机森林提供了一种非常强大的开箱即用算法，通常具有出色的预测准确性。它们具有决策树（除了代理分割外）和装袋法的所有优点，但大大降低了不稳定性和树间相关性。由于增加了分割变量选择属性，随机森林比装袋法更快，因为每次树分割的特征搜索空间更小。然而，随着数据集的扩大，随机森林仍会面临计算速度慢的问题，但与装袋法类似，该算法基于独立步骤，大多数现代实现（例如，ranger、h2o）允许并行化来改善训练时间。"
  },
  {
    "objectID": "bigdata/machlearn/mlforest.html#软件包与数据",
    "href": "bigdata/machlearn/mlforest.html#软件包与数据",
    "title": "有监督学习：随机森林与梯度提升",
    "section": "2.1 软件包与数据",
    "text": "2.1 软件包与数据\n使用以下 R 包。其中部分作为辅助工具使用，但重点在展示如何利用 gbm（B. Greenwell et al. 2018）、xgboost（Chen et al. 2018）、h2o 包实现 GBM：\n\n# Helper packages\nlibrary(dplyr)    # 数据处理工具\n\n# Modeling packages\nlibrary(rpart)\nlibrary(gbm)      # 原始版本的GBM，包括regular & stochastic GBM\n\nLoaded gbm 2.2.2\n\n\nThis version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\n\nlibrary(h2o)      # Java 实现，包含多种 GBM 变体\nlibrary(xgboost)  # Extreme Gradient Boosting 实现\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n\n继续使用 ames_train 数据集，并复用前面的 h2o 环境设置。\n\nh2o.init(max_mem_size = \"10g\")\n\n Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         8 days 5 hours \n    H2O cluster timezone:       Asia/Shanghai \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.44.0.3 \n    H2O cluster version age:    1 year, 11 months and 17 days \n    H2O cluster name:           H2O_started_from_R_liangdan_ats696 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   0.83 GB \n    H2O cluster total cores:    10 \n    H2O cluster allowed cores:  10 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.4.2 (2024-10-31) \n\n\nWarning in h2o.clusterInfo(): \nYour H2O cluster version is (1 year, 11 months and 17 days) old. There may be a newer version available.\nPlease download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n\ntrain_h2o &lt;- as.h2o(ames_train)\nresponse &lt;- \"Sale_Price\"\npredictors &lt;- setdiff(colnames(ames_train), response)"
  },
  {
    "objectID": "bigdata/machlearn/mlforest.html#boosting-的工作原理",
    "href": "bigdata/machlearn/mlforest.html#boosting-的工作原理",
    "title": "有监督学习：随机森林与梯度提升",
    "section": "2.2 Boosting 的工作原理",
    "text": "2.2 Boosting 的工作原理\n许多监督机器学习算法基于单一模型，例如：\n\n普通线性回归\n惩罚回归（penalized regression）\n单棵决策树\n支持向量机（SVM）\n\n而装袋法与随机森林则通过将多个模型组合为一个集成模型来提升表现。集成模型通过平均或投票方式组合单个模型的预测。 由于平均可以降低方差，装袋法更适用于高方差、低偏差的模型（例如深树）。\nBoosting 则相反，它通常更适用于 高偏差、低方差的弱模型。\nBoosting 是一种通用的集成方法，可以将弱模型组合成强模型。尽管理论上 Boosting 可以应用于任意类型的弱学习器，但现实中几乎总是使用 决策树。\n\n2.2.1 顺序集成（Sequential Ensemble）方法\nBoosting 的核心思想：\n\n按序构建多棵弱树，每一棵树用于修正上一棵树的错误。\n\n如图所示，每一棵新树都会重点学习上一棵树预测误差最大的样本。\n\n\n\n\n\nflowchart LR\n    A[数据集] --&gt;|训练| B[模型 1（弱学习器）]\n    B --&gt;|测试| C[误差 1]\n    C --&gt;|基于误差训练| D[模型 2（弱学习器）]\n    D --&gt;|测试| E[误差 2]\n    E --&gt;|基于误差训练| F[模型 3（弱学习器）]\n    F --&gt; G[…… 多轮迭代 ……]\n    G --&gt; H[综合所有子模型]\n    H --&gt; I[最终预测结果]\n\n\n\n\n\n\n\nBoosting 的三大关键：\n① 基学习器（Base learners）\nBoosting 是一个框架，可以使用任何弱学习器，但在实践中，几乎总是采用 浅层决策树。\n② 训练弱模型\n弱学习器的错误率仅略优于随机猜测。Boosting 通过让每一棵树重点学习上一棵树的残差来逐步减少偏差。\n浅树（1–6 次分裂）是典型弱学习器。\n③ 基于残差的顺序训练\nBoosting 回归树过程如下：\n1️⃣ 拟合第一棵树   F₁(x) = y\n2️⃣ 拟合第二棵树以学习第一棵树的残差   h₁(x) = y – F₁(x)   F₂(x) = F₁(x) + h₁(x)\n3️⃣ 拟合第三棵树学习第二棵树的残差   h₂(x) = y – F₂(x)   F₃(x) = F₂(x) + h₂(x)\n……\n最终模型是若干弱学习器的加性组合：\n[ f(x)=_{b=1}^{B} f_b(x)]\n下图展示了利用“决策桩”（单分裂树）逐步逼近复杂函数（正弦波）的例子。\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n2.2.2 梯度下降（Gradient Descent）\nBoosting 目标是最小化损失函数，例如：\n\n回归：SSE（残差平方和）、MSE、RMSE\n分类：log loss、deviance\nMAE（平均绝对值误差）等更稳健的损失值\n\n对于 SSE，其梯度就是残差，因此“拟合残差”就是“沿梯度方向下降”，即当损失函数是SSE时，它的梯度正好等于残差（真值 − 预测值）。因此，让下一棵树“拟合残差”，就是让模型往让损失下降最快的方向走，也就是“沿梯度下降”。\n这也是“Gradient Boosting Machine”名称的来源。\n梯度下降通过反复沿损失函数下降最快的方向（此处为切线方向，即一阶导数）调整参数，直到到达最小值。\n\n\nWarning in geom_segment(aes(x = theta0, xend = theta0, y = 0, yend = loss(theta0)), : All aesthetics have length 1, but the data has 201 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n学习率过小 → 收敛慢 学习率过大 → 会跳过最优点\n随机梯度下降（Stochastic Gradient Descent, SGD）\n并不是所有的损失函数都是凸的（即“碗状”）。损失函数可能存在局部最小值、平台区（plateaus），以及其他不规则形状，这些都会使得找到全局最小值变得困难。随机梯度下降（Stochastic Gradient Descent, SGD） 可以缓解这个问题。其做法是在每一轮中随机抽取一部分训练样本（通常是不放回抽样），并基于该子样本训练下一棵树。这不仅能加快算法速度，而且由于随机抽样引入的随机性，下降损失函数梯度的过程也会带上一些“跳跃性”。虽然这种随机性使得算法无法保证找到绝对的全局最优解，但它反而可能帮助模型跳出局部最小值或平台区，从而更接近全局最优解。"
  },
  {
    "objectID": "bigdata/machlearn/mlforest.html#基本-gbmbasic-gbm",
    "href": "bigdata/machlearn/mlforest.html#基本-gbmbasic-gbm",
    "title": "有监督学习：随机森林与梯度提升",
    "section": "2.3 基本 GBM（Basic GBM）",
    "text": "2.3 基本 GBM（Basic GBM）\n上世纪 90 年代出现多种 Boosting 算法，其中最成功的是 AdaBoost（Freund & Schapire 1999）。2000 年 Friedman 将其与统计学概念（损失函数、加性模型等）结合，推广到回归问题，从而发展出今天常见的 GBM 框架。\n\n2.3.1 GBM 的超参数（Hyperparameters）\nGBM 的超参数分为两类：\n一、Boosting 超参数\n1. 树的数量（n.trees）\n\nGBM 会“追赶”残差，因此树太多很容易过拟合\n常需要几千棵树\n必须通过 CV 确定最佳树数\n\n2. 学习率（learning rate / shrinkage）\n范围：0–1，一般 0.001–0.3\n小学习率：\n\n更好泛化（更稳健）\n防止过拟合\n需要更多树（训练更慢）\n\n二、树结构超参数\n1. 树深度（interaction.depth）\n\n典型范围：3–8\ndepth=1 → 决策桩（简单但需要更多树）\n深树可捕捉交互，但风险是过拟合\n\n2. 叶节点最小样本（n.minobsinnode）\n典型范围：5–15 小值 → 更灵活 大值 → 防止过拟合\n\n\n2.3.2 使用 gbm 包实现 GBM\nR 中最经典的 GBM 实现是 gbm 包。\ngbm::gbm() 使用 formula 接口； gbm::gbm.fit() 使用 X/Y 接口，更高效。\n默认学习率 0.001 通常过小，需要大量树，因此常手动设置：\n\n学习率 = 0.1\n树数 = 5000\n深度 = 3\n10-fold 交叉验证\n\n示例模型如下（训练约 2 分钟）：\n\nset.seed(123)\names_gbm1 &lt;- gbm(\n  formula = Sale_Price ~ .,\n  data = ames_train,\n  distribution = \"gaussian\", # SSE 作为损失函数\n  n.trees = 5000,  # 树数\n  shrinkage = 0.1,  # 学习率\n  interaction.depth = 3,  # 树的深度 \n  n.minobsinnode = 10,    # 叶节点最小样本数\n  cv.folds = 10\n)\n\n利用 CV 得到最佳树数：\n\nbest &lt;- which.min(ames_gbm1$cv.error)\nsqrt(ames_gbm1$cv.error[best])\n\n[1] 22402.07\n\n\n显示了随着树数增加，训练误差（黑色）与 CV 验证误差（绿色，一般更高）的变化。\n\ngbm.perf(ames_gbm1, method = \"cv\")\n\n\n\n\n\n\n\n\n[1] 1119\n\n\n\n\n2.3.3 一般调参策略（General tuning strategy）\nGBM 的表现对超参数高度敏感，其调参比随机森林更困难。典型策略：\n\n首先选择较大的学习率（如 0.1）\n确定此学习率下的最佳树数，将此树数作为后续训练的大致树数范围\n在固定树结构的情况下，调学习率，评估性能与训练时间\n调树结构超参数（深度、最小样本）\n最终降低学习率并增加树数\n使用更严格的 CV（如多次重复 CV）稳定评估结果，如果之前已经是CV，这步可以省略\n\n下面在前面0.1的学习率得到大致树数在1000多的基准下，进一步搜索不同学习率（约3分钟），然后评估性能与训练时间。\n\n# 创建学习率的网络搜索\nhyper_grid &lt;- expand.grid(\n  learning_rate = c(0.3, 0.1, 0.05, 0.01, 0.005),\n  RMSE = NA,\n  trees = NA,\n  time = NA\n)\n\n# 执行搜索\nfor(i in seq_len(nrow(hyper_grid))) {\n\n  # 拟合 gbm\n  set.seed(123)  \n  train_time &lt;- system.time({\n    m &lt;- gbm(\n      formula = Sale_Price ~ .,\n      data = ames_train,\n      distribution = \"gaussian\",\n      n.trees = 5000, # 由基准确定的树数范围\n      shrinkage = hyper_grid$learning_rate[i], \n      interaction.depth = 3,  # 第一步已经确定的树参数\n      n.minobsinnode = 10,\n      cv.folds = 10 \n   )\n  })\n  \n  # 添加SSE，树数和训练时间\n  hyper_grid$RMSE[i]  &lt;- sqrt(min(m$cv.error))\n  hyper_grid$trees[i] &lt;- which.min(m$cv.error)\n  hyper_grid$time[i]  &lt;- train_time[[\"elapsed\"]]\n\n}\n\n# 按最优顺序排结果\narrange(hyper_grid, RMSE)\n\n  learning_rate     RMSE trees   time\n1         0.050 21807.96  1565 39.265\n2         0.010 22102.34  4986 39.761\n3         0.100 22402.07  1119 39.415\n4         0.005 23054.68  4995 38.580\n5         0.300 24411.95   269 39.630\n\n\n那么最佳学习率：0.05\n接着第4步来调树结构（约10分钟） 虽然最佳学习率为0.05，但是以其为基准选择一个较小的学习率0.01，可以让树结构参数精调更稳定，较小的学习率更容易反映出树的结构差异，不会被过大的调整幅度掩盖，同时能够看到树细微变化，避免过拟合。\n\n# 创建学习率的网络搜索\nhyper_grid &lt;- expand.grid(\n  n.trees = 6000,\n  shrinkage = 0.01,  \n  interaction.depth = c(3, 5, 7),\n  n.minobsinnode = c(5, 10, 15)\n)\n\n# 模型拟合函数\nmodel_fit &lt;- function(n.trees, shrinkage, interaction.depth, n.minobsinnode) {\n  set.seed(123)\n  m &lt;- gbm(\n    formula = Sale_Price ~ .,\n    data = ames_train,\n    distribution = \"gaussian\",\n    n.trees = n.trees,\n    shrinkage = shrinkage,\n    interaction.depth = interaction.depth,\n    n.minobsinnode = n.minobsinnode,\n    cv.folds = 10\n  )\n  # 计算 RMSE\n  sqrt(min(m$cv.error))\n}\n\n# 执行搜索\nhyper_grid$rmse &lt;- purrr::pmap_dbl(\n  hyper_grid,\n  ~ model_fit(\n    n.trees = ..1,\n    shrinkage = ..2,\n    interaction.depth = ..3,\n    n.minobsinnode = ..4\n    )\n)\n\n# 排序结果\narrange(hyper_grid, rmse)\n\n  n.trees shrinkage interaction.depth n.minobsinnode     rmse\n1    6000      0.01                 5              5 21505.73\n2    6000      0.01                 7              5 21525.52\n3    6000      0.01                 5             10 21667.50\n4    6000      0.01                 7             10 21706.33\n5    6000      0.01                 3              5 21962.44\n6    6000      0.01                 3             10 21983.03\n7    6000      0.01                 5             15 21999.32\n8    6000      0.01                 7             15 22189.80\n9    6000      0.01                 3             15 22204.50\n\n\n进一步降低学习率与增加树数没有带来明显的增益（从21807.96仅下降到21505.73，但是计算时间上升了3倍）。\n\n\n2.4 随机梯度提升机（Stochastic GBMs）\nBreiman 在开发 bagging 和随机森林算法时（Breiman 1996a; Breiman 2001）的一个重要洞察是：在训练数据集中随机抽取子样本来训练算法，可以进一步减少树之间的相关性，从而提高预测准确性。Friedman（2002）使用了相同的逻辑并相应地更新了提升算法。这种过程被称为随机梯度提升，它有助于减少陷入损失函数局部最小值、平台期和其他不规则地形的可能性，从而找到接近全局最优解。\n\n\n2.4.1 随机超参数\n随机梯度提升有几种变体可以使用，所有这些变体都有额外的超参数：\n\n在创建每棵树之前对行（观测）进行子采样（在 gbm、h2o 和 xgboost 中可用）\n在创建每棵树之前对列（特征）进行子采样（h2o 和 xgboost）\n在考虑每棵树中的每个分割之前对列进行子采样（h2o 和 xgboost）\n\n一般来说，对行的激进子采样（如仅选择50%或更少的训练数据）已被证明是有益的，典型值范围在 0.5–0.8 之间。列子采样对性能的影响很大程度上取决于数据的性质、是否存在强多重共线性或是否有大量噪声特征。类似于随机森林中的 mtry 参数，如果相关预测变量较少（噪声数据较多），较高的列子采样比值往往表现更好，因为它更有可能选择具有最强信号的特征。当相关预测变量较多时，较低的列子采样比值往往表现良好。\n在添加随机过程时，可以将其包含在上述一般调参策略的第4步，或者在找到最优基本模型之后（第6步）。根据经验，没有看到随机超参数与其他提升和树特定超参数之间存在强烈的交互作用。\n\n\n2.4.2 实现\n以下使用 h2o 实现随机 GBM。使用前一节找到的最优超参数，并在此基础上评估在构建每棵树之前对行和列进行子采样的各种值，以及在每个分割之前对列进行子采样。为了加速训练，为单个 GBM 建模过程使用早停，并添加随机搜索标准。\n这个网格搜索只设定运行了10分钟，评估了可能27个模型中的13个，完整跑完可能需要1个小时。\n\n# 精细化的超参数网格\nhyper_grid &lt;- list(\n  sample_rate = c(0.5, 0.75, 1),              # 行子采样\n  col_sample_rate = c(0.5, 0.75, 1),          # 每个分割的列子采样\n  col_sample_rate_per_tree = c(0.5, 0.75, 1)  # 每棵树的列子采样\n)\n\n# 随机网格搜索策略\nsearch_criteria &lt;- list(\n  strategy = \"RandomDiscrete\",\n  stopping_metric = \"mse\",\n  stopping_tolerance = 0.001,   \n  stopping_rounds = 10,         \n  max_runtime_secs = 60*10      \n)\n\nif (\"gbm_grid\" %in% h2o.ls()$key) {\n  h2o.rm(\"gbm_grid\")\n}   # 剔除由于反复运行可能会出现的id重复的对象\n\n# 执行网格搜索 \ngrid &lt;- h2o.grid(\n  algorithm = \"gbm\",\n  grid_id = \"gbm_grid\",\n  x = predictors, \n  y = response,\n  training_frame = train_h2o,\n  hyper_params = hyper_grid,\n  ntrees = 6000,\n  learn_rate = 0.01,\n  max_depth = 7,\n  min_rows = 5,\n  nfolds = 10,\n  stopping_rounds = 10,\n  stopping_tolerance = 0,\n  search_criteria = search_criteria,\n  seed = 123\n)\n\n# 收集结果并按我们选择的模型性能指标排序\ngrid_perf &lt;- h2o.getGrid(\n  grid_id = \"gbm_grid\", \n  sort_by = \"mse\", \n  decreasing = FALSE\n)\n\ngrid_perf\n\nH2O Grid Details\n================\n\nGrid ID: gbm_grid \nUsed hyper parameters: \n  -  col_sample_rate \n  -  col_sample_rate_per_tree \n  -  sample_rate \nNumber of models: 12 \nNumber of failed models: 0 \n\nHyper-Parameter Search Summary: ordered by increasing mse\n   col_sample_rate col_sample_rate_per_tree sample_rate         model_ids\n1          0.50000                  1.00000     0.50000  gbm_grid_model_2\n2          0.75000                  0.50000     0.50000  gbm_grid_model_5\n3          0.50000                  1.00000     0.75000  gbm_grid_model_3\n4          0.75000                  0.75000     0.75000  gbm_grid_model_6\n5          0.75000                  1.00000     0.75000  gbm_grid_model_4\n6          0.50000                  1.00000     1.00000  gbm_grid_model_1\n7          1.00000                  0.75000     1.00000  gbm_grid_model_7\n8          0.75000                  0.75000     1.00000  gbm_grid_model_8\n9          0.50000                  0.50000     0.75000  gbm_grid_model_9\n10         1.00000                  1.00000     1.00000 gbm_grid_model_10\n11         0.75000                  1.00000     0.50000 gbm_grid_model_11\n12         1.00000                  0.50000     1.00000 gbm_grid_model_12\n                mse\n1   452431100.85359\n2   458280855.54982\n3   471111963.66550\n4   474654797.16037\n5   483500693.90715\n6   515864743.58130\n7   528810128.24734\n8   601111769.93020\n9  1291500256.31247\n10 4517273604.34495\n11 5373644981.63226\n12 6345709992.72047\n\n\n网格搜索展示了几个重要的结果：\n\n为每棵树随机抽样行和在每个分割之前随机抽样特征似乎对性能有积极影响\n在创建每棵树之前抽样特征是否有影响尚不明确\n最佳采样值非常低（0.5）；进一步的网格搜索可能有益于评估更低的值\n\n下面的代码片段提取了表现最好的模型。在这种特定情况下，没有看到10折交叉验证RMSE比最佳非随机GBM模型有额外改进。\n\n# 获取通过交叉验证误差选择的最佳模型的 model_id\nbest_model_id &lt;- grid_perf@model_ids[[1]]\nbest_model &lt;- h2o.getModel(best_model_id)\n\n# 现在获取最佳模型的性能指标\nh2o.performance(model = best_model, xval = TRUE)\n\n关键要点总结\n\n随机子采样的优势：\n\n减少树之间的相关性\n帮助逃离损失函数的局部最小值\n提高预测准确性\n\n三种随机策略： | 策略 | 参数名称 | 适用范围 | 典型值 | |——|———-|———-|——–| | 行子采样 | sample_rate | 每棵树 | 0.5-0.8 | | 每棵树列子采样 | col_sample_rate_per_tree | 每棵树 | 0.5-1.0 | | 每个分割列子采样 | col_sample_rate | 每个分割 | 0.5-1.0 |\n调参建议：\n\n行子采样：0.5-0.8 通常最优\n列子采样：\n\n噪声特征多 → 较高值（0.8-1.0）\n相关特征多 → 较低值（0.5-0.7）\n\n\n实现优势：\n\n使用 h2o.grid() 的随机搜索策略\n结合早停机制加速训练\n自动评估多个组合\n\n实际结果解读：\n\n最佳组合：sample_rate = 0.5, col_sample_rate = 0.5, col_sample_rate_per_tree = 0.5\nRMSE = 21270.43（与非随机GBM相当）\n表明在这个特定数据集上，随机性提升有限\n\n\n这个部分展示了如何通过引入随机性来改进梯度提升机的性能，同时保持了调参过程的系统性。通过网格搜索和早停机制，可以高效地找到最优的随机参数组合。"
  },
  {
    "objectID": "bigdata/machlearn/mlforest.html#xgboost",
    "href": "bigdata/machlearn/mlforest.html#xgboost",
    "title": "有监督学习：随机森林与梯度提升",
    "section": "2.5 XGBoost",
    "text": "2.5 XGBoost\n极限梯度提升（XGBoost） 是一个优化的分布式梯度提升库，设计目标是高效、灵活且跨多种语言可移植（Chen 和 Guestrin 2016）。虽然 XGBoost提供了前面展示的相同的提升和基于树的超参数选项，但它相较于传统提升方法还具有以下几个优势：\n\n正则化：XGBoost 提供了额外的正则化超参数，为防止过拟合提供了额外的保护。\n早停（Early Stopping）：与 h2o 类似，XGBoost 实现了早停机制，可以在添加更多树不再带来改进时停止模型评估。\n并行处理：由于梯度提升本质上是顺序的，很难并行化。XGBoost 实现了支持 GPU 和 Spark 兼容性的程序，允许使用强大的分布式处理引擎来拟合梯度提升模型。\n损失函数：XGBoost 允许用户定义和优化梯度提升模型，使用自定义的目标函数和评估标准。\n继续现有模型：用户可以训练一个 XGBoost 模型，保存结果，然后稍后返回该模型并继续构建结果。允许在不从头开始的情况下继续训练模型。\n不同的基学习器：大多数 GBM 实现都基于决策树，但 XGBoost 还提供了提升的广义线性模型。\n多种语言支持：XGBoost 在 R、Python、Julia、Scala、Java 和 C++ 中都有实现。\n\n除了跨多种语言提供支持外，XGBoost 在 R 中也可以通过多种方式实现。主要 R 实现是 xgboost 包；也可以使用 caret 作为元引擎来实现 XGBoost。h2o 包也提供了 XGBoost 的实现。这里演示 xgboost 包的使用。\n\n2.5.1 XGBoost 超参数\nxgboost 提供了额外的超参数，可以帮助减少过拟合的可能性，从而降低预测变异性，进而提高准确性。\n\n2.5.1.1 正则化\nxgboost 提供了多种正则化参数来帮助减少模型复杂性并防止过拟合。第一个参数是 gamma，这是一个伪正则化超参数，称为拉格朗日乘子，控制给定树的复杂性。gamma 指定了在树的叶节点上进行进一步分割所需的最小损失减少量。当指定 gamma 时，xgboost 会将树生长到指定的最大深度，然后修剪树以找到并移除不满足指定 gamma 的分割。gamma 在 GBM 中的树变得更深时，以及当训练和测试交叉验证误差存在显著差异时，值得探索。gamma 的取值范围为 0−∞（0 表示无约束，较大的数字表示更高的正则化）。什么算作大的 gamma 值取决于损失函数，但一般来说，如果 gamma 有影响，1-20 之间的较低值就足够了。\n另外两个传统的正则化参数包括 alpha 和 lambda。alpha 提供 L1 正则化，lambda 提供 L2 正则化。将两者都设置为大于 0 会产生弹性网正则化；与 gamma 类似，这些参数的取值范围也是 0−∞。这些正则化参数限制了树中叶节点的权重（或影响）变得极端。\n这三个超参数（gamma、alpha、lambda）都用于约束模型复杂性并减少过拟合。虽然 gamma 更常用，但调参策略应该探索所有三个参数的影响。正则化使过拟合模型在训练数据上更加保守，在某些情况下，这可以改善验证误差。\n\n\n2.5.1.2 Dropout\nDropout 是减少过拟合的另一种方法，也可以描述为正则化。由 Srivastava 等人（2014a）开发的 dropout 方法已被广泛应用于深度学习中，以防止深度神经网络过拟合。Dropout 也可以用于解决 GBM 中的过拟合问题。在构建 GBM 时，集成开始时添加的前几棵树通常主导模型性能，而后面添加的树通常只改善特征空间的一小部分预测。这常常会增加过拟合的风险，而 dropout 的思想是通过在提升序列中随机丢弃树来构建集成。这通常被称为 DART（Rashmi 和 Gilad-Bachrach 2015），因为它最初是在多重加性回归树（MART）的背景下探索的；DART 指的是 Dropout Additive Regression Trees。丢弃的百分比是另一个正则化参数。\n通常，当 gamma、alpha 或 lambda 无法帮助控制过拟合时，探索 DART 超参数将是下一个最佳选择。\n\n\n\n2.5.2 调参策略\n探索 xgboost 超参数的一般调参策略建立在基本和随机 GBM 调参策略的基础上：\n\n增加树的数量并调参学习率，使用早停\n调参树特定的超参数\n探索随机 GBM 属性\n如果发生严重的过拟合（例如，训练和交叉验证误差之间存在很大差异），探索正则化超参数\n如果你发现与默认设置明显不同的超参数值，请确保重新调参学习率\n获得最终的”最优”模型\n\n使用 xgboost 运行 XGBoost 模型需要一些额外的数据准备。xgboost 要求特征输入为矩阵，响应为向量。因此，为了提供特征的矩阵输入,需要将分类变量进行数值编码（即独热编码、标签编码）。以下代码对所有分类特征进行数值标签编码，并将训练数据框转换为矩阵：\n\nlibrary(recipes)\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stringr':\n\n    fixed\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nxgb_prep &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;%\n  step_integer(all_nominal()) %&gt;% # 类别变量数值编码\n  prep(training = ames_train, retain = TRUE) %&gt;%\n  juice()\n\nX &lt;- as.matrix(xgb_prep[setdiff(names(xgb_prep), \"Sale_Price\")]) # 训练数据转为矩阵\nY &lt;- xgb_prep$Sale_Price\n\nxgboost 接受三种不同类型的特征矩阵：普通的 R 矩阵、来自 Matrix 包的稀疏矩阵，或者 xgboost 内部的 xgb.DMatrix 对象。\n接下来，进行了网格搜索，发现以下模型超参数表现较好。RMSE接近前面常规和随机 GBM 模型。\n\nset.seed(123)\names_xgb &lt;- xgb.cv(\n  data = X,\n  label = Y,\n  nrounds = 6000, # 迭代次数，树的总数\n  objective = \"reg:squarederror\", # 目标函数\n  early_stopping_rounds = 50,  # 50次迭代没有任何改进则终止\n  nfold = 10, # 交叉验证折数\n  params = list(\n    eta = 0.1, # 梯度提升的学习速率\n    max_depth = 3, # 树的最大深度\n    min_child_weight = 3, # 叶子节点的大小\n    subsample = 0.8, # 行子采样比例\n    colsample_bytree = 1.0), # 列子采样比例（此处不控制）\n  verbose = 0\n)  \n\n# 最小测试交叉验证 RMSE\nmin(ames_xgb$evaluation_log$test_rmse_mean)\n\n[1] 23341.22\n\n\n接下来，通过执行一个检查各种正则化参数（gamma、lambda 和 alpha）的网格搜索来评估过拟合是否限制了我们模型的性能。结果表明，最佳表现的模型使用 lambda = 1，而且 alpha 或 gamma 似乎没有任何一致的模式。然而，即使 lambda = 1，交叉验证 RMSE 也没有比之前的 XGBoost 模型有所改善。\n由于学习率（eta）较低，完整的笛卡尔网格搜索需要很长时间。这里粗略设定参数进行调试。\n\n# 超参数网格\nhyper_grid &lt;- expand.grid(\n  eta = 0.01,\n  max_depth = 3, \n  min_child_weight = 3,\n  subsample = 0.5, \n  colsample_bytree = 0.5,\n  gamma = c(0, 1, 10),\n  lambda = c(0, 1e-2, 0.1),\n  alpha = c(0, 1e-2, 0.1),\n  rmse = 0,          # 用于存储 RMSE 结果的位置\n  trees = 0          # 用于存储所需树数量的位置\n)\n\n# 网格搜索\nfor(i in seq_len(nrow(hyper_grid))) {\n  set.seed(123)\n  m &lt;- xgb.cv(\n    data = X,\n    label = Y,\n    nrounds = 4000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 5, \n    nfold = 5,\n    verbose = 0,\n    params = list( \n      eta = hyper_grid$eta[i], \n      max_depth = hyper_grid$max_depth[i],\n      min_child_weight = hyper_grid$min_child_weight[i],\n      subsample = hyper_grid$subsample[i],\n      colsample_bytree = hyper_grid$colsample_bytree[i],\n      gamma = hyper_grid$gamma[i], \n      lambda = hyper_grid$lambda[i], \n      alpha = hyper_grid$alpha[i]\n    ) \n  )\n  hyper_grid$rmse[i] &lt;- min(m$evaluation_log$test_rmse_mean)\n  hyper_grid$trees[i] &lt;- m$best_iteration\n}\n\n# 结果\nhyper_grid %&gt;%\n  filter(rmse &gt; 0) %&gt;%\n  arrange(rmse) %&gt;%\n  glimpse()\n\nRows: 27\nColumns: 10\n$ eta              &lt;dbl&gt; 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,…\n$ max_depth        &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ min_child_weight &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ subsample        &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5…\n$ colsample_bytree &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5…\n$ gamma            &lt;dbl&gt; 0, 1, 10, 0, 1, 10, 0, 1, 10, 0, 1, 10, 0, 1, 10, 0, …\n$ lambda           &lt;dbl&gt; 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10,…\n$ alpha            &lt;dbl&gt; 0.01, 0.01, 0.01, 0.00, 0.00, 0.00, 0.10, 0.10, 0.10,…\n$ rmse             &lt;dbl&gt; 23106.61, 23106.61, 23106.61, 23106.61, 23106.61, 231…\n$ trees            &lt;dbl&gt; 1144, 1144, 1144, 1144, 1144, 1144, 1144, 1144, 1144,…\n\n\n一旦找到了最优超参数，就使用 xgb.train 或 xgboost 拟合最终模型。确保使用交叉验证期间找到的最优树数量。添加正则化没有带来改进，因此在最终模型中排除了它们。\n\n# 最优参数列表\nparams &lt;- list(\n  eta = 0.01,  \n  max_depth = 3,\n  min_child_weight = 3,\n  subsample = 0.5,\n  colsample_bytree = 0.5,\n  lambda = 0.1,\n  alpha = 0.01\n)\n\n# 训练最终模型\nxgb.fit.final &lt;- xgboost(\n  params = params,\n  data = X,\n  label = Y,\n  nrounds = 1144,\n  objective = \"reg:squarederror\",\n  verbose = 0\n)"
  },
  {
    "objectID": "bigdata/machlearn/mlforest.html#特征解释-1",
    "href": "bigdata/machlearn/mlforest.html#特征解释-1",
    "title": "有监督学习：随机森林与梯度提升",
    "section": "2.6 特征解释",
    "text": "2.6 特征解释\n测量 GBM 特征重要性和影响的方法与随机森林相同。与随机森林类似，gbm 和 h2o 包提供了基于杂质的特征重要性。xgboost 实际上提供了三种内置的特征重要性度量：\n\nGain（增益）：\n\n相当于随机森林中的杂质度量\n\n是最常用的模型中心度量\n\nCoverage（覆盖率）：\n\n量化由该特征影响的观测值的相对数量\n例如：如果有100个观测值、4个特征和3棵树，假设 x1 在 tree1、tree2 和 tree3 中分别用于决定10、5和2个观测值的叶节点；那么该度量将计算该特征的覆盖率为 10+5+2=17 个观测值\n对所有4个特征进行计算并表示为百分比\n\nFrequency（频率）：\n\n表示特定特征在模型树中出现次数的相对百分比\n在上述例子中，如果 x1 在 tree1、tree2 和 tree3 中分别用于2次、1次和3次分割；那么 x1 的权重为 2+1+3=6\nx1 的频率计算为所有特征权重中的百分比权重\n\n\n如果使用杂质（gain）度量检查最终模型中前10个最具影响力的特征，会看到与随机森林模型非常相似的结果。主要区别是不再将 Neighborhood 视为顶级影响特征，这可能是由于对分类特征进行标签编码的方式造成的。\n默认情况下，vip::vip() 使用 gain 方法进行特征重要性计算，但可以使用 type 参数评估其他类型。也可以使用 xgboost::xgb.ggplot.importance() 来绘制各种特征重要性度量，但需要先在最终模型上运行 xgb.importance()。\n\n# 变量重要性图\nvip::vip(xgb.fit.final) \n\n\n\n\n\n\n\n\n特征重要性度量对比表\n\n\n\n\n\n\n\n\n\n度量类型\n定义\n计算方式\n适用场景\n\n\n\n\nGain\n每个特征分割带来的杂质减少总和\n所有使用该特征的分割的增益总和\n评估特征对预测贡献\n\n\nCoverage\n特征影响的观测值数量\n影响观测值的总和/总观测值\n评估特征的覆盖范围\n\n\nFrequency\n特征在树中出现的频率\n特征分割次数/总分割次数\n评估特征的使用频率\n\n\n\nGBM（梯度提升机） 是最具强大集成算法之一，在预测准确性上通常名列前茅。虽然它们不如许多其他机器学习算法直观，且计算需求更高，但它们是你的工具箱中不可或缺的组成部分。\n一些替代算法。例如：\nLightGBM - Ke 等人（2017） 开发的梯度提升框架 - 叶优先树生长 vs 传统的层优先树生长 - 树生长更深时，专注于扩展单个分支而非生长多个分支\n- 在 R 中可用\nCatBoost - Dorogush、Ershov 和 Gulin（2018） 开发的梯度提升框架 - 专注于高效编码分类特征 - 在梯度提升过程中使用专门的方法处理分类变量 - 在 R 中可用\nGBM 家族算法对比\n\n\n\n\n\n\n\n\n\n\n算法\n树生长策略\n分类特征处理\n主要优势\nR 包\n\n\n\n\nXGBoost\n层优先\n需预处理\n正则化、早停、并行\nxgboost\n\n\nLightGBM\n叶优先\n自动处理\n速度快、内存效率高\nlightgbm\n\n\nCatBoost\n对称树\n原生支持\n分类特征处理、过拟合控制\ncatboost\n\n\n\n实际应用建议\n\n模型选择策略：\n\n# 推荐的测试顺序\nmodels_to_try &lt;- c(\n  \"xgboost\",    # 通常第一选择\n  \"lightgbm\",   # 速度优先\n  \"catboost\",   # 分类特征多时\n  \"h2o_gbm\"     # 简单部署\n)\n\n性能预期：\n\n\n\n数据集特征\n推荐算法\n预期RMSE改善\n\n\n\n\n中等规模\nXGBoost\n基准\n\n\n大规模\nLightGBM\n20-50%更快\n\n\n分类特征多\nCatBoost\n5-15%更好\n\n\n\n调参优先级：\n\n第一步：学习率 + 树数量（早停）\n第二步：树深度 + 最小子节点权重\n第三步：随机性参数（子采样）\n第四步：正则化参数\n\n\n总结：GBM 系列算法是现代机器学习中的先进代表算法，在 Kaggle 竞赛和工业应用中持续领先。掌握 GBM 算法能够显著提升机器学习建模能力。\n关键要点：\n- ✅ 预测准确性顶尖\n- ✅ 特征重要性可解释\n- ✅ 多种实现可选\n- ⚠️ 调参复杂\n- ⚠️ 计算资源需求高"
  },
  {
    "objectID": "bigdata/machlearn/mlforest.html#参考书籍",
    "href": "bigdata/machlearn/mlforest.html#参考书籍",
    "title": "有监督学习：随机森林与梯度提升",
    "section": "参考书籍",
    "text": "参考书籍\n\nBradley Boehmke & Brandon Greenwell，Hands-On Machine Learning with R，CRC Press, 2020.\n\nPang-Ning Tan 数据挖掘导论（第2版），机械工业出版社，2019.\n\nIan Foster等 Big Data and Social Science: Data Science Methods and Tools for Research and Practice, CRC Press, 2021."
  }
]